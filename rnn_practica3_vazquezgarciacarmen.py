# -*- coding: utf-8 -*-
"""Practica3_VazquezGarciaCarmen.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Nh3Da0A2DBoItDv_GYFQBkKJl6gxbKDT

#**Práctica 3 de Redes neuronales para el Procesamiento del Lenguaje Natural**

#Tema 3: Redes recurrentes para PLN


### Alumna: Carmen Vázquez García

Durante esta práctica, **se utilizan los materiales aportados para la primera práctica**. Por este motivo, **los apartado de carga, visualización y preprocesamiento de los datos son los mismos que en la práctica anterior**.

Para comenzar se comienza con la carga de *dataset* y un análisis de los datos que encontramos, visualizando los posibles problemas, soluciones encontradas, niveles de balanceo...

Luego se procede a la limpieza y preprocesado del texto, para poder dejar el texto preparado para que sea procoesado por los sistemas de aprendizaje automático que se utilizarán posteriormente.

Tras ello, se recurre a estos algoritmos de aprendizaje automático clásico. En este caso, se recurre a una amplia variedad de algoritmos, entre los que se explorarán distintos parámetros para la obtención de mejores resultados.
Para finalizar, se pondrá el foco en los sistemas basados en redes neuronales, donde se explorarán distintos hiperparámetros para la mejora de estas.

En primer lugar se dejarán las librerías necesarias preparadas para su uso, como se muestra a continuación:
"""

# Commented out IPython magic to ensure Python compatibility.
# Librerias necesarias
# Matplotlib conf
import matplotlib.pyplot as plt
# %matplotlib inline
params = {'legend.fontsize': 'x-large',
          'figure.figsize': (15, 5),
         'axes.labelsize': 'x-large',
         'axes.titlesize':'x-large',
         'xtick.labelsize': 40,
         'ytick.labelsize': 40
}
plt.rcParams.update(params)
# Seaborn conf
import seaborn as sns
sns.set(style='darkgrid')
sns.set_palette(sns.color_palette("Blues"))

import sys

# Procesado de datos
import pandas
import numpy as np
import operator

# Modelos
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer
from sklearn.feature_selection import SelectKBest, chi2

from sklearn.metrics import classification_report, accuracy_score
from sklearn.metrics import confusion_matrix

import warnings
warnings.filterwarnings('ignore')

"""## *Dataset*

Para comenzar con el tratamiento de datos, es necesario cargarlos y prepararlos para el entrenamiento. En este caso, como se indica en la práctica, no se utilizarán los datos en local. Recurriremos a los contenidos especificando rutas relativas y no absolutas.
"""

# Mediante estos comandos podemos enlazar nuestro notebook en Colab con nuestro almacenamiento en Google Drive
from google.colab import drive
drive.mount('/content/drive/')

# Cargando el dataset mediante rutas relativas (Colab)
training_set = pandas.read_csv("/content/drive/MyDrive/train_reviews.csv", quotechar='"', header=0, sep=",")
test_set = pandas.read_csv("/content/drive/My Drive/test_reviews.csv", quotechar='"', header=0, sep=",")

"""En este caso no se nos han proporcionado las etiquetas del conjunto de test. Pero más adelante recurriremos a otra forma para poder evaluar nuestros modelos.

Para saber cómo debemos trabajar con el *dataset*, es esencial visualizarlo. Por tanto, a continuación inspeccionamos el conjunto de entrenamiento y el de test.
"""

# Inspeccionamos el aspecto del conjunto de entrenamiento
training_set.head(30)

# Inspeccionamos el aspecto del conjunto de test
test_set.head(30)

"""Como se ha observado, el texto en general está bastante limpio. Aun así, encontramos algunas etiquetas que se deben eliminar, como algunas etiquetas. Esto lo realizaremos una vez hayamos observado que en la columna `sentiment` no hay datos distintos a *negative* y *positive*, paso que veremos a continuación."""

accepted_labels = {"negative", "positive"}
rows_with_problems = training_set[training_set.sentiment.isin(accepted_labels) == False]
rows_with_problems

"""Por lo que hemos podido observar, no se muestra ningún resultado que no sea *negative* o *positive*. Por tanto, no hay ninguna
información en la columna `sentiment` que no sean estos dos valores. Esto deja claro que no tenemos que modificar nada de esta columna.

Tras haber confirmado que no existe ninguna problemática con la columna `sentiment`, procedemos con la limpieza de la columna `review` de ambos conjuntos. Anteriormente vimos que lo único que ensuciaba el contenido eran algunas etiquetas (en este caso, saltos de línea), por lo que las sustituiremos por un espacio.
"""

# Sustituimos las etiquetas de salto de página en la columna 'review' por un espacio
training_set['review'] = training_set['review'].str.replace(r'<br\s*/?>', ' ')

# Creamos un archivo nuevo con el dataframe limpio
training_set.to_csv('training_set_limpio.csv', index=False)

# Inspeccionamos el aspecto del conjunto de entrenamiento limpio
training_set.head(30)

"""A continuación, realizamos exactamente el mismo proceso con la columna `review` de test_set."""

# Reemplazamos las etiquetas de salto de página por un espacio en la columna 'review' del test_set
test_set['review'] = test_set['review'].str.replace(r'<br\s*/?>', ' ')

# Crear un archivo nuevo con el dataframe limpio del test_set
test_set.to_csv('test_set_limpio.csv', index=False)

# Inspeccionamos el aspecto del conjunto de entrenamiento limpio
test_set.head(30)

"""Al volver a visualizar la columna `sentiment` hemos podido observar que las etiquetas se han sustituido por espacios, por lo que ahora podemos juntar ambos sets en un mismo *dataset* para facilitar el preprocesado."""

dataset = pandas.concat([training_set,test_set])

"""# Inspección de los datos

Comenzaremos por observar la distribución de las dos clases (*negative* y *positive*) para ver si existe algún desbalanceo de datos.
"""

from collections import Counter
sns.countplot(data=training_set, x=training_set.sentiment, order=[x for x, count in sorted(Counter(training_set.sentiment).items(), key=lambda x: -x[1])], palette="Blues_r")
plt.xticks(rotation=90)

"""Como hemos podido observar, no hay ningún tipo de desbalanceo. Por tanto, no nos tenemos que preocupar por clases desbalanceadas.

Para seguir con la inspección de datos, vamos a agrupar por sentimiento (positivo o negativo) y concatenar por cada grupo en una sola cadena de texto. Luego, utilizaremos un tokenizador para dividir estas cadenas de texto en palabras. Tras esto, se cuenta la frecuencia de cada palabra y se muestran las 25 palabras más comunes en un gráfico de barras para *negative* y *positive*.
"""

import pandas as pd
from matplotlib import interactive

# Agrupamos los textos por clases
df = pd.DataFrame({"review": training_set.review, "sentiment": training_set.sentiment})
grouped = df.groupby(["sentiment"]).apply(lambda x: x["review"].sum())
grouped_df = pd.DataFrame({"sentiment": grouped.index, "review": grouped.values})

# Tokenizamos las frases utilizando el espacio en blanco como separador
from nltk.tokenize import WhitespaceTokenizer
tokenizer = WhitespaceTokenizer()

for ii, review in enumerate(grouped_df.review):
    pd.DataFrame(tokenizer.tokenize(review)).apply(pd.value_counts).head(25).plot(kind="bar")
    plt.title(grouped_df.sentiment[ii], fontsize=20)
    plt.xticks(fontsize=15)
    interactive(True)
    plt.show()

"""Al observar las palabras más repetidas, lo que se podía esperar era encontrar términos representativos que nos ayudaran a distinguir entre reseñas positivas y negativas. Sin embargo, notamos que hay conjuntos de palabras muy similares que aportan poca información en ambas categorías, las ***stopwords***. Su presencia afecta negativamente el rendimiento del clasificador, por lo que es necesario eliminarlas. De hecho, debido a estas palabras, nos cuesta percibir la diferencia entre reseñas negativas y positivas, ya que prácticamente lo único que encontramos son estas palabras que no aportan información relevante.

También notamos que algunas palabras se consideran diferentes simplemente por la diferencia de mayúsculas y minúsculas, como *The* y *the*. Para abordar este problema, debemos **normalizar** las palabras.

Además, encontramos variantes de un mismo verbo, como *are* e *is*. Para lidiar con esto, podemos aplicar técnicas como el ***stemming*** o la **lematización** para reducir las palabras a su forma base y asegurarnos de que se traten de manera coherente en el análisis de texto.

## Preprocesado y preparación de datos


Comenzaremos con un preprocesado básico del *pipeline* de PLN
 - Extracción de palabras
 - Lematización para evitar que palabras con la misma raíz se repitan
 - *Stemming* para agrupar palabras con el mismo significado
 - Eliminación de *stopwords*

Para ello, vamos a crear la función **process_text** que engloba todo el preprocesado del texto. En este caso, utilizaremos la librería NLTK.
"""

from nltk.stem import *
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords

import re

import nltk
nltk.download('stopwords')
nltk.download('wordnet')

def process_text(raw_text):
    # Consideramos únicamente letras utilizando una expresión regular
    letters_only = re.sub("[^a-zA-Z]", " ",raw_text)
    # Convertimos todo a minúsculas
    words = letters_only.lower().split()

    # Eliminamos las stopwords
    stops = set(stopwords.words("english"))
    not_stop_words = [w for w in words if not w in stops]

    # Lematización
    wordnet_lemmatizer = WordNetLemmatizer()
    lemmatized = [wordnet_lemmatizer.lemmatize(word) for word in not_stop_words]

    # Stemming
    stemmer = PorterStemmer()
    stemmed = [stemmer.stem(word) for word in lemmatized]

    return( " ".join( stemmed ))

"""Ahora que disponemos de la función, la aplicamos al contenido textual del dataset: `review`. Para mantener el contenido original, creamos una nueva columna."""

dataset['clean_review'] = dataset['review'].apply(lambda x: process_text(x))
dataset.head()

"""A continuación, visualizamos lo que tenemos hasta el momento:"""

df = pd.DataFrame({"review": dataset[0:len(training_set)].clean_review, "sentiment": dataset[0:len(training_set)].sentiment})
grouped = df.groupby(["sentiment"]).apply(lambda x: x["review"].sum())
grouped_df = pd.DataFrame({"sentiment": grouped.index, "review": grouped.values})

from nltk.tokenize import WhitespaceTokenizer
tokenizer = WhitespaceTokenizer()

for ii, text in enumerate(grouped_df.review):
    pd.DataFrame(tokenizer.tokenize(text)).apply(pd.value_counts).head(25).plot(kind="bar")
    plt.title(grouped_df.sentiment[ii], fontsize=20)
    plt.xticks(fontsize=15)
    interactive(True)
    plt.show()

"""Como se puede apreciar en ambos gráficos, ha cambiado bastante. Hemos eliminado las *stopwords*, por lo que las palabras que observamos ahora son mucho más representativas. Además, se aprecia un uso más distintivo de algunas palabras entre las reseñas positivas y negativas, ya que antes debido a las *stopwords* el contenido era prácticamente el mismo.

Por ejemplo, aunque en las reseñas negativas también encontramos el adjetivo *good*, podemos ver que hay bastante uso también de *bad*, como podíamos esperar en la reseña negativa. Sin embargo, en el caso de las reseñas positivas, no encotramos *bad*. Lo que predomina es el adjetivo *good*, al ser reseñas positivas.

**Para continuar, vamos a divir nuestro *dataset* de nuevo para obtener el conjunto de entrenamiento y el de test, que se ha realizado de la siguiente forma:**

**X_train:** Se coge desde el inicio (índice 0) hasta el índice que representa el final del conjunto de entrenamiento (denotado como len(training_set)). Este subconjunto de datos consiste en las columnas `id` y `clean_review` del conjunto de entrenamiento, que se utilizan como características de entrada para el modelo.

**y_train:** Al igual que X_train, se toma una porción del *dataset* desde el inicio hasta el final del conjunto de entrenamiento, pero esta vez se extraen solo las etiquetas de clase `sentiment` correspondientes a las reseñas en el conjunto de entrenamiento.

**X_test:** Para el conjunto de prueba, se toma una porción de dataset que va desde el final del conjunto de entrenamiento (índice len(training_set)) hasta el final de *dataset*. Esto incluye las mismas columnas que en X_train, pero corresponden a las reseñas del conjunto de prueba.

**y_test:** Al igual que X_test, se coge desde el final del conjunto de entrenamiento hasta el final del *dataset* y se extraen las etiquetas de clase `sentiment` correspondientes a las reseñas en el conjunto de prueba.
"""

X_train = dataset[0:len(training_set)][["id", "clean_review"]]
y_train = dataset[0:len(training_set)][["sentiment"]]
X_test = dataset[len(training_set):len(dataset)][["id", "clean_review"]]
y_test = dataset[len(training_set):len(dataset)][["sentiment"]]

# Hot encoding para sentiment
from sklearn import preprocessing
le = preprocessing.LabelEncoder()
le.fit(y_train.sentiment.values)
target_sentiment = le.classes_
encoded_y_train = le.transform(y_train.sentiment.values)

# Con esta función podremos ver cómo evoluciona el entrenamiento y la validación de los modelos.

import matplotlib.pyplot as plt
plt.style.use('ggplot')

def plot_history(history):
    acc = history.history['acc']
    val_acc = history.history['val_acc']
    loss = history.history['loss']
    val_loss = history.history['val_loss']
    x = range(1, len(acc) + 1)

    plt.figure(figsize=(12, 5))
    plt.subplot(1, 2, 1)
    plt.plot(x, acc, 'b', label='Training acc')
    plt.plot(x, val_acc, 'r', label='Validation acc')
    plt.title('Training and validation accuracy')
    plt.legend()
    plt.subplot(1, 2, 2)
    plt.plot(x, loss, 'b', label='Training loss')
    plt.plot(x, val_loss, 'r', label='Validation loss')
    plt.title('Training and validation loss')
    plt.legend()

# Herramienta para la gestión de las etiquetas de salida
from tensorflow.keras.utils import to_categorical

Y_train = to_categorical(encoded_y_train, 2)
Y_test = to_categorical(le.transform(y_test.values), 2)

def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    import itertools
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')
    plt.figure()
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.show()

"""##Modelo de espacio vectorial

**Modelo de espacio vectorial**

Vamos a aplicar un modelo de espacio vectorial para representar documentos de texto. Utilizaremos la técnica de Bolsa de Palabras (BoW), que implica convertir el texto en vectores numéricos para facilitar su procesamiento por algoritmos de clasificación.

En este enfoque:


1.   Asignamos un identificador a cada palabra en nuestro conjunto de datos.
2.   Creamos una matriz llamada «document-term» utilizando CountVectorizer de sklearn. Esta matriz, denotada como X[i, j], representa la frecuencia de ocurrencia de cada palabra w en cada documento d. Cada fila i corresponde a un documento, y cada columna j a una palabra en el diccionario.

Al aplicar CountVectorizer a las reseñas limpias, obtenemos las características necesarias para ambos conjuntos de clases, negativas y positivas. Este enfoque nos permite convertir el contenido textual en información numérica estructurada que se pueda aprovechar por algoritmos de clasificación.
"""

count_vect = CountVectorizer(analyzer = "word") # set min_df = frequency

train_features = count_vect.fit_transform(X_train.clean_review)
test_features = count_vect.transform(X_test.clean_review)

"""Aunque el tamaño del vocabulario es muy grande, podemos inspeccionar hasta cierto punto los vectores que obtenemos tras realizar este paso:"""

# Observemos el vocabulario
print('Longitud del vocabulario: ')
print(len(count_vect.vocabulary_))
print()
print("Observemos el vocabulario:")
print(count_vect.vocabulary_)
print()

"""Transformamos uno de los datos de entrada en el vector de palabras que se va a utilizar como entrada en nuestros experimentos:"""

# Ejemplo original (primer título)
print("Original:")
print(dataset.review.head()[0])
print()

# Ejemplo tras el preprocesado
print("Preprocesado:")
print(X_train.clean_review[0])
print()

# Convertimos el primer título
v0 = count_vect.transform([X_train.clean_review[0]]).toarray()[0]
print('Convertido: ')
print(v0)
print()

# Es demasiado grande para observarlo, imprimimos su longitud
print('Longitud del vector: ')
print(len(v0))
print()

# ¿Cuántas palabras contiene la frase?
print('Número de palabras: ')
print(np.sum(v0))
print()

# ¿En qué posiciones del vector se encuentran las palabras?
result = np.where(v0 == 1)
print("Índices de las palabras")
print(result[0], sep='\n')
print()

# Recuperemos la frase original
print('Original:')
print(count_vect.inverse_transform(v0.reshape(1, -1)))
print()

"""Se observa cómo, partiendo de la frase original «People tried to make me believe that the premise [...]», tras el preprocesado se obtienen los tokens «peopl tri make believ premis [...]» (se ha normalizado, eliminado *stopwords* y se ha aplicado lematización y *stemming* a las palabras).

El vector resultante tras la transformación se compone de 31106 valores (tantos como palabras en el vocabulario), de las cuáles 271 serán 1, y el resto 0. Al tener tantas componentes ni siquiera podemos observar dónde se localizan los valores 1, ya que se trata de un vector one-hot muy disperso. Se pueden recuperar fácilmente las palabras originales para comprobar que el vector efectivamente representa la frase original.
Ahora extraemos las *features* del texto limpio de las reseñas.
"""

train_text_features = count_vect.fit_transform(X_train.clean_review)
test_text_features = count_vect.transform(X_test.clean_review)

"""#BoW y TF-IDF

Las redes neuronales recurrentes (RNN) destacan por su capacidad para modelar secuencias de datos y capturar dependencias temporales. A diferencia de las representaciones tradicionales como Bag-of-Words (BoW) y TF-IDF, que tratan el texto como una colección de palabras independientes, las RNN consideran la estructura secuencial del lenguaje. Esto significa que pueden entender el contexto y las relaciones entre las palabras en una oración, lo que les permite capturar mejor la semántica y el significado de un texto.

Por otro lado, BoW y TF-IDF no tienen en cuenta el orden de las palabras ni las relaciones entre ellas. Esto lleva a una pérdida de información importante en el procesamiento del lenguaje natural, especialmente en tareas donde la estructura y el contexto son totalmente esenciales, como sucede en esta práctica (análisis de sentimientos).

En cuanto a las pruebas realizadas con diferentes arquitecturas de RNN, como LSTM, GRU, redes recurrentes apiladas y bidireccionales, se llevan a cabo con el objetivo de evaluar su desempeño en esta tarea específica. Sin embargo, es importante tener en cuenta que debido a la naturaleza de las RNN, que pueden capturar la estructura secuencial del texto, no tiene mucho sentido utilizar representaciones vectoriales como BoW o TF-IDF.

Por lo tanto, si bien se pueden realizar pruebas comparativas entre diferentes arquitecturas de RNN para evaluar su eficacia en diversas tareas de PLN, no se profundizará en el uso de representaciones vectoriales tradicionales, ya que su naturaleza discreta y estática no se ajusta bien al paradigma de las RNN. En su lugar, se dará prioridad a la evaluación de las RNN en términos de su capacidad para capturar la semántica y la estructura del lenguaje natural de manera más efectiva.

Además, por este mismo motivo, solo se comentarán algunos modelos con representación BoW y TF-IDF.

#BoW

##LSTM

Las LSTM son un tipo especial de red neuronal recurrente (RNN) diseñadas para manejar problemas de dependencia a largo plazo. Las redes neuronales recurrentes tradicionales pueden tener dificultades para aprender dependencias a largo plazo debido al problema de desvanecimiento de gradiente, donde la información sobre pasos de tiempo anteriores se puede perder a medida que se propaga hacia atrás a través de la red.

Las LSTM abordan este problema mediante el uso de unidades de memoria que pueden aprender y recordar información durante largos períodos de tiempo. Estas unidades de memoria están compuestas por varias compuertas que controlan el flujo de información dentro de la celda de memoria. Las compuertas principales en una unidad LSTM son:



*   Puerta de olvido (Forget Gate): Decide qué información debe olvidarse o mantenerse en la celda de memoria.
*   Puerta de entrada (Input Gate): Decide qué nueva información debe agregarse a la celda de memoria.
*   Puerta de salida (Output Gate): Controla qué información se enviará como salida de la celda de memoria.

Además de estas compuertas, también hay una «celda de memoria» que puede mantener y actualizar el estado de la memoria a lo largo del tiempo.

##Pruebas LSTM

##Arquitectura

**Capa LSTM:**

La primera capa de la red utiliza una **estructura LSTM** (Long Short-Term Memory). Tiene **64 unidades**, lo que significa que la capa LSTM tendrá 64 celdas de memoria que procesarán la secuencia de entrada. La opción **return_sequences=False** indica que la capa LSTM no devolverá secuencias completas de salida, sino solo la salida en el último paso de tiempo, lo que es adecuado para muchos problemas de clasificación y regresión.


**Capa Densa:**

Después de la capa LSTM, sigue una **capa densa con 128 unidades**. Esta capa completamente conectada toma la salida de la capa LSTM y la transforma en un espacio de características más compacto y expresivo.
Utiliza la **función de activación ReLU** (Rectified Linear Unit), que se usa en capas ocultas de redes neuronales para solucionar el problema de la desaparición del gradiente y su eficiencia computacional.

**Capa de Dropout:**

La capa de **Dropout** se utiliza para prevenir el sobreajuste al «apagar» aleatoriamente un porcentaje de las unidades (neuronas) de la capa durante el entrenamiento.
En este caso, se establece una tasa de dropout del 0.5, lo que significa que cada unidad tiene una probabilidad del 50% de ser desactivada durante el entrenamiento. Esto ayuda a regularizar el modelo y reducir la dependencia de unidades específicas, lo que generaliza mejor el modelo para datos nuevos.

**Capa de Salida:**

La última capa es una **capa densa con una sola unidad**, que se utiliza para la **clasificación binaria** en este caso.
La función de activación sigmoide se aplica a la salida de esta capa, lo que produce una probabilidad en el rango [0, 1]. Esto es adecuado para problemas de clasificación binaria, donde la salida se interpreta como la probabilidad de que la entrada pertenezca a la clase positiva.

##Justificación

**Capa LSTM:**

Las redes LSTM se usan en problemas de secuencias debido a su capacidad para modelar dependencias a largo plazo en los datos. Esto es especialmente útil en problemas donde el contexto pasado es importante para predecir el siguiente elemento en la secuencia. Por ejemplo, en análisis de sentimientos, la estructura de las oraciones anteriores puede afectar el sentimiento de la oración actual.
La elección de 64 unidades en la capa LSTM es un punto de partida razonable. Es lo suficientemente grande como para capturar patrones complejos en los datos, pero no tan grande como para aumentar excesivamente la complejidad del modelo, lo que podría conducir al sobreajuste.

**Capa Densa:**

Después de la capa LSTM, una capa densa se utiliza para procesar las características extraídas de la secuencia por la capa LSTM. Esta capa ayuda a reducir la dimensionalidad de los datos y extraer características más relevantes para la tarea en cuestión.
La elección de 128 unidades en la capa densa proporciona una representación más densa de las características aprendidas en la secuencia de entrada, lo que aumenta la capacidad del modelo para capturar patrones más complejos y sutiles.

**Capa de Dropout:**

La adición de una capa de dropout ayuda a regularizar el modelo al reducir el sobreajuste. Durante el entrenamiento, las unidades se apagan aleatoriamente con una probabilidad específica, lo que previene la coadaptación de las unidades y mejora la generalización del modelo a datos nuevos.
La tasa de dropout del 50% es comúnmente utilizada y proporciona un buen equilibrio entre la prevención del sobreajuste y la retención de información importante durante el entrenamiento.

**Capa de Salida:**

La capa de salida con una unidad y función de activación sigmoide es apropiada para problemas de clasificación binaria, como el análisis de sentimientos. La función sigmoide produce una salida en el rango [0, 1], que puede interpretarse directamente como la probabilidad de pertenencia a la clase positiva.
"""

import tensorflow as tf
from keras.models import Sequential
from keras.layers import LSTM, GRU, Bidirectional, Dense, Dropout, Flatten

# Número de épocas
epochs = 10
# Tamaño de batch
batch_size = 16

# Definir una arquitectura de LSTM
model_lstm = Sequential()

# Capas recurrentes LSTM
model_lstm.add(LSTM(64, return_sequences=False, input_shape=(train_text_features.shape[1], 1)))
model_lstm.add(Dense(units=128, activation='relu'))
model_lstm.add(Dropout(0.5))
model_lstm.add(Dense(units=1, activation='sigmoid'))

# Compilar el modelo LSTM
optimizer_lstm = tf.keras.optimizers.Adam(learning_rate=0.001)
model_lstm.compile(optimizer=optimizer_lstm, loss='binary_crossentropy', metrics=['acc'])

# Resumen del modelo
model_lstm.summary()

# Definir EarlyStopping
early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

# Entrenar el modelo y almacenar el historial en la variable history
history = model_lstm.fit(train_text_features.toarray().astype(float), encoded_y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2, callbacks=[early_stopping])

# Llamar a la función plot_history con el historial
plot_history(history)

predict_x=model_lstm.predict(test_text_features.toarray().astype(float))
y_pred = (predict_x > 0.5).astype(int)

# Se deshace la transformación de las predicciones para obtener las predicciones como negativas o positivas
y_pred = le.inverse_transform(y_pred)

print("Classification Report")
print
print(classification_report(y_test, y_pred, target_names=["negative", "positive"]))
cm = confusion_matrix(y_test, y_pred, labels=["negative", "positive"])

plot_confusion_matrix(cm, classes=["negative", "positive"])

print("Final Accuracy")
print(accuracy_score(y_test, y_pred))

"""**Resultados del entrenamiento:**

Durante el entrenamiento, se ejecutaron 4 épocas.
En cada época, se muestra la pérdida (loss) y la precisión (accuracy) tanto en el conjunto de entrenamiento como en el conjunto de validación.
La pérdida de entrenamiento (loss) y la precisión de entrenamiento (accuracy) se muestran en la salida para cada época. También se proporcionan las métricas de pérdida y precisión para el conjunto de validación (val_loss y val_acc).
Basado en los resultados del entrenamiento, el modelo no mejora con el tiempo, ya que la precisión de validación no está aumentando y la pérdida de validación no está disminuyendo.

**Resultados de la clasificación:**
Se muestra un informe de clasificación que proporciona precisiones, recall, f1-score y soporte para cada clase (en este caso, negative y positive).
Se observa que la precisión y el recall para la clase negative son ambos 0.00, lo que indica que el modelo no predice correctamente esta clase.
La precisión para la clase positive es del 49%, lo que significa que aproximadamente la mitad de las predicciones para esta clase son correctas.
El informe también proporciona la precisión promedio ponderada (weighted avg) y la precisión macro promedio (macro avg), que son 0.24 y 0.49 respectivamente.

**Matriz de confusión:**
La matriz de confusión muestra el número de verdaderos positivos, verdaderos negativos, falsos positivos y falsos negativos.
En este caso, la mayoría de las predicciones caen en la clase positive, con solo 0 predicciones correctas para la clase negative. Esto es consistente con las métricas de precisión y recall del informe de clasificación.

###Mejora de BoW LSTM
"""

import tensorflow as tf
from keras.models import Sequential
from keras.layers import LSTM, Dense, Dropout

# Número de épocas
epochs = 20
# Tamaño de batch
batch_size = 32

# Definir una arquitectura de LSTM más robusta con Early Stopping
model = Sequential()

# Capas LSTM
model.add(LSTM(units=32, input_shape=(train_text_features.shape[1], 1), return_sequences=True))
model.add(Dropout(0.25))

model.add(LSTM(units=64, return_sequences=False))
model.add(Dropout(0.25))

# Capas densas
model.add(Dense(units=128, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(units=1, activation='sigmoid'))

# Compilar el modelo
optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)
model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['acc'])

# Resumen del modelo
model.summary()

from keras.callbacks import EarlyStopping

# Definir EarlyStopping
early_stopping = EarlyStopping(monitor='val_loss', patience=3)

# Entrenar el modelo y almacenar el historial en la variable history
history = model.fit(train_text_features.toarray().astype(float), encoded_y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2, callbacks=[early_stopping])

# Llamar a la función plot_history con el historial
plot_history(history)

"""Hemos definido dos capas LSTM, con 32 y 64 unidades respectivamente. Estas capas están apiladas una encima de la otra, y la última tiene return_sequences=False porque no necesitamos que devuelva secuencias en este caso.
Las capas Dropout se mantienen para la regularización.
Las capas densas se mantienen igual.
"""

predict_x=model.predict(test_text_features.toarray().astype(float))
y_pred = (predict_x > 0.5).astype(int)

# Se deshace la transformación de las predicciones para obtener las predicciones como negativas o positivas
y_pred = le.inverse_transform(y_pred)

print("Classification Report")
print
print(classification_report(y_test, y_pred, target_names=["negative", "positive"]))
cm = confusion_matrix(y_test, y_pred, labels=["negative", "positive"])

plot_confusion_matrix(cm, classes=["negative", "positive"])

print("Final Accuracy")
print(accuracy_score(y_test, y_pred))

"""##Cambios realizados:
**Aumento de las épocas de entrenamiento:**

Se ha incrementado el número de épocas de entrenamiento de 10 a 20. Esto permite que el modelo tenga más oportunidades para ajustarse a los datos y mejorar su rendimiento.

**Ajuste del tamaño de lote (batch size):**

Se ha reducido el tamaño de lote de entrenamiento de 16 a 32. Un tamaño de lote más grande puede ayudar a estabilizar el proceso de entrenamiento y mejorar la convergencia del modelo.

**Agregación de capas LSTM y Dropout:**

Se añadió una segunda capa LSTM con 64 unidades y sin retorno de secuencias, seguida por una capa de Dropout con una tasa del 25%. Esto aumenta la capacidad del modelo para capturar patrones complejos en los datos secuenciales y ayuda a prevenir el sobreajuste.


##Mejoras Observadas:

**Precisión del modelo:**

La precisión del modelo ha mejorado del 49 % al 51 % en el conjunto de datos de validación.

**Desempeño en la clase negative:**

En el informe de clasificación, se observa que la precisión para la clase negative ha aumentado al 51 %, lo que indica que el modelo ahora puede predecir mejor esta clase en comparación con el modelo anterior.

**Desempeño general del modelo:**

A pesar de que la precisión para la clase "positive" sigue siendo baja, el desempeño general del modelo ha mejorado en términos de precisión promedio ponderada y precisión macro promedio en el conjunto de datos de validación.

##GRU

**Arquitectura del modelo GRU:**

La arquitectura consta de una capa GRU con 64 unidades.
Se sigue una capa densa con 128 unidades y una función de activación ReLU.
Se aplica una capa de Dropout con una tasa de 0.5 para regularizar el modelo y prevenir el sobreajuste.
Finalmente, se agrega una capa de salida con una unidad y función de activación sigmoide para la clasificación binaria.

**Resumen del modelo GRU:**
La cantidad total de parámetros a aprender es de 21,313.
Todos los parámetros son entrenables.

**Resultados del entrenamiento y evaluación:**
Durante el entrenamiento, se realizaron 10 épocas.
El tamaño del lote (batch size) fue de 16.
En el conjunto de datos de validación, se observa una pérdida (loss) de alrededor de 0.69 y una precisión (accuracy) de aproximadamente 0.49 en todas las épocas.

El modelo no logra aprender de manera efectiva, ya que la precisión y la pérdida no muestran una mejora significativa a lo largo del entrenamiento.

El informe de clasificación muestra que el modelo solo predice correctamente la clase positive, mientras que la precisión para la clase negative es muy baja.
"""

from keras.models import Sequential
from keras.layers import LSTM, GRU, Bidirectional, Dense, Dropout, Flatten

# Número de épocas
epochs = 10
# Tamaño de batch
batch_size = 16

# Definir una arquitectura de GRU
model_gru = Sequential()

# Capas recurrentes GRU
model_gru.add(GRU(64, return_sequences=False, input_shape=(train_text_features.shape[1], 1)))
model_gru.add(Dense(units=128, activation='relu'))
model_gru.add(Dropout(0.5))
model_gru.add(Dense(units=1, activation='sigmoid'))

# Compilar el modelo GRU
optimizer_gru = tf.keras.optimizers.Adam(learning_rate=0.001)
model_gru.compile(optimizer=optimizer_gru, loss='binary_crossentropy', metrics=['acc'])

# Resumen del modelo GRU
model_gru.summary()

# Definir EarlyStopping
early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

# Entrenar el modelo y almacenar el historial en la variable history
history = model_gru.fit(train_text_features.toarray().astype(float), encoded_y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2, callbacks=[early_stopping])

# Llamar a la función plot_history con el historial
plot_history(history)

predict_x=model_gru.predict(test_text_features.toarray().astype(float))
y_pred = (predict_x > 0.5).astype(int)

# Se deshace la transformación de las predicciones para obtener las predicciones como negativas o positivas
y_pred = le.inverse_transform(y_pred)

print("Classification Report")
print
print(classification_report(y_test, y_pred, target_names=["negative", "positive"]))
cm = confusion_matrix(y_test, y_pred, labels=["negative", "positive"])

plot_confusion_matrix(cm, classes=["negative", "positive"])

print("Final Accuracy")
print(accuracy_score(y_test, y_pred))

"""#Red recurrente apilada

**Red recurrente apilada**

**Definición de hiperparámetros:**

Se establece el número de épocas en 40, lo que indica cuántas veces se recorrerá todo el conjunto de datos durante el entrenamiento.
El tamaño del lote (batch size) se fija en 32, que determina cuántas muestras se procesan antes de actualizar los pesos del modelo.

**Definición del modelo:**

Se inicializa un modelo secuencial model_stacked_rnn, que es una pila lineal de capas.
Se añaden dos capas LSTM apiladas al modelo:

La primera capa LSTM tiene 64 unidades y está configurada para devolver secuencias (return_sequences=True). Esto significa que cada paso de tiempo de la secuencia de entrada se mapea a una salida de la misma longitud.

La segunda capa LSTM también tiene 64 unidades y se configura para no devolver secuencias (return_sequences=False). En esta configuración, la capa LSTM solo devuelve la salida correspondiente al último paso de tiempo.

Se añade una capa densa con 128 unidades y función de activación ReLU.

Se aplica una capa de Dropout con una tasa del 0.5 para regularizar el modelo y reducir el sobreajuste.
Finalmente, se añade una capa de salida con una unidad y función de activación sigmoide para la clasificación binaria.

**Compilación del modelo:**

Se compila el modelo utilizando el optimizador Adam con una tasa de aprendizaje de 0.001.
La función de pérdida se establece en 'binary_crossentropy', adecuada para problemas de clasificación binaria.
Con 'acc' se evalúa la precisión durante el entrenamiento.
"""

# Número de épocas
epochs = 40
# Tamaño de batch
batch_size = 32

# Definir una arquitectura de red recurrente apilada
model_stacked_rnn = Sequential()

# Capas recurrentes LSTM apiladas
model_stacked_rnn.add(LSTM(64, return_sequences=True, input_shape=(train_text_features.shape[1], 1)))
model_stacked_rnn.add(LSTM(64, return_sequences=False))
model_stacked_rnn.add(Dense(units=128, activation='relu'))
model_stacked_rnn.add(Dropout(0.5))
model_stacked_rnn.add(Dense(units=1, activation='sigmoid'))

# Compilar el modelo de red recurrente apilada
optimizer_stacked_rnn = tf.keras.optimizers.Adam(learning_rate=0.001)
model_stacked_rnn.compile(optimizer=optimizer_stacked_rnn, loss='binary_crossentropy', metrics=['acc'])

# Resumen del modelo de red recurrente apilada
model_stacked_rnn.summary()

# Definir EarlyStopping
early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

# Entrenar el modelo y almacenar el historial en la variable history
history = model_stacked_rnn.fit(train_text_features.toarray().astype(float), encoded_y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2, callbacks=[early_stopping])

# Llamar a la función plot_history con el historial
plot_history(history)

predict_x=model_stacked_rnn.predict(test_text_features.toarray().astype(float))
y_pred = (predict_x > 0.5).astype(int)

# Se deshace la transformación de las predicciones para obtener las predicciones como negativas o positivas
y_pred = le.inverse_transform(y_pred)

print("Classification Report")
print
print(classification_report(y_test, y_pred, target_names=["negative", "positive"]))
cm = confusion_matrix(y_test, y_pred, labels=["negative", "positive"])

plot_confusion_matrix(cm, classes=["negative", "positive"])

print("Final Accuracy")
print(accuracy_score(y_test, y_pred))

"""#Red recurrente bidireccional

**Red recurrente bidireccional**

**Definición de hiperparámetros:**

Se establece el número de épocas en 40, lo que indica cuántas veces se recorrerá todo el conjunto de datos durante el entrenamiento.
El tamaño del lote (batch size) se fija en 32, que determina cuántas muestras se procesan antes de actualizar los pesos del modelo.

**Definición del modelo:**

Se inicializa un modelo secuencial model_bidirectional_rnn, que es una pila lineal de capas.
Se añade una capa bidireccional LSTM al modelo utilizando Bidirectional.

La capa bidireccional LSTM tiene 64 unidades y se configura para no devolver secuencias (return_sequences=False). Esta capa procesa la entrada tanto en el sentido directo como en el inverso, lo que permite capturar tanto la información pasada como la futura en la secuencia.

Se añade una capa densa con 128 unidades y función de activación ReLU.

Se aplica una capa de Dropout con una tasa del 0.5 para regularizar el modelo y reducir el sobreajuste.

Finalmente, se añade una capa de salida con una unidad y función de activación sigmoide para la clasificación binaria.

**Compilación del modelo:**

Se compila el modelo utilizando el optimizador Adam con una tasa de aprendizaje de 0.001.

La función de pérdida se establece en 'binary_crossentropy', adecuada para problemas de clasificación binaria.

Se usa 'acc' como métrica para evaluar la precisión durante el entrenamiento.
"""

# Número de épocas
epochs = 40
# Tamaño de batch
batch_size = 32

# Definir una arquitectura de red recurrente bidireccional
model_bidirectional_rnn = Sequential()

# Capas recurrentes bidireccionales LSTM
model_bidirectional_rnn.add(Bidirectional(LSTM(64, return_sequences=False), input_shape=(train_text_features.shape[1], 1)))
model_bidirectional_rnn.add(Dense(units=128, activation='relu'))
model_bidirectional_rnn.add(Dropout(0.5))
model_bidirectional_rnn.add(Dense(units=1, activation='sigmoid'))

# Compilar el modelo de red recurrente bidireccional
optimizer_bidirectional_rnn = tf.keras.optimizers.Adam(learning_rate=0.001)
model_bidirectional_rnn.compile(optimizer=optimizer_bidirectional_rnn, loss='binary_crossentropy', metrics=['acc'])

# Resumen del modelo de red recurrente bidireccional
model_bidirectional_rnn.summary()

# Definir EarlyStopping
early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

# Entrenar el modelo y almacenar el historial en la variable history
history = model_bidirectional_rnn.fit(train_text_features.toarray().astype(float), encoded_y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2, callbacks=[early_stopping])

# Llamar a la función plot_history con el historial
plot_history(history)

predict_x=model_bidirectional_rnn.predict(test_text_features.toarray().astype(float))
y_pred = (predict_x > 0.5).astype(int)

# Se deshace la transformación de las predicciones para obtener las predicciones como negativas o positivas
y_pred = le.inverse_transform(y_pred)

print("Classification Report")
print
print(classification_report(y_test, y_pred, target_names=["negative", "positive"]))
cm = confusion_matrix(y_test, y_pred, labels=["negative", "positive"])

plot_confusion_matrix(cm, classes=["negative", "positive"])

print("Final Accuracy")
print(accuracy_score(y_test, y_pred))

"""#TF-IDF"""

from sklearn.feature_extraction.text import TfidfTransformer

X_train["all"] = X_train[['clean_review']].apply(lambda x: ''.join(x), axis=1)
X_test["all"] = X_test[['clean_review']].apply(lambda x: ''.join(x), axis=1)

count_vect = CountVectorizer(analyzer = "word", max_features=10000)
train_features = count_vect.fit_transform(X_train['all'])
test_features = count_vect.transform(X_test['all'])

# Se utiliza L2 (norma euclídea) para normalizar los vectores resultantes
tfidf = TfidfTransformer(norm="l2")
train_text_tfidf_features = tfidf.fit_transform(train_features)
test_text_tfidf_features = tfidf.fit_transform(test_features)

from tensorflow.keras.utils import to_categorical

Y_train = to_categorical(encoded_y_train, 2)
Y_test = to_categorical(le.transform(y_test.values), 2)

print("Dimensión de los vectores:", train_text_tfidf_features.shape[1])

"""##LSTM"""

from keras.models import Sequential
from keras.layers import LSTM, GRU, Bidirectional, Dense, Dropout, Flatten

# Número de épocas
epochs = 40
# Tamaño de batch
batch_size = 32

# Definir una arquitectura de LSTM
model_lstm = Sequential()

# Capas recurrentes LSTM
model_lstm.add(LSTM(64, return_sequences=False, input_shape=(train_text_tfidf_features.shape[1], 1)))
model_lstm.add(Dense(units=128, activation='relu'))
model_lstm.add(Dropout(0.5))
model_lstm.add(Dense(units=1, activation='sigmoid'))

# Compilar el modelo LSTM
optimizer_lstm = tf.keras.optimizers.Adam(learning_rate=0.001)
model_lstm.compile(optimizer=optimizer_lstm, loss='binary_crossentropy', metrics=['acc'])

# Definir EarlyStopping
early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

# Entrenar el modelo y almacenar el historial en la variable history
history = model_lstm.fit(train_text_tfidf_features.toarray().astype(float), encoded_y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2, callbacks=[early_stopping])

# Llamar a la función plot_history con el historial
plot_history(history)

predict_x=model_lstm.predict(test_text_tfidf_features.toarray().astype(float))
y_pred=np.argmax(predict_x,axis=1)

# Se deshace la transformación de las predicciones para obtener las predicciones como negativas o positivas
y_pred = le.inverse_transform(y_pred)

print("Classification Report")
print
print(classification_report(y_test, y_pred, target_names=["negative", "positive"]))
cm = confusion_matrix(y_test, y_pred, labels=["negative", "positive"])

plot_confusion_matrix(cm, classes=["negative", "positive"])

print("Final Accuracy")
print(accuracy_score(y_test, y_pred))

"""##GRU

**Capa GRU (Gated Recurrent Unit):**

Se añaden dos capas de GRU al modelo.
Cada capa tiene 32 unidades (o neuronas), lo que significa que tiene 32 unidades de memoria para recordar patrones en los datos secuenciales.

La primera capa de GRU tiene return_sequences=True, lo que significa que devuelve secuencias completas en lugar de solo la salida en el último paso de tiempo. Esto es necesario para conectarla con la siguiente capa recurrente.

La función de activación utilizada es tanh, que se ha usado esta vez como prueba ya que se usa en capas recurrentes para manejar gradientes de forma más eficaz que otras funciones de activación como ReLU.

**Capas densas con Dropout y regularización L2:**

Se añaden dos capas densas adicionales después de las capas GRU para la clasificación final.

La primera capa densa tiene 64 unidades y utiliza la función de activación ReLU. También se aplica una regularización L2 con un parámetro de regularización de 0.01. La regularización L2 ayuda a prevenir el sobreajuste al penalizar los valores grandes en los pesos del modelo.

Se añade una capa de Dropout con una tasa del 0.5 después de la primera capa densa. La capa de Dropout desactiva aleatoriamente algunas neuronas durante el entrenamiento para evitar el sobreajuste.

**Capa de salida:**

Se añade una capa de salida con dos unidades y función de activación softmax. Esto se debe a que el problema parece ser de clasificación binaria, ya que se utiliza la pérdida de entropía cruzada binaria (binary_crossentropy).

**Compilación del modelo:**

Se compila el modelo utilizando el optimizador Adam, que es un optimizador popular para el entrenamiento de redes neuronales.

Se elige 'binary_crossentropy' como función de pérdida, adecuada para problemas de clasificación binaria.

Se utiliza 'acc' como métrica para evaluar la precisión del modelo durante el entrenamiento.
"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import GRU, Dense, Dropout
from tensorflow.keras.regularizers import l2

model = Sequential()

# Capa GRU con units=32
model.add(GRU(units=32, input_shape=(train_text_tfidf_features.shape[1], 1), activation='tanh', return_sequences=True))

# Capa GRU con units=32
model.add(GRU(units=32, activation='tanh'))

# Capas densas con Dropout y regularización L2
model.add(Dense(units=64, activation='relu', kernel_regularizer=l2(0.01)))
model.add(Dropout(0.5))
model.add(Dense(units=2, activation='softmax'))

# Compilar el modelo
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])

# Imprimir el resumen del modelo
print(model.summary())

# Definir el objeto EarlyStopping
early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

history = model.fit(train_text_tfidf_features.toarray().astype(float), Y_train, epochs=40, batch_size=32, validation_split=0.2)
plot_history(history)

predict_x=model.predict(test_text_tfidf_features.toarray().astype(float))
y_pred=np.argmax(predict_x,axis=1)

# Se deshace la transformación de las predicciones para obtener las predicciones como negativas o positivas
y_pred = le.inverse_transform(y_pred)

print("Classification Report")
print
print(classification_report(y_test, y_pred, target_names=["negative", "positive"]))
cm = confusion_matrix(y_test, y_pred, labels=["negative", "positive"])

plot_confusion_matrix(cm, classes=["negative", "positive"])

print("Final Accuracy")
print(accuracy_score(y_test, y_pred))

"""A continuación, se realizará una nueva prueba:"""

from keras.models import Sequential
from keras.layers import LSTM, GRU, Bidirectional, Dense, Dropout, Flatten

# Número de épocas
epochs = 10
# Tamaño de batch
batch_size = 16

# Definir una arquitectura de GRU
model_gru = Sequential()

# Capas recurrentes GRU
model_gru.add(GRU(64, return_sequences=False, input_shape=(train_text_tfidf_features.shape[1], 1)))
model_gru.add(Dense(units=128, activation='relu'))
model_gru.add(Dropout(0.5))
model_gru.add(Dense(units=1, activation='sigmoid'))

# Compilar el modelo GRU
optimizer_gru = tf.keras.optimizers.Adam(learning_rate=0.001)
model_gru.compile(optimizer=optimizer_gru, loss='binary_crossentropy', metrics=['acc'])

# Resumen del modelo GRU
model_gru.summary()

# Definir EarlyStopping
early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

# Entrenar el modelo y almacenar el historial en la variable history
history = model_gru.fit(train_text_tfidf_features.toarray().astype(float), encoded_y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2, callbacks=[early_stopping])

# Llamar a la función plot_history con el historial
plot_history(history)

predict_x=model_gru.predict(test_text_tfidf_features.toarray().astype(float))
y_pred=np.argmax(predict_x,axis=1)

# Se deshace la transformación de las predicciones para obtener las predicciones como negativas o positivas
y_pred = le.inverse_transform(y_pred)

print("Classification Report")
print
print(classification_report(y_test, y_pred, target_names=["negative", "positive"]))
cm = confusion_matrix(y_test, y_pred, labels=["negative", "positive"])

plot_confusion_matrix(cm, classes=["negative", "positive"])

print("Final Accuracy")
print(accuracy_score(y_test, y_pred))

"""Los resultados obtenidos con este nuevo modelo son similares a los anteriores, lo cual es atribuible en gran medida al uso del TF-IDF como método de representación de características. Aunque se ha cambiado la arquitectura del modelo a una GRU y se ha ajustado la configuración, el TF-IDF no es capaz de capturar la semántica de manera efectiva en contextos de análisis de texto para tareas de aprendizaje profundo. Por lo tanto, es probable que la limitación principal resida en la naturaleza de la representación de características más que en la arquitectura del modelo en sí.

#Red recurrente apilada
"""

# Número de épocas
epochs = 40
# Tamaño de batch
batch_size = 32

# Definir una arquitectura de red recurrente apilada
model_stacked_rnn = Sequential()

# Capas recurrentes LSTM apiladas
model_stacked_rnn.add(LSTM(64, return_sequences=True, input_shape=(train_text_tfidf_features.shape[1], 1)))
model_stacked_rnn.add(LSTM(64, return_sequences=False))
model_stacked_rnn.add(Dense(units=128, activation='relu'))
model_stacked_rnn.add(Dropout(0.5))
model_stacked_rnn.add(Dense(units=1, activation='sigmoid'))

# Compilar el modelo de red recurrente apilada
optimizer_stacked_rnn = tf.keras.optimizers.Adam(learning_rate=0.001)
model_stacked_rnn.compile(optimizer=optimizer_stacked_rnn, loss='binary_crossentropy', metrics=['acc'])

# Resumen del modelo de red recurrente apilada
model_stacked_rnn.summary()

# Definir EarlyStopping
early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

# Entrenar el modelo y almacenar el historial en la variable history
history = model_stacked_rnn.fit(train_text_tfidf_features.toarray().astype(float), encoded_y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2, callbacks=[early_stopping])

# Llamar a la función plot_history con el historial
plot_history(history)

predict_x=model_stacked_rnn.predict(test_text_tfidf_features.toarray().astype(float))
y_pred=np.argmax(predict_x,axis=1)

# Se deshace la transformación de las predicciones para obtener las predicciones como negativas o positivas
y_pred = le.inverse_transform(y_pred)

print("Classification Report")
print
print(classification_report(y_test, y_pred, target_names=["negative", "positive"]))
cm = confusion_matrix(y_test, y_pred, labels=["negative", "positive"])

plot_confusion_matrix(cm, classes=["negative", "positive"])

print("Final Accuracy")
print(accuracy_score(y_test, y_pred))

"""#Red recurrente bidireccional"""

# Número de épocas
epochs = 40
# Tamaño de batch
batch_size = 32

# Definir una arquitectura de red recurrente bidireccional
model_bidirectional_rnn = Sequential()

# Capas recurrentes bidireccionales LSTM
model_bidirectional_rnn.add(Bidirectional(LSTM(64, return_sequences=False), input_shape=(train_text_tfidf_features.shape[1], 1)))
model_bidirectional_rnn.add(Dense(units=128, activation='relu'))
model_bidirectional_rnn.add(Dropout(0.5))
model_bidirectional_rnn.add(Dense(units=1, activation='sigmoid'))

# Compilar el modelo de red recurrente bidireccional
optimizer_bidirectional_rnn = tf.keras.optimizers.Adam(learning_rate=0.001)
model_bidirectional_rnn.compile(optimizer=optimizer_bidirectional_rnn, loss='binary_crossentropy', metrics=['acc'])

# Resumen del modelo de red recurrente bidireccional
model_bidirectional_rnn.summary()

# Definir EarlyStopping
early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

# Entrenar el modelo y almacenar el historial en la variable history
history = model_bidirectional_rnn.fit(train_text_tfidf_features.toarray().astype(float), encoded_y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2, callbacks=[early_stopping])

# Llamar a la función plot_history con el historial
plot_history(history)

predict_x=model_bidirectional_rnn.predict(test_text_tfidf_features.toarray().astype(float))
y_pred=np.argmax(predict_x,axis=1)

# Se deshace la transformación de las predicciones para obtener las predicciones como negativas o positivas
y_pred = le.inverse_transform(y_pred)

print("Classification Report")
print
print(classification_report(y_test, y_pred, target_names=["negative", "positive"]))
cm = confusion_matrix(y_test, y_pred, labels=["negative", "positive"])

plot_confusion_matrix(cm, classes=["negative", "positive"])

print("Final Accuracy")
print(accuracy_score(y_test, y_pred))

"""## Conclusiones BoW y TF-IDF

Como se ha podido observar utilizar Bag of Words (BoW) o TF-IDF (Term Frequency-Inverse Document Frequency) con redes neuronales recurrentes (RNN) no es la mejor opción debido a las características inherentes de estas técnicas y las propiedades específicas de las RNN.

En primer lugar, se encuentra la **perdida de secuencia**: Las RNN, especialmente las del tipo LSTM (Long Short-Term Memory) o GRU (Gated Recurrent Unit), están diseñadas para modelar secuencias y capturar dependencias a largo plazo en los datos. BoW y TF-IDF, al representar un documento como un conjunto de palabras sin considerar el orden, pierden esta información de secuencia, que es **totalmente esencial** para las RNN.

Por otro lado, **el tamaño fijo de entrada**: Las RNN generalmente operan con secuencias de longitud variable. Sin embargo, BoW y TF-IDF generan vectores de longitud fija que representan la presencia o frecuencia de cada palabra en el vocabulario. Esto es un problema, ya que se necesitaría algún tipo de truncamiento o relleno de secuencia para hacer que todas las entradas tengan la misma longitud, lo cual no es ideal para las RNN.

Para finalizar, la **ignorancia del significado semántico**: BoW y TF-IDF no capturan el significado semántico de las palabras ni las relaciones entre ellas. **Esto es la mayor limitación, que además es MUY significativa, ya que estas redes se basan en comprender el contexto y la semántica es crucial.** Las RNN pueden capturar estas relaciones semánticas gracias a su capacidad para procesar secuencias de manera secuencial.

## Word embeddings

## Embeddings
"""

# Importamos capas densas, capas de embeddings y capas para el aplanado de dimensiones
from keras.layers import Dense, Embedding, Flatten, GlobalMaxPool1D

# Importamos las herramientas para generar el modelo y trabajar con las entradas
from keras.models import Sequential, Model
from tensorflow.python.keras.models import Input

# Utilidades para el preprocesado de textos: tokenizador y rellenado de secuencias
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences

from sklearn.model_selection import train_test_split

# Herramienta para la gestión de las etiquetas de salida
from tensorflow.keras.utils import to_categorical

# Herramienta para implementar Early Stopping (criterios de parada del entrenamiento)
from keras.callbacks import EarlyStopping

# Utilidad para el trabajo con embeddings preentrenados
from keras.initializers import Constant

# Número de épocas (iteraciones globales)
epochs = 40

# Dimensión de la capa de embeddings
emb_dim = 100

# Tamaño de batch (lote)
batch_size = 32

# Número de palabras tenidas en cuenta (reducción del vocabulario)
n_most_common_words = 50000

# Máxima longitud del documento
max_len = 256

tokenizer = Tokenizer()
tokenizer.fit_on_texts(X_train.clean_review)
sequences_train = tokenizer.texts_to_sequences(X_train.clean_review)
sequences_test = tokenizer.texts_to_sequences(X_test.clean_review)
vocab_size = len(tokenizer.word_index) + 1

print("Tamaño del vocabulario: ", vocab_size)

train_text_nn = pad_sequences(sequences_train, maxlen=max_len)
test_text_nn = pad_sequences(sequences_test, maxlen=max_len)

Y_train = to_categorical(encoded_y_train, 2)
Y_test = to_categorical(le.transform(y_test.values), 2)

"""#LSTM

Las redes neuronales recurrentes, como las LSTM utilizadas en este modelo, son capaces de capturar relaciones temporales y dependencias a largo plazo en secuencias de texto. Esto es crucial para comprender el contexto en el que aparecen las palabras en una reseña y cómo contribuyen al sentimiento general expresado en ella.

Las reseñas pueden variar en longitud, y las LSTM son capaces de manejar secuencias de entrada de longitud variable. Esto significa que el modelo puede procesar reseñas de diferentes longitudes sin necesidad de truncarlas o rellenarlas artificialmente.

**Al utilizar embeddings de palabras en lugar de representaciones como Bag of Words (BoW) o TF-IDF, el modelo puede capturar de manera más efectiva el significado semántico y contextual de las palabras en las reseñas.** Esto mejora el rendimiento del modelo en tareas de análisis de sentimientos.

La arquitectura del modelo es flexible y adaptable a diferentes dominios y conjuntos de datos. Se pueden ajustar fácilmente hiperparámetros como el número de unidades LSTM, la dimensión de los embeddings de palabras y la tasa de dropout para optimizar el rendimiento del modelo en la tarea específica de clasificación de sentimientos.

Este modelo utiliza una arquitectura de red neuronal recurrente (RNN) LSTM:

**Capa de Embedding:** Se utiliza para transformar las representaciones de palabras en vectores densos de números reales. En este caso, el tamaño del embedding es de 100 dimensiones. Se ha optado por esta dimensión para capturar suficiente información semántica de las palabras.

**Capas LSTM:** Se han utilizado dos capas LSTM para capturar relaciones de dependencia a largo plazo en los datos de texto. Ambas capas tienen 64 unidades LSTM para procesar las secuencias de entrada. Se ha aplicado una técnica de dropout tanto en la entrada como en la recurrencia (recurrent_dropout) para regularizar el modelo y prevenir el sobreajuste.

**Capas densas:** Después de las capas LSTM, se añaden dos capas densas, una con 64 unidades y activación ReLU, y otra capa de salida con 2 unidades y activación softmax. La capa de salida produce la distribución de probabilidad de la clasificación binaria (positivo o negativo) para cada entrada.

**Compilación del modelo:** Se utiliza el optimizador Adam y la función de pérdida binary_crossentropy, adecuados para problemas de clasificación binaria. La métrica de precisión (accuracy) se utiliza para evaluar el rendimiento del modelo durante el entrenamiento.

Se han elegido estos hiperparámetros concretos para intentar capturar la complejidad de las relaciones semánticas en el texto.

El número de épocas se ha establecido en 40 para permitir suficiente tiempo de entrenamiento y evitar el sobreajuste.

La dimensión de embedding de 100 se considera adecuada para representar eficientemente las palabras en un espacio vectorial denso.

Además, se ha utilizado una técnica de dropout para regularizar el modelo y reducir el sobreajuste.
"""

from keras.callbacks import EarlyStopping
from keras.models import Sequential
from keras.layers import Embedding, LSTM, Dense, Dropout

model = Sequential()

# Capa de Embedding
model.add(Embedding(input_dim=vocab_size,
                    output_dim=emb_dim,
                    input_length=train_text_nn.shape[1]))

# Capa LSTM con dropout para regularización
model.add(LSTM(units=64, dropout=0.5, recurrent_dropout=0.5, return_sequences=True))

# Capa LSTM con dropout para regularización
model.add(LSTM(units=64, dropout=0.5, recurrent_dropout=0.5))

# Capas Densas con dropout para regularización
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.5))

# Capa de Salida
model.add(Dense(2, activation='softmax'))

# Compilar el modelo
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])

# Imprimir resumen del modelo
print(model.summary())

history = model.fit(train_text_nn, Y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2, callbacks=[EarlyStopping(monitor='val_loss', mode='min', patience=7, min_delta=0.01)])
plot_history(history)

"""Los valores de pérdida (loss) y precisión (accuracy) reportados durante el entrenamiento y la validación del modelo muestran:

La pérdida en el conjunto de validación aumenta con el tiempo, lo que indica que el modelo está teniendo dificultades para generalizar a datos no vistos. Esto puede sugerir que el modelo está sobreajustando al conjunto de entrenamiento.

La precisión en el conjunto de validación también disminuye o se estanca, lo que respalda la idea de que el modelo no está generalizando bien a nuevos datos. La precisión en el conjunto de entrenamiento es alta, lo que sugiere, una vez más, un posible sobreajuste.

Dado que la precisión en el conjunto de entrenamiento es alta pero la precisión en el conjunto de validación es baja, se concluye que el modelo se está sobreajustando los datos de entrenamiento.
"""

predict_x=model.predict(test_text_nn)
y_pred=np.argmax(predict_x,axis=1)

# Se deshace la transformación de las predicciones para obtener las predicciones como negativas o positivas
y_pred = le.inverse_transform(y_pred)

print("Classification Report")
print
print(classification_report(y_test, y_pred, target_names=["negative", "positive"]))
cm = confusion_matrix(y_test, y_pred, labels=["negative", "positive"])

plot_confusion_matrix(cm, classes=["negative", "positive"])

print("Final Accuracy")
print(accuracy_score(y_test, y_pred))

"""##Prueba/mejora LSTM

Esta vez, se va a probar otra red neuronal recurrente LSTM, pero con hiperparámetros diferentes.

En este modelo se implementan los siguientes cambios con el objetivo de mejorar el rendimiento y evitar los problemas observados anteriormente:

**Reducción de la tasa de aprendizaje:** Se ha definido un nuevo optimizador Adam con una tasa de aprendizaje más baja (0.0001) en comparación con el valor predeterminado. Esto se hace para permitir que el modelo converja más lentamente y, con suerte, evitar el sobreajuste al proporcionar una mayor estabilidad durante el entrenamiento.

**Eliminación de regularización L2 en las capas densas:** La regularización L2 se ha eliminado esta vez de las capas densas. Esto se hace para simplificar el modelo y evitar una regularización excesiva que podría estar contribuyendo al bajo rendimiento del otro modelo.

**Ajuste de la arquitectura de las capas densas:** Se ha mantenido una capa densa con activación ReLU y se ha agregado una capa de dropout para regularización. Esto permite que el modelo aprenda representaciones más complejas y ayuda a prevenir el sobreajuste al introducir aleatoriedad durante el entrenamiento.

**Optimización de los hiperparámetros de dropout en las capas LSTM:** Se ha mantenido el dropout recurrente en las capas LSTM para regularizar el modelo y evitar el sobreajuste.
"""

from keras.callbacks import EarlyStopping
from keras.models import Sequential
from keras.layers import Embedding, LSTM, Dense, Dropout
from keras.regularizers import l2
from keras.optimizers import Adam

model = Sequential()

# Capa de Embedding
model.add(Embedding(input_dim=vocab_size,
                    output_dim=emb_dim,
                    input_length=train_text_nn.shape[1]))

# Capa LSTM con dropout para regularización
model.add(LSTM(units=64, dropout=0.5, recurrent_dropout=0.5, return_sequences=True))

# Capa LSTM con dropout para regularización
model.add(LSTM(units=64, dropout=0.5, recurrent_dropout=0.5))

# Capas Densas con dropout para regularización y regularización L2
model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.01)))
model.add(Dropout(0.5))

# Capa de Salida
model.add(Dense(2, activation='softmax'))

# Definir un optimizador con una tasa de aprendizaje más baja
optimizer = Adam(learning_rate=0.0001)

# Compilar el modelo con el nuevo optimizador
model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['acc'])

# Imprimir resumen del modelo
print(model.summary())

history = model.fit(train_text_nn, Y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2, callbacks=[EarlyStopping(monitor='val_loss', mode='min', patience=7, min_delta=0.01)])
plot_history(history)

predict_x=model.predict(test_text_nn)
y_pred=np.argmax(predict_x,axis=1)

# Se deshace la transformación de las predicciones para obtener las predicciones como negativas o positivas
y_pred = le.inverse_transform(y_pred)

print("Classification Report")
print
print(classification_report(y_test, y_pred, target_names=["negative", "positive"]))
cm = confusion_matrix(y_test, y_pred, labels=["negative", "positive"])

plot_confusion_matrix(cm, classes=["negative", "positive"])

print("Final Accuracy")
print(accuracy_score(y_test, y_pred))

"""Los resultados obtenidos muestran una mejora significativa, como se ha podido observar mediante plot_history(history).

**Pérdida (loss) reducida:** La pérdida en el conjunto de validación se ha reducido significativamente a lo largo de las épocas. Comenzando desde una pérdida alta de alrededor de 1.25 en la primera época, disminuye gradualmente a aproximadamente 0.67 en la cuarta época y luego se mantiene relativamente estable alrededor de ese valor. Esto indica que el modelo está mejorando su capacidad para generalizar a datos no vistos.

**Precisión (accuracy) mejorada:** La precisión en el conjunto de validación también ha mejorado considerablemente. Comenzando desde alrededor del 49. 6% en la primera época, aumenta constantemente y alcanza aproximadamente el 85.9 % en la décima época. Aunque hay fluctuaciones menores en la precisión después de la décima época, se mantiene en un nivel relativamente alto, indicando que el modelo está haciendo predicciones más precisas sobre los datos de validación.

**Regularización eficaz:** La inclusión de dropout y la regularización L2 en las capas densas y recurrentes LSTM ha ayudado a prevenir el sobreajuste y mejorar la capacidad del modelo para generalizar. Esto se evidencia en la estabilidad de las métricas de pérdida y precisión en el conjunto de validación a lo largo del entrenamiento.

##GRU

El modelo consta de las siguientes capas:

**Capa de embedding:** Se utiliza para transformar los índices de palabras en vectores densos de longitud fija. Esta capa convierte la secuencia de índices de palabras en una secuencia de vectores densos de longitud emb_dim.

**Capa GRU con dropout:** La primera capa GRU se utiliza para procesar la secuencia de vectores densos de palabras generada por la capa de embedding. Esta capa GRU tiene 64 unidades y se aplica dropout para regularizar el modelo y prevenir el sobreajuste.

**Capa GRU con dropout:** La segunda capa GRU también tiene 64 unidades y se aplica dropout para regularizar el modelo.

**Capas densas con dropout:** Después de las capas GRU, se agregan dos capas densas con activación ReLU para aprender representaciones más abstractas de los datos. Se aplica dropout para prevenir el sobreajuste.

**Capa de salida:** La capa de salida consta de dos neuronas con activación softmax, que se utilizan para realizar la clasificación binaria entre las clases positiva y negativa de las reseñas.

El modelo se compila utilizando el optimizador Adam y la función de pérdida binary_crossentropy, ya que se trata de un problema de clasificación binaria. Además, se monitorea la precisión (acc) como métrica de evaluación.
"""

from keras.callbacks import EarlyStopping
from keras.models import Sequential
from keras.layers import Embedding, GRU, Dense, Dropout

model = Sequential()

# Capa de Embedding
model.add(Embedding(input_dim=vocab_size,
                    output_dim=emb_dim,
                    input_length=train_text_nn.shape[1]))

# Capa GRU con dropout para regularización
model.add(GRU(units=64, dropout=0.5, recurrent_dropout=0.5, return_sequences=True))

# Capa GRU con dropout para regularización
model.add(GRU(units=64, dropout=0.5, recurrent_dropout=0.5))

# Capas Densas con dropout para regularización
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.5))

# Capa de Salida
model.add(Dense(2, activation='softmax'))

# Compilar el modelo
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])

# Imprimir resumen del modelo
print(model.summary())

history = model.fit(train_text_nn, Y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2, callbacks=[EarlyStopping(monitor='val_loss', mode='min', patience=7, min_delta=0.01)])
plot_history(history)

"""Los resultados obtenidos durante el entrenamiento del modelo no son satisfactorios. Durante las primeras épocas, la precisión en los datos de validación parece estar mejorando, pero luego se estanca o incluso disminuye, lo que indica un sobreajuste o una incapacidad del modelo para generalizar correctamente a nuevos datos.

En la primera época, la precisión en los datos de entrenamiento es del 66,84%, mientras que en los datos de validación es del 83,13 %.

Sin embargo, en las épocas siguientes, la precisión en los datos de validación disminuye, alcanzando el 84,38 % en la última época, pero aun así, hay una brecha significativa entre la precisión en los datos de entrenamiento y los de validación, lo que sugiere sobreajuste.

El valor de pérdida también muestra un patrón similar: disminuye en las primeras épocas, pero luego aumenta o se mantiene estable, lo que indica que el modelo no está mejorando su capacidad para generalizar a nuevos datos.
"""

predict_x=model.predict(test_text_nn)
y_pred=np.argmax(predict_x,axis=1)

# Se deshace la transformación de las predicciones para obtener las predicciones como negativas o positivas
y_pred = le.inverse_transform(y_pred)

print("Classification Report")
print
print(classification_report(y_test, y_pred, target_names=["negative", "positive"]))
cm = confusion_matrix(y_test, y_pred, labels=["negative", "positive"])

plot_confusion_matrix(cm, classes=["negative", "positive"])

print("Final Accuracy")
print(accuracy_score(y_test, y_pred))

"""##Prueba/mejora GRU

A continuación se realiza una prueba para mejorar los resultados obtenidos:

**Capa de Dropout adicional:** Se ha añadido una capa de Dropout adicional después de las capas GRU para reducir la posibilidad de sobreajuste. El Dropout aleatoriamente "apaga" algunas unidades durante el entrenamiento, lo que ayuda a prevenir que el modelo se vuelva demasiado dependiente de ciertas características.

**Regularización L2 en capas densas:** Se ha introducido regularización L2 en las capas densas para penalizar los pesos grandes y evitar el sobreajuste. Esto ayuda a reducir la complejidad del modelo y promueve coeficientes de peso más pequeños.

**Optimizador con tasa de aprendizaje más baja:** Se ha modificado el optimizador Adam para que tenga una tasa de aprendizaje más baja (0.0001). Una tasa de aprendizaje más baja puede ayudar al modelo a converger de manera más suave y evitar oscilaciones bruscas en la función de pérdida.
"""

from keras.callbacks import EarlyStopping
from keras.models import Sequential
from keras.layers import Embedding, GRU, Dense, Dropout
from keras.optimizers import Adam
from keras.regularizers import l2

model = Sequential()

# Capa de Embedding
model.add(Embedding(input_dim=vocab_size,
                    output_dim=emb_dim,
                    input_length=train_text_nn.shape[1]))

# Capa GRU con dropout para regularización
model.add(GRU(units=64, dropout=0.5, recurrent_dropout=0.5, return_sequences=True))

# Capa GRU con dropout para regularización
model.add(GRU(units=64, dropout=0.5, recurrent_dropout=0.5))

# Capa de Dropout adicional para evitar overfitting
model.add(Dropout(0.5))

# Capas Densas con dropout para regularización
model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.01)))
model.add(Dropout(0.5))

# Capa de Salida
model.add(Dense(2, activation='softmax'))

# Definir un optimizador con una tasa de aprendizaje más baja
optimizer = Adam(learning_rate=0.0001)

# Compilar el modelo con el nuevo optimizador
model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['acc'])

# Imprimir resumen del modelo
print(model.summary())

history = model.fit(train_text_nn, Y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2, callbacks=[EarlyStopping(monitor='val_loss', mode='min', patience=7, min_delta=0.01)])
plot_history(history)

"""Después de realizar las modificaciones en el modelo, se observa una mejora significativa en el rendimiento durante el entrenamiento y la validación:

**Pérdida y precisión:** En comparación con el resultado anterior, la pérdida durante el entrenamiento ha disminuido considerablemente, lo que indica que el modelo está aprendiendo de manera más efectiva. Además, la precisión durante el entrenamiento ha aumentado, lo que indica que el modelo está mejorando en la clasificación de los datos de entrenamiento.

**Validación:** La pérdida y la precisión en los datos de validación también han mejorado en comparación con los resultados anteriores. El modelo entonces generaliza mejor a datos no vistos y es más efectivo en la clasificación de nuevas muestras.

**Tendencia positiva:** Además, la tendencia de la pérdida y la precisión durante las épocas sugiere una mejora continua a lo largo del entrenamiento, lo que indica que el modelo sigue aprendiendo y mejorando su capacidad de generalización.
"""

predict_x=model.predict(test_text_nn)
y_pred=np.argmax(predict_x,axis=1)

# Se deshace la transformación de las predicciones para obtener las predicciones como negativas o positivas
y_pred = le.inverse_transform(y_pred)

print("Classification Report")
print
print(classification_report(y_test, y_pred, target_names=["negative", "positive"]))
cm = confusion_matrix(y_test, y_pred, labels=["negative", "positive"])

plot_confusion_matrix(cm, classes=["negative", "positive"])

print("Final Accuracy")
print(accuracy_score(y_test, y_pred))

"""##Redes recurrentes apiladas

Se ha creado una red neuronal recurrente apilada utilizando capas GRU con dropout para regularización. La decisión de apilar varias capas GRU se basa en varios motivos:

**Mayor capacidad de representación:** Al apilar múltiples capas GRU, la red neuronal puede capturar patrones de mayor complejidad en los datos secuenciales. Cada capa GRU puede aprender representaciones más abstractas de los datos de entrada, lo que potencialmente mejora la capacidad del modelo para modelar relaciones temporales y capturar dependencias a largo plazo en la secuencia de palabras.

**Extracción de características jerárquicas:** Las capas GRU en la arquitectura apilada actúan de manera similar a las capas ocultas en una red neuronal convolucional (CNN) apilada. Al agregar más capas, la red puede aprender a extraer características en múltiples niveles de abstracción, lo que puede ser beneficioso para la tarea de análisis de sentimientos, donde las palabras individuales pueden combinar de maneras complejas para expresar opiniones.

**Regularización y reducción del overfitting:** El uso de dropout recurrente en cada capa GRU ayuda a regularizar el modelo y prevenir el sobreajuste. Esto es crucial cuando se trabaja con datos secuenciales, ya que las redes neuronales recurrentes son propensas al sobreajuste debido a la naturaleza de la propagación hacia atrás a lo largo de la secuencia.
"""

from keras.models import Sequential
from keras.layers import Embedding, GRU, Dense, Dropout

model = Sequential()

# Capa de Embedding
model.add(Embedding(input_dim=vocab_size,
                    output_dim=emb_dim,
                    input_length=train_text_nn.shape[1]))

# Primera capa GRU apilada con dropout para regularización
model.add(GRU(units=64, dropout=0.5, recurrent_dropout=0.5, return_sequences=True))

# Segunda capa GRU apilada con dropout para regularización
model.add(GRU(units=64, dropout=0.5, recurrent_dropout=0.5, return_sequences=True))

# Tercera capa GRU apilada con dropout para regularización
model.add(GRU(units=64, dropout=0.5, recurrent_dropout=0.5))

# Capas Densas con dropout para regularización
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.5))

# Capa de Salida
model.add(Dense(2, activation='softmax'))

# Compilar el modelo
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])

# Imprimir resumen del modelo
print(model.summary())

history = model.fit(train_text_nn, Y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2, callbacks=[EarlyStopping(monitor='val_loss', mode='min', patience=7, min_delta=0.01)])
plot_history(history)

"""Los resultados obtenidos en las últimas épocas del entrenamiento del modelo no son los esperados. Aunque la pérdida en el conjunto de validación se mantiene relativamente baja en las primeras épocas, se observa un aumento notable en la pérdida y una tendencia a la disminución de la precisión en las épocas posteriores."""

predict_x=model.predict(test_text_nn)
y_pred=np.argmax(predict_x,axis=1)

# Se deshace la transformación de las predicciones para obtener las predicciones como negativas o positivas
y_pred = le.inverse_transform(y_pred)

print("Classification Report")
print
print(classification_report(y_test, y_pred, target_names=["negative", "positive"]))
cm = confusion_matrix(y_test, y_pred, labels=["negative", "positive"])

plot_confusion_matrix(cm, classes=["negative", "positive"])

print("Final Accuracy")
print(accuracy_score(y_test, y_pred))

"""##Prueba/mejora redes recurrentes apiladas

A continuación, se presentan las mejoras planteadas para el siguiente modelo:

**Agregación de capas Dropout:** Se han agregado capas Dropout después de cada capa GRU. El Dropout es una técnica de regularización que ayuda a prevenir el sobreajuste al desactivar aleatoriamente un cierto porcentaje de las neuronas durante el entrenamiento. Esto ayuda a que el modelo no dependa demasiado de un subconjunto particular de neuronas y, por lo tanto, mejora su capacidad para generalizar a datos nuevos.

**Reducción de la tasa de aprendizaje:** Se ha reducido la tasa de aprendizaje del optimizador Adam a 0.0001. Reducir la tasa de aprendizaje puede ayudar a suavizar el proceso de optimización y permitir que el modelo converja de manera más efectiva a una solución óptima, especialmente cuando se enfrenta a datos complejos o ruidosos.

**Uso de EarlyStopping:** Se ha definido una instancia de EarlyStopping para monitorear la pérdida en el conjunto de validación. EarlyStopping detiene el entrenamiento del modelo si no hay mejoras en la pérdida del conjunto de validación después de un cierto número de épocas (en este caso, 3 épocas). Esto ayuda a prevenir el sobreajuste al detener el entrenamiento cuando el modelo comienza a sobreajustarse a los datos de entrenamiento.
"""

from keras.models import Sequential
from keras.layers import Embedding, GRU, Dense, Dropout
from keras.optimizers import Adam
from keras.callbacks import EarlyStopping

model = Sequential()

# Capa de Embedding
model.add(Embedding(input_dim=vocab_size,
                    output_dim=emb_dim,
                    input_length=train_text_nn.shape[1]))

# Primera capa GRU apilada con dropout para regularización
model.add(GRU(units=64, dropout=0.5, recurrent_dropout=0.5, return_sequences=True))
model.add(Dropout(0.5))

# Segunda capa GRU apilada con dropout para regularización
model.add(GRU(units=64, dropout=0.5, recurrent_dropout=0.5, return_sequences=True))
model.add(Dropout(0.5))

# Tercera capa GRU apilada con dropout para regularización
model.add(GRU(units=64, dropout=0.5, recurrent_dropout=0.5))
model.add(Dropout(0.5))

# Capas Densas con dropout para regularización
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.5))

# Capa de Salida
model.add(Dense(2, activation='softmax'))

# Reducción de la tasa de aprendizaje del optimizador Adam
optimizer = Adam(learning_rate=0.0001)

# Compilar el modelo
model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['acc'])

# Definir EarlyStopping
early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

# Imprimir resumen del modelo
print(model.summary())

history = model.fit(train_text_nn, Y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2, callbacks=[EarlyStopping(monitor='val_loss', mode='min', patience=7, min_delta=0.01)])
plot_history(history)

"""A pesar de las mejoras realizadas en el modelo, aún observamos una alta pérdida en el conjunto de validación en las últimas épocas. Esto sugiere que el modelo todavía no está generalizando bien a datos nuevos y que podría estar sobreajustando a los datos de entrenamiento.

La alta pérdida en el conjunto de validación indica que el modelo está teniendo dificultades para generalizar a ejemplos que no ha visto durante el entrenamiento.
"""

predict_x=model.predict(test_text_nn)
y_pred=np.argmax(predict_x,axis=1)

# Se deshace la transformación de las predicciones para obtener las predicciones como negativas o positivas
y_pred = le.inverse_transform(y_pred)

print("Classification Report")
print
print(classification_report(y_test, y_pred, target_names=["negative", "positive"]))
cm = confusion_matrix(y_test, y_pred, labels=["negative", "positive"])

plot_confusion_matrix(cm, classes=["negative", "positive"])

print("Final Accuracy")
print(accuracy_score(y_test, y_pred))

"""**Reducción del dropout en las capas GRU:** Se ha reducido la tasa de dropout en las capas GRU a 0.3, así como en la capa recurrent_dropout, lo que puede ayudar a regularizar el modelo de manera más suave y permitir un mejor aprendizaje de los datos.

**Ajuste en la capa densa:** Se ha mantenido una capa densa con 64 unidades y activación ReLU para mantener la capacidad del modelo de aprender características complejas de los datos.

**Cambio en la tasa de dropout en la capa densa:** Se ha mantenido el dropout en la capa densa en 0.5 para continuar regularizando la red y prevenir el sobreajuste.

**Reducción de la tasa de aprendizaje del optimizador Adam:** La tasa de aprendizaje del optimizador Adam se ha reducido a 0.0001 para permitir una convergencia más suave del modelo y reducir la posibilidad de oscilaciones bruscas en la optimización.

##Prueba/mejora redes recurrentes apiladas
"""

from keras.models import Sequential
from keras.layers import Embedding, GRU, Dense, Dropout
from keras.optimizers import Adam
from keras.callbacks import EarlyStopping

model = Sequential()

# Capa de Embedding
model.add(Embedding(input_dim=vocab_size,
                    output_dim=emb_dim,
                    input_length=train_text_nn.shape[1]))

# Primera capa GRU apilada con dropout para regularización
model.add(GRU(units=64, dropout=0.3, recurrent_dropout=0.3, return_sequences=True))

# Segunda capa GRU apilada con dropout para regularización
model.add(GRU(units=64, dropout=0.3, recurrent_dropout=0.3, return_sequences=True))

# Tercera capa GRU apilada con dropout para regularización
model.add(GRU(units=64, dropout=0.3, recurrent_dropout=0.3))

# Capas Densas con dropout para regularización
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.5))

# Capa de Salida
model.add(Dense(2, activation='softmax'))

# Reducción de la tasa de aprendizaje del optimizador Adam
optimizer = Adam(learning_rate=0.0001)

# Compilar el modelo
model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['acc'])

history = model.fit(train_text_nn, Y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2, callbacks=[EarlyStopping(monitor='val_loss', mode='min', patience=3, min_delta=0.01)])
plot_history(history)

"""Los resultados obtenidos hasta el momento podrían no ser los esperados, dado que la pérdida en el conjunto de validación aún no ha alcanzado el nivel deseado, y la precisión tampoco ha alcanzado un nivel óptimo. Aunque se observa una mejora gradual en la precisión a lo largo de las épocas, la pérdida en el conjunto de validación sigue aumentando después de un cierto punto."""

predict_x=model.predict(test_text_nn)
y_pred=np.argmax(predict_x,axis=1)

# Se deshace la transformación de las predicciones para obtener las predicciones como negativas o positivas
y_pred = le.inverse_transform(y_pred)

print("Classification Report")
print
print(classification_report(y_test, y_pred, target_names=["negative", "positive"]))
cm = confusion_matrix(y_test, y_pred, labels=["negative", "positive"])

plot_confusion_matrix(cm, classes=["negative", "positive"])

print("Final Accuracy")
print(accuracy_score(y_test, y_pred))

"""##Redes recurrentes bidireccionales

A continuación, se presenta el modelo de red recurrente bidireccional creado en la siguiente celda:

**Capa de Embedding:** Esta capa convierte los índices de palabras en vectores densos de longitud emb_dim. Estos vectores son entrenables y aprendidos durante el proceso de entrenamiento del modelo.

**Capa bidireccional de GRU:** Se han añadido dos capas bidireccionales de unidades de GRU (Unidad Recurrente Gated) para capturar información tanto de las secuencias pasadas como de las futuras. Cada capa tiene 64 unidades y tiene dropout y dropout recurrente configurados en 0.5 para regularizar el modelo y prevenir el sobreajuste. La primera capa bidireccional devuelve secuencias porque está configurada con return_sequences=True.

**Capas densas con Dropout:** Después de las capas bidireccionales, se agregan dos capas densas. La primera capa densa tiene 64 unidades con activación ReLU y se aplica dropout con una tasa del 0.5 para regularizar el modelo.

**Capa de salida:** La última capa densa tiene 2 unidades con activación softmax, que se usa para la clasificación binaria. Esto produce la salida final del modelo, que representa las probabilidades de pertenecer a cada una de las clases.

**Compilación del modelo:** El modelo se compila utilizando el optimizador Adam y la función de pérdida de entropía cruzada binaria. La métrica de precisión (accuracy) también se registra durante el entrenamiento.
"""

from keras.models import Sequential
from keras.layers import Embedding, Bidirectional, GRU, Dense, Dropout

model = Sequential()

# Capa de Embedding
model.add(Embedding(input_dim=vocab_size,
                    output_dim=emb_dim,
                    input_length=train_text_nn.shape[1]))

# Capa Bidireccional de GRU con dropout para regularización
model.add(Bidirectional(GRU(units=64, dropout=0.5, recurrent_dropout=0.5, return_sequences=True)))

# Capa Bidireccional de GRU con dropout para regularización
model.add(Bidirectional(GRU(units=64, dropout=0.5, recurrent_dropout=0.5)))

# Capas Densas con dropout para regularización
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.5))

# Capa de Salida
model.add(Dense(2, activation='softmax'))

# Compilar el modelo
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])

# Imprimir resumen del modelo
print(model.summary())

history = model.fit(train_text_nn, Y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2, callbacks=[EarlyStopping(monitor='val_loss', mode='min', patience=7, min_delta=0.01)])
plot_history(history)

"""Los resultados muestran que:

**Pérdida de entrenamiento y validación:** Durante el entrenamiento, la pérdida de entrenamiento disminuye gradualmente, lo cual es un buen indicio de que el modelo está aprendiendo de los datos de entrenamiento. Sin embargo, la pérdida de validación se mantiene relativamente alta y no mejora significativamente con el tiempo. Esto sugiere que el modelo no generaliza bien a los datos que no ha visto durante el entrenamiento.

**Precisión de entrenamiento y validación:** La precisión de entrenamiento es alta, lo que indica que el modelo se ajusta bien a los datos de entrenamiento. Sin embargo, la precisión de validación es más baja y no aumenta significativamente a medida que avanza el entrenamiento. Esto sugiere que el modelo no es capaz de generalizar bien a datos nuevos.
"""

predict_x=model.predict(test_text_nn)
y_pred=np.argmax(predict_x,axis=1)

# Se deshace la transformación de las predicciones para obtener las predicciones como negativas o positivas
y_pred = le.inverse_transform(y_pred)

print("Classification Report")
print
print(classification_report(y_test, y_pred, target_names=["negative", "positive"]))
cm = confusion_matrix(y_test, y_pred, labels=["negative", "positive"])

plot_confusion_matrix(cm, classes=["negative", "positive"])

print("Final Accuracy")
print(accuracy_score(y_test, y_pred))

"""##Prueba/mejora redes recurrentes bidireccionales

Para solucionar esto, se llevarán a cabo pruebas con redes recurrentes bidireccionales.

**Capa bidireccional de GRU con dropout:** Se ha utilizado una capa bidireccional de GRU con dropout para regularización en lugar de capas GRU simples. Además, el dropout se utiliza para evitar el sobreajuste al descartar aleatoriamente algunas unidades durante el entrenamiento.

**Dimensionalidad de las capas GRU:** Se ha reducido la dimensionalidad de las capas GRU a 64 y 32 unidades respectivamente. Reducir el número de unidades puede ayudar a controlar la complejidad del modelo y evitar el sobreajuste, especialmente en modelos más profundos.

**Regularización L2 en las capas densas:** Se ha agregado regularización L2 a las capas densas para penalizar los pesos grandes y favorecer modelos más simples. También para ayudar a prevenir el sobreajuste y mejorar la generalización del modelo.

**Compilación del modelo con optimizador Adam:** Se ha utilizado el optimizador Adam para compilar el modelo.
"""

from keras.models import Sequential
from keras.layers import Embedding, Bidirectional, GRU, Dense, Dropout
from keras.regularizers import l2

model = Sequential()

# Capa de Embedding
model.add(Embedding(input_dim=vocab_size,
                    output_dim=emb_dim,
                    input_length=train_text_nn.shape[1]))

# Capa Bidireccional de GRU con dropout para regularización
model.add(Bidirectional(GRU(units=64, dropout=0.3, recurrent_dropout=0.3, return_sequences=True)))

# Capa Bidireccional de GRU con dropout para regularización
model.add(Bidirectional(GRU(units=32, dropout=0.3, recurrent_dropout=0.3)))

# Capas Densas con regularización L2 y dropout para regularización
model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.01)))
model.add(Dropout(0.5))

# Capa de Salida
model.add(Dense(2, activation='softmax'))

# Compilar el modelo
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])

# Imprimir resumen del modelo
print(model.summary())

history = model.fit(train_text_nn, Y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2, callbacks=[EarlyStopping(monitor='val_loss', mode='min', patience=7, min_delta=0.01)])
plot_history(history)

"""**Resultados:**

**Alta pérdida y baja precisión en validación:** La pérdida en el conjunto de validación es relativamente alta y la precisión es baja.

**Pérdida y precisión en entrenamiento y validación:** La pérdida y la precisión en el conjunto de entrenamiento son considerablemente mejores que en el conjunto de validación, lo que confirma la presencia de sobreajuste.

**Estancamiento del rendimiento:** Después de las primeras épocas, no se observa una mejora significativa en la pérdida y la precisión del conjunto de validación. Esto indica que el modelo ha alcanzado un punto de estancamiento en su capacidad de aprendizaje.

**Variación en la precisión de la validación:** Se observa una variación en la precisión del conjunto de validación a lo largo de las épocas. Esto puede ser indicativo de la sensibilidad del modelo a la inicialización de los pesos y la aleatoriedad en la selección de los datos de validación.

**Pérdida creciente en el tiempo:** Aunque la precisión puede fluctuar, la pérdida en el conjunto de validación tiende a aumentar con el tiempo. Esto sugiere que el modelo está teniendo dificultades para generalizar los patrones en los datos de validación.
"""

predict_x=model.predict(test_text_nn)
y_pred=np.argmax(predict_x,axis=1)

# Se deshace la transformación de las predicciones para obtener las predicciones como negativas o positivas
y_pred = le.inverse_transform(y_pred)

print("Classification Report")
print
print(classification_report(y_test, y_pred, target_names=["negative", "positive"]))
cm = confusion_matrix(y_test, y_pred, labels=["negative", "positive"])

plot_confusion_matrix(cm, classes=["negative", "positive"])

print("Final Accuracy")
print(accuracy_score(y_test, y_pred))

"""**Se han realizado varias pruebas utilizando redes recurrentes bidireccionales en un intento de mejorar el rendimiento del modelo. Sin embargo, los resultados obtenidos muestran que estas arquitecturas no lograron mejorar significativamente el desempeño en comparación con modelos unidireccionales.**

A pesar de la inclusión de capas bidireccionales y otras técnicas como la regularización y el ajuste de hiperparámetros, los modelos no lograron superar los problemas de sobreajuste y la incapacidad para generalizar bien a datos no vistos.

## Embeddings preentrenados

En este apartado, se busca realizar una prueba a raíz de un modelo base. Este modelo base que utilizaremos para todas las pruebas es el mismo, es decir, la misma estructura de red neuronal, con la diferencia de que adaptaremos esta arquitectura para cada tipo de red neuronal recurrente que estamos evaluando (LSTM, GRU, redes apiladas y redes bidireccionales).

Esto significa que mantendremos la misma cantidad de capas, la misma cantidad de unidades en cada capa, así como las mismas técnicas de regularización y activación. La única variación entre las pruebas será la arquitectura específica de la red recurrente que estamos empleando.
"""

import os

embeddings_index = {}
# Colab
f = open('/content/drive/MyDrive/glove.6B.50d.txt')

for line in f:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs

f.close()

print('Found %s word vectors.' % len(embeddings_index))
word_index = tokenizer.word_index
print('Found %s unique tokens.' % len(word_index))

emb_dim = 50

num_words = min(n_most_common_words, len(word_index)) + 1

embedding_matrix = np.zeros((num_words, emb_dim))

for word, i in word_index.items():
    if i > n_most_common_words:
        continue
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        # Si la palabra existe añadimos su vector a la matriz
        embedding_matrix[i] = embedding_vector
    else:
        # Si no existe, se le asigna un vector aleatorio
        embedding_matrix[i] = np.random.randn(emb_dim)

"""##LSTM

**Capa de Embedding:** Se utiliza una capa de embedding con pesos iniciales preentrenados. Los pesos se inicializan con la matriz de embeddings pasada como argumento. Esta capa tiene trainable=True, lo que permite ajustar los embeddings durante el entrenamiento.

**Capas LSTM:** Se han añadido dos capas LSTM con dropout recurrente del 50% para regularización. La primera capa devuelve secuencias (return_sequences=True) para proporcionar la salida de cada paso de tiempo a la siguiente capa LSTM.

**Capas densas:** Después de las capas LSTM, se añaden capas densas con activación ReLU y dropout para reducir el sobreajuste. La capa de salida utiliza una función de activación softmax para obtener las probabilidades de pertenencia a cada clase.

El modelo se compila utilizando el optimizador Adam y la función de pérdida de entropía cruzada binaria, ya que se trata de un problema de clasificación binaria. La métrica de precisión (acc) también se supervisa durante el entrenamiento.
"""

from keras.models import Sequential
from keras.layers import Embedding, LSTM, Dense, Dropout
from keras.initializers import Constant
from keras.callbacks import EarlyStopping

model = Sequential()
model.add(Embedding(num_words,
                    emb_dim,
                    embeddings_initializer=Constant(embedding_matrix),
                    input_length=train_text_nn.shape[1],
                    trainable=True))

# Añadir capas LSTM
model.add(LSTM(units=32, dropout=0.5, recurrent_dropout=0.5, return_sequences=True))
model.add(LSTM(units=64, dropout=0.5, recurrent_dropout=0.5))

# Capas densas con activación ReLU y dropout para reducir el sobreajuste
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.5))

model.add(Dense(2, activation='softmax'))

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])

print(model.summary())

history = model.fit(train_text_nn, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.2, callbacks=[EarlyStopping(monitor='val_loss', mode='min', patience=7, min_delta=0.01)])
plot_history(history)

"""Los motivos por los que se ha considerado que pueden surgir estos picos en plot_history(history) son los siguientes:

**Variabilidad de los datos:** Aunque el modelo aprenda patrones generales, la variabilidad inherente en los datos puede causar fluctuaciones en las métricas de rendimiento a lo largo de las épocas.

**Sensibilidad al aprendizaje:** Durante el entrenamiento, el modelo puede encontrar puntos de ajuste que resulten en mejoras temporales o empeoramientos en el rendimiento. Estas fluctuaciones pueden deberse a la sensibilidad del modelo a las diferentes configuraciones de los pesos y los datos de entrenamiento presentados en cada época.

**Regularización y dropout:** La inclusión de técnicas de regularización como el dropout puede hacer que el modelo generalice mejor, pero también puede llevar a una mayor variabilidad en el rendimiento durante el entrenamiento.

**Sobreajuste:** Si el modelo se sobreajusta a los datos de entrenamiento, es probable que veamos fluctuaciones más pronunciadas en las métricas de rendimiento a lo largo del entrenamiento. Estos picos pueden indicar que el modelo está encontrando patrones específicos en los datos de entrenamiento que no se generalizan bien a nuevos datos.
"""

predict_x=model.predict(test_text_nn)
y_pred=np.argmax(predict_x,axis=1)

# Se deshace la transformación de las predicciones para obtener las predicciones como negativas o positivas
y_pred = le.inverse_transform(y_pred)

print("Classification Report")
print
print(classification_report(y_test, y_pred, target_names=["negative", "positive"]))
cm = confusion_matrix(y_test, y_pred, labels=["negative", "positive"])

plot_confusion_matrix(cm, classes=["negative", "positive"])

print("Final Accuracy")
print(accuracy_score(y_test, y_pred))

"""##GRU

En esta nueva configuración del modelo, hemos sustituido las capas LSTM por capas GRU (Gated Recurrent Unit), para ver si con este cambio, los resultado mejoran. Las GRU son una variante simplificada de las LSTM (Long Short-Term Memory), por lo que tienen estructuras internas más simples y, por lo tanto, son más eficientes computacionalmente.

La razón principal para probar esta configuración es evaluar si el cambio de las LSTM a las GRU tiene un impacto en el rendimiento del modelo. A menudo, las GRU son más rápidas de entrenar y requieren menos recursos computacionales, lo que podría permitir un entrenamiento más rápido y, potencialmente, un mejor rendimiento en términos de precisión y estabilidad.

Respecto a los picos mencionados anteriormente, las GRU, al tener menos parámetros que las LSTM, podrían ser menos susceptibles al sobreajuste y, por lo tanto, podrían ayudar a suavizar las fluctuaciones en la pérdida y la precisión durante el entrenamiento. Esto reduciría la presencia de picos.
"""

from keras.models import Sequential
from keras.layers import Embedding, GRU, Dense, Dropout
from keras.initializers import Constant
from keras.callbacks import EarlyStopping

model = Sequential()
model.add(Embedding(num_words,
                    emb_dim,
                    embeddings_initializer=Constant(embedding_matrix),
                    input_length=train_text_nn.shape[1],
                    trainable=True))

# Añadir capas GRU
model.add(GRU(units=32, dropout=0.5, recurrent_dropout=0.5, return_sequences=True))
model.add(GRU(units=64, dropout=0.5, recurrent_dropout=0.5))

# Capas densas con activación ReLU y dropout para reducir el sobreajuste
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.5))

model.add(Dense(2, activation='softmax'))

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])

print(model.summary())

history = model.fit(train_text_nn, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.2, callbacks=[EarlyStopping(monitor='val_loss', mode='min', patience=7, min_delta=0.01)])
plot_history(history)

"""**Resultados de LSTM:**

Epoch 1: train_loss = 0.6678, train_acc = 0.5913, val_loss = 0.6077, val_acc = 0.6819

Epoch 5: train_loss = 0.3958, train_acc = 0.8314, val_loss = 0.6224, val_acc = 0.7744

Epoch 10: train_loss = 0.2088, train_acc = 0.9241, val_loss = 0.5288, val_acc = 0.8256

Epoch 13: train_loss = 0.1391, train_acc = 0.9488, val_loss = 0.6554, val_acc = 0.8075

Epoch 15: train_loss = 0.1395, train_acc = 0.9488, val_loss = 0.5167, val_acc = 0.8537

**Resultados de GRU:**

Epoch 1: train_loss = 0.6917, train_acc = 0.5311, val_loss = 0.6636, val_acc = 0.6000

Epoch 5: train_loss = 0.3953, train_acc = 0.8375, val_loss = 0.4582, val_acc = 0.7937

Epoch 9: train_loss = 0.2734, train_acc = 0.8941, val_loss = 0.3678, val_acc = 0.8531

Epoch 12: train_loss = 0.1883, train_acc = 0.9267, val_loss = 0.3814, val_acc = 0.8606

Epoch 16: train_loss = 0.1215, train_acc = 0.9552, val_loss = 0.4743, val_acc = 0.8706

**Análisis:**

**Pérdida y precisión en el conjunto de entrenamiento (train_loss, train_acc):**

Ambos modelos muestran una mejora en la pérdida y precisión en el conjunto de entrenamiento a medida que avanzan las épocas. La GRU parece tener una mejor precisión en el conjunto de entrenamiento en la mayoría de las épocas.

**Pérdida y precisión en el conjunto de validación (val_loss, val_acc):**

La GRU muestra una tendencia hacia una disminución constante en la pérdida y un aumento en la precisión en el conjunto de validación. Mientras tanto, la LSTM muestra una variación en la pérdida y la precisión en el conjunto de validación.


**En general, la GRU parece tener un mejor desempeño tanto en el conjunto de entrenamiento como en el conjunto de validación en términos de pérdida y precisión. Además, la GRU muestra una tendencia más consistente hacia una mejora en estas métricas a medida que avanzan las épocas en comparación con la LSTM.**
"""

predict_x=model.predict(test_text_nn)
y_pred=np.argmax(predict_x,axis=1)

# Se deshace la transformación de las predicciones para obtener las predicciones como negativas o positivas
y_pred = le.inverse_transform(y_pred)

print("Classification Report")
print
print(classification_report(y_test, y_pred, target_names=["negative", "positive"]))
cm = confusion_matrix(y_test, y_pred, labels=["negative", "positive"])

plot_confusion_matrix(cm, classes=["negative", "positive"])

print("Final Accuracy")
print(accuracy_score(y_test, y_pred))

"""##Apilada

Para esta prueba, se ha implementado una red neuronal recurrente utilizando dos capas apiladas de unidades GRU. Esto se ha construido sobre el modelo base que utilizamos previamente, manteniendo la misma estructura general pero agregando una segunda capa GRU.
"""

from keras.models import Sequential
from keras.layers import Embedding, GRU, Dense, Dropout
from keras.initializers import Constant
from keras.callbacks import EarlyStopping

model = Sequential()
model.add(Embedding(num_words,
                    emb_dim,
                    embeddings_initializer=Constant(embedding_matrix),
                    input_length=train_text_nn.shape[1],
                    trainable=True))

# Primera capa GRU apilada con dropout para regularización
model.add(GRU(units=32, dropout=0.5, recurrent_dropout=0.5, return_sequences=True))

# Segunda capa GRU apilada con dropout para regularización
model.add(GRU(units=64, dropout=0.5, recurrent_dropout=0.5))

# Capas densas con activación ReLU y dropout para reducir el sobreajuste
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.5))

model.add(Dense(2, activation='softmax'))

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])

print(model.summary())

history = model.fit(train_text_nn, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.2, callbacks=[EarlyStopping(monitor='val_loss', mode='min', patience=7, min_delta=0.01)])
plot_history(history)

"""Según se ha visto anteriormente, parece ser que los resultados obtenidos con la arquitectura de red neuronal recurrente apilada (dos capas GRU) son los más prometedores hasta el momento. Esto se puede deber a:

**Mayor capacidad de aprendizaje:** Al tener múltiples capas GRU apiladas, el modelo tiene una mayor capacidad para aprender y capturar patrones complejos en los datos.

**Regularización y reducción del sobreajuste:** El uso de dropout en cada capa GRU ayuda a prevenir el sobreajuste al desactivar aleatoriamente algunas unidades durante el entrenamiento. Esto puede ayudar al modelo a generalizar mejor a datos nuevos y no vistos.
"""

predict_x=model.predict(test_text_nn)
y_pred=np.argmax(predict_x,axis=1)

# Se deshace la transformación de las predicciones para obtener las predicciones como negativas o positivas
y_pred = le.inverse_transform(y_pred)

print("Classification Report")
print
print(classification_report(y_test, y_pred, target_names=["negative", "positive"]))
cm = confusion_matrix(y_test, y_pred, labels=["negative", "positive"])

plot_confusion_matrix(cm, classes=["negative", "positive"])

print("Final Accuracy")
print(accuracy_score(y_test, y_pred))

"""##Bidireccionales

El modelo utilizado es un GRU bidireccional, variante del modelo GRU convencional en la que se utilizan capas GRU en ambas direcciones de la secuencia de entrada. Esto significa que la información se procesa tanto en la dirección original de la secuencia como en la dirección inversa. Esto se logra mediante la adición de dos conjuntos de capas GRU: una capa para procesar la secuencia en orden ascendente y otra capa para procesarla en orden descendente.

Para resumir, este modelo captura patrones de dependencia tanto hacia adelante como hacia atrás en la secuencia de entrada. Esto puede ser especialmente útil en aplicaciones donde la información contextual de toda la secuencia es importante para la tarea en cuestión.
"""

from keras.models import Sequential
from keras.layers import Embedding, Bidirectional, GRU, Dense, Dropout
from keras.initializers import Constant
from keras.callbacks import EarlyStopping

model = Sequential()
model.add(Embedding(num_words,
                    emb_dim,
                    embeddings_initializer=Constant(embedding_matrix),
                    input_length=train_text_nn.shape[1],
                    trainable=True))

# Capa bidireccional GRU con dropout para regularización
model.add(Bidirectional(GRU(units=32, dropout=0.5, recurrent_dropout=0.5, return_sequences=True)))

# Capa bidireccional GRU con dropout para regularización
model.add(Bidirectional(GRU(units=64, dropout=0.5, recurrent_dropout=0.5)))

# Capas densas con activación ReLU y dropout para reducir el sobreajuste
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.5))

model.add(Dense(2, activation='softmax'))

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])

print(model.summary())

history = model.fit(train_text_nn, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.2, callbacks=[EarlyStopping(monitor='val_loss', mode='min', patience=7, min_delta=0.01)])
plot_history(history)

"""En este caso, parece que el modelo bidireccional tiene una precisión ligeramente mayor en comparación con el modelo apilado GRU en términos de métricas de precisión (acc). Sin embargo, al observar las pérdidas tanto en el conjunto de entrenamiento como en el conjunto de validación (train_loss y val_loss), podemos ver que el modelo bidireccional tiende a tener pérdidas más altas, lo que indica que está teniendo dificultades para aprender los datos de entrenamiento y generalizar a los datos de validación.

Aunque la precisión puede ser un indicador importante del rendimiento del modelo, también es indispensable considerar y valorar las pérdidas, ya que proporcionan información sobre la capacidad del modelo para minimizar el error durante el entrenamiento y la generalización.

**En este caso, el modelo apilado GRU parece tener un mejor rendimiento en términos de pérdida, lo que sugiere que está aprendiendo de manera más efectiva los patrones en los datos y generalizando mejor a nuevos ejemplos.**
"""

predict_x=model.predict(test_text_nn)
y_pred=np.argmax(predict_x,axis=1)

# Se deshace la transformación de las predicciones para obtener las predicciones como negativas o positivas
y_pred = le.inverse_transform(y_pred)

print("Classification Report")
print
print(classification_report(y_test, y_pred, target_names=["negative", "positive"]))
cm = confusion_matrix(y_test, y_pred, labels=["negative", "positive"])

plot_confusion_matrix(cm, classes=["negative", "positive"])

print("Final Accuracy")
print(accuracy_score(y_test, y_pred))

"""Aunque el modelo de GRU bidireccional se basa en el mismo principio que el modelo de GRU convencional, la introducción de la bidireccionalidad agrega complejidad al proceso de entrenamiento y puede afectar el rendimiento del modelo de diferentes maneras.

**Mayor complejidad computacional:** Al procesar la secuencia en ambas direcciones, el modelo bidireccional tiene más parámetros que aprender y, por lo tanto, requiere más recursos computacionales y tiempo de entrenamiento.

**Mayor riesgo de sobreajuste:** La bidireccionalidad aumenta la capacidad del modelo para capturar información contextual en la secuencia, lo que puede llevar a un mayor riesgo de sobreajuste, especialmente si el conjunto de datos de entrenamiento es limitado.

**Información futura no disponible durante la predicción:** Durante el entrenamiento, el modelo bidireccional tiene acceso a la información futura de la secuencia, ya que puede procesarla tanto en sentido ascendente como descendente.
Sin embargo, durante la inferencia o predicción, el modelo solo puede procesar la secuencia en una dirección, lo que significa que la información futura no está disponible. Esto podría afectar su capacidad para generalizar a datos no vistos.

# Conclusiones

##BoW y TF-IDF

Tras la realización de varias pruebas con distintos modelos, se puede concluir que usar Bag of Words (BoW) y TF-IDF (Term Frequency-Inverse Document Frequency) como representaciones de texto para entrenar redes neuronales recurrentes (RNN) es muy poco efectivo y eficiente. Esto se debe a distintos motivos, que se mencionan a continuación:

**Perdida de secuencialidad:** BoW y TF-IDF tratan cada palabra de manera independiente y no capturan la estructura secuencial del texto. Esto significa que se pierde información sobre el orden y las dependencias entre palabras en una oración o documento. Las RNN, por otro lado, están diseñadas para trabajar con datos secuenciales y se benefician de esta estructura para capturar relaciones contextuales.

**Dimensión fija de entrada:** Tanto BoW como TF-IDF generan vectores de longitud fija para representar documentos, lo que es problemático cuando se trabaja con datos de longitud variable, como en el caso de texto. Las RNN, por su naturaleza, pueden manejar secuencias de longitud variable y adaptarse a diferentes longitudes de entrada, lo que les permite capturar información más completa y rica.

**Pérdida de información semántica:** **BoW y TF-IDF no capturan la semántica de las palabras ni las relaciones contextuales entre ellas. Estas representaciones tratan todas las palabras como características independientes y no tienen en cuenta su significado en el contexto del texto.** **En contraste, los word embeddings capturan la semántica y el contexto de las palabras en un espacio vectorial de baja dimensionalidad, lo que permite a las RNN aprender representaciones más ricas y significativas del texto.**

**Esparsidad y alta dimensionalidad:** BoW y TF-IDF tienden a generar representaciones dispersas y de alta dimensionalidad, especialmente en conjuntos de datos con un vocabulario grande. Esto puede resultar en modelos RNN que son difíciles de entrenar debido a la alta dimensionalidad de los datos y a la presencia de características poco relevantes o ruido en la representación.

##Word embeddings

**En relación a los word embeddings, se puede concluir que el uso de embeddings, ya sean preentrenados o entrenados junto con el modelo, son la mejor opción en el contexto de entrenamientos de redes neuronales recurrentes (RNN).** Estos capturan la semántica y el contexto de las palabras en un espacio vectorial de baja dimensionalidad, por lo que parten de los mismos principios que las RNN.

**Los resultados obtenidos mediante el uso de word embeddings son satisfactorios, con una accuracy del 85 %. Además, los hiperparámetros se han podido mejorar para evitar en la medida de lo posible distintas problemáticas en el entrenamiento (como el overfitting).**

Además, por las pruebas realizadas, se ha llegado a la conclusión de que la selección adecuada de hiperparámetros es fundamental para lograr un rendimiento óptimo en los modelos de aprendizaje automático. **Sin embargo, debido a limitaciones computacionales, no he podido realizar una búsqueda exhaustiva de hiperparámetros utilizando técnicas como el grid search, que habría sido ideal para seleccionar los mejores hiperparámetros.**

El grid search implica probar sistemáticamente diferentes combinaciones de valores de hiperparámetros, lo que es extremadamente costoso computacionalmente, especialmente en entornos donde no se dispone de acceso a unidades de procesamiento gráfico (GPU) adecuadas.

**En mi caso, la falta de acceso a GPU ha limitado mi capacidad para realizar experimentos eficientes y ha prolongado enormemente los tiempos de entrenamiento al utilizar solo la CPU (para la realización de la gran mayoría de la práctica en Google Colab).**

##Embeddings preentrenados

Como conclusión, **es interesante observar que, a pesar de que el modelo de GRU bidireccional obtuvo la accuracy más alta entre los modelos probados, el entrenamiento que mejor se dio fue con el modelo de GRU apilado.** **Por tanto, es importante destacar que la métrica de accuracy por sí sola puede no ser suficiente para evaluar completamente el rendimiento de un modelo.**

El hecho de que el modelo de GRU apilado haya mostrado un mejor comportamiento durante el entrenamiento indica una mejor capacidad de generalización o una convergencia más rápida hacia un óptimo local durante el proceso de optimización. Aunque la accuracy en el conjunto de validación puede ser alta en el modelo de GRU bidireccional, el comportamiento durante el entrenamiento, como una convergencia más lenta o una mayor variabilidad en la función de pérdida, sugiere que el modelo podría estar teniendo dificultades para aprender y generalizar patrones en los datos de manera efectiva.
"""