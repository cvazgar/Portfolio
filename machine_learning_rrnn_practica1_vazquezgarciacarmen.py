# -*- coding: utf-8 -*-
"""Practica1_VazquezGarciaCarmen.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fJj6kjf_qWm1o-Eal8K4McWC0So5Tn77

#**Práctica 1 de Redes neuronales para el Procesamiento del Lenguaje Natural**

#Tema 1: Introducción


### Alumna: Carmen Vázquez García

Durante esta práctica se ha diseñado un sistema automático para
clasificar reseñas de películas en positivas y negativas. Para esto, se ha recurrido al *dataset* aportado por el profesorado, que consta de 10000 reseñas. Dentro de ellas, 8000 se han utilizado como conjunto de entrenamiento, mientras que las 2000 restantes se reservan para probar el sistema.

Por tanto, para comenzar se comienza con la carga de *dataset* y un análisis de los datos que encontramos, visualizando los posibles problemas, soluciones encontradas, niveles de balanceo...

Luego se procede a la limpieza y preprocesado del texto, para poder dejar el texto preparado para que sea procoesado por los sistemas de aprendizaje automático que se utilizarán posteriormente.

Tras ello, se recurre a estos algoritmos de aprendizaje automático clásico. En este caso, se recurre a una amplia variedad de algoritmos, entre los que se explorarán distintos parámetros para la obtención de mejores resultados.
Para finalizar, se pondrá el foco en los sistemas basados en redes neuronales, donde se explorarán distintos hiperparámetros para la mejora de estas.

En primer lugar se dejarán las librerías necesarias preparadas para su uso, como se muestra a continuación:
"""

# Commented out IPython magic to ensure Python compatibility.
# Librerias necesarias
# Matplotlib conf
import matplotlib.pyplot as plt
# %matplotlib inline
params = {'legend.fontsize': 'x-large',
          'figure.figsize': (15, 5),
         'axes.labelsize': 'x-large',
         'axes.titlesize':'x-large',
         'xtick.labelsize': 40,
         'ytick.labelsize': 40
}
plt.rcParams.update(params)
# Seaborn conf
import seaborn as sns
sns.set(style='darkgrid')
sns.set_palette(sns.color_palette("Blues"))

import sys

# Procesado de datos
import pandas
import numpy as np
import operator

# Modelos
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer
from sklearn.feature_selection import SelectKBest, chi2

from sklearn.metrics import classification_report, accuracy_score
from sklearn.metrics import confusion_matrix

import warnings
warnings.filterwarnings('ignore')

"""## *Dataset*

Para comenzar con el tratamiento de datos, es necesario cargarlos y prepararlos para el entrenamiento. En este caso, como se indica en la práctica, no se utilizarán los datos en local. Recurriremos a los contenidos especificando rutas relativas y no absolutas.
"""

# Mediante estos comandos podemos enlazar nuestro notebook en Colab con nuestro almacenamiento en Google Drive
from google.colab import drive
drive.mount('/content/drive/')

# Cargando el dataset mediante rutas relativas (Colab)
training_set = pandas.read_csv("/content/drive/MyDrive/train_reviews.csv", quotechar='"', header=0, sep=",")
test_set = pandas.read_csv("/content/drive/My Drive/test_reviews.csv", quotechar='"', header=0, sep=",")

"""En este caso no se nos han proporcionado las etiquetas del conjunto de test. Pero más adelante recurriremos a otra forma para poder evaluar nuestros modelos.

Para saber cómo debemos trabajar con el *dataset*, es esencial visualizarlo. Por tanto, a continuación inspeccionamos el conjunto de entrenamiento y el de test.
"""

# Inspeccionamos el aspecto del conjunto de entrenamiento
training_set.head(30)

# Inspeccionamos el aspecto del conjunto de test
test_set.head(30)

"""Como se ha observado, el texto en general está bastante limpio. Aun así, encontramos algunas etiquetas que se deben eliminar, como algunas etiquetas. Esto lo realizaremos una vez hayamos observado que en la columna `sentiment` no hay datos distintos a *negative* y *positive*, paso que veremos a continuación."""

accepted_labels = {"negative", "positive"}
rows_with_problems = training_set[training_set.sentiment.isin(accepted_labels) == False]
rows_with_problems

"""Por lo que hemos podido observar, no se muestra ningún resultado que no sea *negative* o *positive*. Por tanto, no hay ninguna
información en la columna `sentiment` que no sean estos dos valores. Esto deja claro que no tenemos que modificar nada de esta columna.

Tras haber confirmado que no existe ninguna problemática con la columna `sentiment`, procedemos con la limpieza de la columna `review` de ambos conjuntos. Anteriormente vimos que lo único que ensuciaba el contenido eran algunas etiquetas (en este caso, saltos de línea), por lo que las sustituiremos por un espacio.
"""

# Sustituimos las etiquetas de salto de página en la columna 'review' por un espacio
training_set['review'] = training_set['review'].str.replace(r'<br\s*/?>', ' ')

# Creamos un archivo nuevo con el dataframe limpio
training_set.to_csv('training_set_limpio.csv', index=False)

# Inspeccionamos el aspecto del conjunto de entrenamiento limpio
training_set.head(30)

"""A continuación, realizamos exactamente el mismo proceso con la columna `review` de test_set."""

# Reemplazamos las etiquetas de salto de página por un espacio en la columna 'review' del test_set
test_set['review'] = test_set['review'].str.replace(r'<br\s*/?>', ' ')

# Crear un archivo nuevo con el dataframe limpio del test_set
test_set.to_csv('test_set_limpio.csv', index=False)

# Inspeccionamos el aspecto del conjunto de entrenamiento limpio
test_set.head(30)

"""Al volver a visualizar la columna `sentiment` hemos podido observar que las etiquetas se han sustituido por espacios, por lo que ahora podemos juntar ambos sets en un mismo *dataset* para facilitar el preprocesado."""

dataset = pandas.concat([training_set,test_set])

"""# Inspección de los datos

Comenzaremos por observar la distribución de las dos clases (*negative* y *positive*) para ver si existe algún desbalanceo de datos.
"""

from collections import Counter
sns.countplot(data=training_set, x=training_set.sentiment, order=[x for x, count in sorted(Counter(training_set.sentiment).items(), key=lambda x: -x[1])], palette="Blues_r")
plt.xticks(rotation=90)

"""Como hemos podido observar, no hay ningún tipo de desbalanceo. Por tanto, no nos tenemos que preocupar por clases desbalanceadas.

Para seguir con la inspección de datos, vamos a agrupar por sentimiento (positivo o negativo) y concatenar por cada grupo en una sola cadena de texto. Luego, utilizaremos un tokenizador para dividir estas cadenas de texto en palabras. Tras esto, se cuenta la frecuencia de cada palabra y se muestran las 25 palabras más comunes en un gráfico de barras para *negative* y *positive*.
"""

import pandas as pd
from matplotlib import interactive

# Agrupamos los textos por clases
df = pd.DataFrame({"review": training_set.review, "sentiment": training_set.sentiment})
grouped = df.groupby(["sentiment"]).apply(lambda x: x["review"].sum())
grouped_df = pd.DataFrame({"sentiment": grouped.index, "review": grouped.values})

# Tokenizamos las frases utilizando el espacio en blanco como separador
from nltk.tokenize import WhitespaceTokenizer
tokenizer = WhitespaceTokenizer()

for ii, review in enumerate(grouped_df.review):
    pd.DataFrame(tokenizer.tokenize(review)).apply(pd.value_counts).head(25).plot(kind="bar")
    plt.title(grouped_df.sentiment[ii], fontsize=20)
    plt.xticks(fontsize=15)
    interactive(True)
    plt.show()

"""Al observar las palabras más repetidas, lo que se podía esperar era encontrar términos representativos que nos ayudaran a distinguir entre reseñas positivas y negativas. Sin embargo, notamos que hay conjuntos de palabras muy similares que aportan poca información en ambas categorías, las ***stopwords***. Su presencia afecta negativamente el rendimiento del clasificador, por lo que es necesario eliminarlas. De hecho, debido a estas palabras, nos cuesta percibir la diferencia entre reseñas negativas y positivas, ya que prácticamente lo único que encontramos son estas palabras que no aportan información relevante.

También notamos que algunas palabras se consideran diferentes simplemente por la diferencia de mayúsculas y minúsculas, como *The* y *the*. Para abordar este problema, debemos **normalizar** las palabras.

Además, encontramos variantes de un mismo verbo, como *are* e *is*. Para lidiar con esto, podemos aplicar técnicas como el ***stemming*** o la **lematización** para reducir las palabras a su forma base y asegurarnos de que se traten de manera coherente en el análisis de texto.

## Preprocesado y preparación de datos


Comenzaremos con un preprocesado básico del *pipeline* de PLN
 - Extracción de palabras
 - Lematización para evitar que palabras con la misma raíz se repitan
 - *Stemming* para agrupar palabras con el mismo significado
 - Eliminación de *stopwords*

Para ello, vamos a crear la función **process_text** que engloba todo el preprocesado del texto. En este caso, utilizaremos la librería NLTK.
"""

from nltk.stem import *
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords

import re

import nltk
nltk.download('stopwords')
nltk.download('wordnet')

def process_text(raw_text):
    # Consideramos únicamente letras utilizando una expresión regular
    letters_only = re.sub("[^a-zA-Z]", " ",raw_text)
    # Convertimos todo a minúsculas
    words = letters_only.lower().split()

    # Eliminamos las stopwords
    stops = set(stopwords.words("english"))
    not_stop_words = [w for w in words if not w in stops]

    # Lematización
    wordnet_lemmatizer = WordNetLemmatizer()
    lemmatized = [wordnet_lemmatizer.lemmatize(word) for word in not_stop_words]

    # Stemming
    stemmer = PorterStemmer()
    stemmed = [stemmer.stem(word) for word in lemmatized]

    return( " ".join( stemmed ))

"""Ahora que disponemos de la función, la aplicamos al contenido textual del dataset: `review`. Para mantener el contenido original, creamos una nueva columna."""

dataset['clean_review'] = dataset['review'].apply(lambda x: process_text(x))
dataset.head()

"""A continuación, visualizamos lo que tenemos hasta el momento:"""

df = pd.DataFrame({"review": dataset[0:len(training_set)].clean_review, "sentiment": dataset[0:len(training_set)].sentiment})
grouped = df.groupby(["sentiment"]).apply(lambda x: x["review"].sum())
grouped_df = pd.DataFrame({"sentiment": grouped.index, "review": grouped.values})

from nltk.tokenize import WhitespaceTokenizer
tokenizer = WhitespaceTokenizer()

for ii, text in enumerate(grouped_df.review):
    pd.DataFrame(tokenizer.tokenize(text)).apply(pd.value_counts).head(25).plot(kind="bar")
    plt.title(grouped_df.sentiment[ii], fontsize=20)
    plt.xticks(fontsize=15)
    interactive(True)
    plt.show()

"""Como se puede apreciar en ambos gráficos, ha cambiado bastante. Hemos eliminado las *stopwords*, por lo que las palabras que observamos ahora son mucho más representativas. Además, se aprecia un uso más distintivo de algunas palabras entre las reseñas positivas y negativas, ya que antes debido a las *stopwords* el contenido era prácticamente el mismo.

Por ejemplo, aunque en las reseñas negativas también encontramos el adjetivo *good*, podemos ver que hay bastante uso también de *bad*, como podíamos esperar en la reseña negativa. Sin embargo, en el caso de las reseñas positivas, no encotramos *bad*. Lo que predomina es el adjetivo *good*, al ser reseñas positivas.

**Para continuar, vamos a divir nuestro *dataset* de nuevo para obtener el conjunto de entrenamiento y el de test, que se ha realizado de la siguiente forma:**

**X_train:** Se coge desde el inicio (índice 0) hasta el índice que representa el final del conjunto de entrenamiento (denotado como len(training_set)). Este subconjunto de datos consiste en las columnas `id` y `clean_review` del conjunto de entrenamiento, que se utilizan como características de entrada para el modelo.

**y_train:** Al igual que X_train, se toma una porción del *dataset* desde el inicio hasta el final del conjunto de entrenamiento, pero esta vez se extraen solo las etiquetas de clase `sentiment` correspondientes a las reseñas en el conjunto de entrenamiento.

**X_test:** Para el conjunto de prueba, se toma una porción de dataset que va desde el final del conjunto de entrenamiento (índice len(training_set)) hasta el final de *dataset*. Esto incluye las mismas columnas que en X_train, pero corresponden a las reseñas del conjunto de prueba.

**y_test:** Al igual que X_test, se coge desde el final del conjunto de entrenamiento hasta el final del *dataset* y se extraen las etiquetas de clase `sentiment` correspondientes a las reseñas en el conjunto de prueba.
"""

X_train = dataset[0:len(training_set)][["id", "clean_review"]]
y_train = dataset[0:len(training_set)][["sentiment"]]
X_test = dataset[len(training_set):len(dataset)][["id", "clean_review"]]
y_test = dataset[len(training_set):len(dataset)][["sentiment"]]

"""Las etiquetas son *strings*, pero es preferible tenerlas codificadas numéricamente. Para ello, utilizamos la utilidad `LabelEncoder` de sklearn."""

# Hot encoding para sentiment
from sklearn import preprocessing
le = preprocessing.LabelEncoder()
le.fit(y_train.sentiment.values)
target_sentiment = le.classes_
encoded_y_train = le.transform(y_train.sentiment.values)

"""**Modelo de espacio vectorial**

Vamos a aplicar un modelo de espacio vectorial para representar documentos de texto. Utilizaremos la técnica de Bolsa de Palabras (BoW), que implica convertir el texto en vectores numéricos para facilitar su procesamiento por algoritmos de clasificación.

En este enfoque:


1.   Asignamos un identificador a cada palabra en nuestro conjunto de datos.
2.   Creamos una matriz llamada «document-term» utilizando CountVectorizer de sklearn. Esta matriz, denotada como X[i, j], representa la frecuencia de ocurrencia de cada palabra w en cada documento d. Cada fila i corresponde a un documento, y cada columna j a una palabra en el diccionario.

Al aplicar CountVectorizer a las reseñas limpias, obtenemos las características necesarias para ambos conjuntos de clases, negativas y positivas. Este enfoque nos permite convertir el contenido textual en información numérica estructurada que se pueda aprovechar por algoritmos de clasificación.
"""

count_vect = CountVectorizer(analyzer = "word") # set min_df = frequency

train_features = count_vect.fit_transform(X_train.clean_review)
test_features = count_vect.transform(X_test.clean_review)

"""Aunque el tamaño del vocabulario es muy grande, podemos inspeccionar hasta cierto punto los vectores que obtenemos tras realizar este paso:"""

# Observemos el vocabulario
print('Longitud del vocabulario: ')
print(len(count_vect.vocabulary_))
print()
print("Observemos el vocabulario:")
print(count_vect.vocabulary_)
print()

"""Transformamos uno de los datos de entrada en el vector de palabras que se va a utilizar como entrada en nuestros experimentos:"""

# Ejemplo original (primer título)
print("Original:")
print(dataset.review.head()[0])
print()

# Ejemplo tras el preprocesado
print("Preprocesado:")
print(X_train.clean_review[0])
print()

# Convertimos el primer título
v0 = count_vect.transform([X_train.clean_review[0]]).toarray()[0]
print('Convertido: ')
print(v0)
print()

# Es demasiado grande para observarlo, imprimimos su longitud
print('Longitud del vector: ')
print(len(v0))
print()

# ¿Cuántas palabras contiene la frase?
print('Número de palabras: ')
print(np.sum(v0))
print()

# ¿En qué posiciones del vector se encuentran las palabras?
result = np.where(v0 == 1)
print("Índices de las palabras")
print(result[0], sep='\n')
print()

# Recuperemos la frase original
print('Original:')
print(count_vect.inverse_transform(v0.reshape(1, -1)))
print()

"""Se observa cómo, partiendo de la frase original «People tried to make me believe that the premise [...]», tras el preprocesado se obtienen los tokens «peopl tri make believ premis [...]» (se ha normalizado, eliminado *stopwords* y se ha aplicado lematización y *stemming* a las palabras).

El vector resultante tras la transformación se compone de 31106 valores (tantos como palabras en el vocabulario), de las cuáles 271 serán 1, y el resto 0. Al tener tantas componentes ni siquiera podemos observar dónde se localizan los valores 1, ya que se trata de un vector one-hot muy disperso. Se pueden recuperar fácilmente las palabras originales para comprobar que el vector efectivamente representa la frase original.
Ahora extraemos las *features* del texto limpio de las reseñas.
"""

train_text_features = count_vect.fit_transform(X_train.clean_review)
test_text_features = count_vect.transform(X_test.clean_review)

"""Ya tenemos nuestro *dataset* en el formato deseado, por lo que podemos comenzar a crear nuestros clasificadores.

A continuación se muestran algunas funciones para entrenar, evaluar y comparar los modelos que se creen:

*   Una función `train_and_evaluate_classifier` que utiliza los datos de entrenamiento, la definición de un clasificador específico, y una cuadrícula con los parámetros del clasificador para optimizarlo (grid search).
*   Una función `plot_learning_curve` que proporciona la curva de aprendizaje del algoritmo, así como la puntuación obtenida tras la validación cruzada. A esta función se le llama desde la función anterior.
*   Una función `plot_confusion_matrix` para dibujar la matriz de confusión relativa a las predicciones realizadas sobre el conjunto de test.




"""

import numpy as np
from sklearn.model_selection import learning_curve

def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,
                        train_sizes=np.linspace(.1, 1.0, 5)):
    """
    Generate a simple plot of the test and traning learning curve.

    Parameters
    ----------
    estimator : object type that implements the "fit" and "predict" methods
        An object of that type which is cloned for each validation.

    title : string
        Title for the chart.

    X : array-like, shape (n_samples, n_features)
        Training vector, where n_samples is the number of samples and
        n_features is the number of features.

    y : array-like, shape (n_samples) or (n_samples, n_features), optional
        Target relative to X for classification or regression;
        None for unsupervised learning.

    ylim : tuple, shape (ymin, ymax), optional
        Defines minimum and maximum yvalues plotted.

    cv : integer, cross-validation generator, optional
        If an integer is passed, it is the number of folds (defaults to 3).
        Specific cross-validation objects can be passed, see
        sklearn.cross_validation module for the list of possible objects
    """

    plt.figure()
    train_sizes, train_scores, test_scores = learning_curve(
        estimator, X, y, cv=5, n_jobs=1, train_sizes=train_sizes)
    train_scores_mean = np.mean(train_scores, axis=1)
    train_scores_std = np.std(train_scores, axis=1)
    test_scores_mean = np.mean(test_scores, axis=1)
    test_scores_std = np.std(test_scores, axis=1)

    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,
                     train_scores_mean + train_scores_std, alpha=0.1,
                     color="r")
    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,
                     test_scores_mean + test_scores_std, alpha=0.1, color="g")
    plt.plot(train_sizes, train_scores_mean, 'o-', color="r",
             label="Training score")
    plt.plot(train_sizes, test_scores_mean, 'o-', color="g",
             label="Cross-validation score")

    plt.xlabel("Training examples")
    plt.ylabel("Score")
    plt.legend(loc="best")
    plt.grid("on")
    if ylim:
        plt.ylim(ylim)
    plt.title(title)

def train_and_evaluate_classifier(X, yt, estimator, grid):
    """Train and Evaluate a estimator (defined as input parameter) on the given labeled data using accuracy."""

    # Se realiza validación cruzada sobre el clasificador seleccionado
    from sklearn.model_selection import ShuffleSplit
    cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)

    from sklearn.model_selection import GridSearchCV
    classifier = GridSearchCV(estimator=estimator, cv=cv,  param_grid=grid, error_score=0.0, n_jobs = -1, verbose = 5)

    # Entrenamiento
    print("Training model")
    classifier.fit(X, yt)

    # Puntuación de la validación cruzada
    from sklearn.model_selection import cross_val_score
    print("CV-scores for each grid configuration")
    means = classifier.cv_results_['mean_test_score']
    stds = classifier.cv_results_['std_test_score']
    for mean, std, params in sorted(zip(means, stds, classifier.cv_results_['params']), key=lambda x: -x[0]):
        print("Accuracy: %0.3f (+/-%0.03f) for params: %r" % (mean, std * 2, params))
    print()

    # Curva de aprendizaje
    print("Learning curve for the best estimator")
    title = 'Learning Curves (' + str(estimator.__class__).replace("'>", "").split(".")[-1] + ' )'
    plot_learning_curve(classifier.best_estimator_, title, X, yt, cv=cv)
    plt.show()

    return classifier

def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    import itertools
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')
    plt.figure()
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.show()

"""## Primer clasificador: Naïve Bayes

Antes de recurrir a la paliación directa de redes neuronales para abordar un problema, es muy interesante explorar alternativas más simples en el ámbito de algoritmos de aprendizaje automático. Estos métodos menos complejos pueden ser adecuados y eficaces, por lo que se puede evitar la necesidad de asignar recursos significativos que suelen demandar las redes neuronales. Cuando los conjuntos de datos no son considerablemente extensos o cuando se dispone de recursos limitados, la experimentación con enfoques más simples puede proporcionar soluciones eficaces sin incurrir en la carga computacional y de recursos asociada a las redes neuronales.

En este contexto, y considerando las razones mencionadas, se llevará a cabo la exploración de diversos algoritmos de aprendizaje automático antes de recurrir directamente a las redes neuronales. Esta estrategia nos permitirá evaluar la idoneidad y eficacia de métodos más simples. En consecuencia, en este caso, se realizarán pruebas con distintos sistemas y enfoques para determinar la mejor aproximación al problema antes de considerar el uso de redes neuronales.

Comenzamos aplicamos Naïve Bayes para realizar clasificación textual. Tal y como se ha visto, necesitamos calcular la máxima probabilidad a priori (Maximum Apriori Probability o MAP) para predecir la clase a la que pertenece un documento dado.

### Texto de la noticia

En este caso utilizaremos la columna `clean_review` para generar las *features*:
"""

from sklearn.naive_bayes import MultinomialNB

# En este caso no hace falta optimizar ningún parámetro
nb_grid= {}

nb_text_cls = train_and_evaluate_classifier(train_text_features, encoded_y_train, MultinomialNB(), nb_grid)

print("Predicting on the test set")
y_pred = nb_text_cls.predict(test_features)

# Se deshace la transformación de las predicciones para obtener las predicciones como negativas o positivas
y_pred = le.inverse_transform(y_pred)

print("Classification Report")
print
print(classification_report(y_test, y_pred, target_names=["negative", "positive"]))
cm = confusion_matrix(y_test, y_pred, labels=["negative", "positive"])

plot_confusion_matrix(cm, classes=["negative", "positive"])

print("Final Accuracy")
print(accuracy_score(y_test, y_pred,))

"""Se puede observar que, utilizando un algoritmo sencillo como Naïve Bayes, obtenemos un 84 % de *accuracy*. Este resultado es bastante bueno para empezar.

Como queremos mejorar estos resultados, utilizaremos otras metodologías de algoritmos más complejos con más información, que nos ayudarán a ello.

## Segundo clasificador: SVM

Las máquinas de soporte vectorial (SVM) son especialmente eficaces en la clasificación de texto y tienden a generar modelos más precisos.
A través de la biblioteca sklearn, procederemos a implementar un clasificador SVM con el objetivo de evaluar si podemos mejorar los resultados obtenidos con Naïve Bayes en la tarea de clasificación de reseñas negativas y positivas.

Utilizamos la función `train_and_evaluate_classifier`, donde sí que existen parámetros para especificar el *grid search*, a diferencia de Naïve Bayes:
"""

# SVM
from sklearn.svm import SVC

svm_grid = [
  {'C': [1, 10, 100], 'kernel': ['linear']},
  {'C': [10, 100, 1000], 'gamma': [0.0001], 'kernel': ['rbf']},
 ]

svm_cls = train_and_evaluate_classifier(train_text_features, encoded_y_train, SVC(), svm_grid)

"""Obtenemos el mejor clasificador y lo utilizamos para predecir los datos de test:"""

print("Best estimator found by grid search:")
print(svm_cls.best_estimator_)

print("Predicting sentiment on the test set")
y_pred = svm_cls.predict(test_text_features)

# Se deshace la transformación de las predicciones para obtener las predicciones como negativas o positivas
y_pred = le.inverse_transform(y_pred)

print("Classification Report")
print
print(classification_report(y_test, y_pred, target_names=["negative", "positive"]))
cm = confusion_matrix(y_test, y_pred, labels=["negative", "positive"])

plot_confusion_matrix(cm, classes=["negative", "positive"])

print("Final Accuracy")
print(accuracy_score(y_test, y_pred,))

"""**Se han realizado pruebas de SVM con diferentes tipos de kernel (rbf, lineal, polinómico y sigmoide) y diferentes valores de parámetro C y gamma para cada tipo de kernel. El principal problema es que el coste computacional es bastante mayor de tiempo y recursos. Este ejemplo probado tardó unos 20 minutos, pero al con otros valores y tipos de kernel, el tiempo estimado ha sido superior a una hora. Por tanto, aunque haya realizado varias pruebas, solo me he quedado con la que proporcionaba mejores resultados y que era sencilla.**"""

# SVM
from sklearn.svm import SVC

svm_grid = [
  {'C': [1, 10, 100], 'kernel': ['linear']},
  {'C': [10, 100, 1000], 'gamma': [0.001], 'kernel': ['rbf']},
]

svm_cls = train_and_evaluate_classifier(train_text_features, encoded_y_train, SVC(), svm_grid)

print("Best estimator found by grid search:")
print(svm_cls.best_estimator_)

print("Predicting sentiment on the test set")
y_pred = svm_cls.predict(test_text_features)

# Se deshace la transformación de las predicciones para obtener las predicciones como negativas o positivas
y_pred = le.inverse_transform(y_pred)

print("Classification Report")
print
print(classification_report(y_test, y_pred, target_names=["negative", "positive"]))
cm = confusion_matrix(y_test, y_pred, labels=["negative", "positive"])

plot_confusion_matrix(cm, classes=["negative", "positive"])

print("Final Accuracy")
print(accuracy_score(y_test, y_pred,))

"""La diferencia en los valores de gamma en el kernel rbf afecta la forma en que el modelo SVM interpreta la influencia de los puntos de datos en el espacio de características.

Un valor más bajo de gamma (como 0.0001) hace que el modelo considere un área más amplia alrededor de cada punto de datos para determinar la frontera de decisión. Esto puede conducir a un modelo más suave y menos propenso al sobreajuste.

Un valor más alto de gamma (como 0.001) hace que el modelo se centre más en los puntos de datos individuales, lo que puede llevar a una frontera de decisión más ajustada y compleja. Esto podría dar lugar a un modelo más ajustado a los datos de entrenamiento, pero podría ser propenso a sobreajuste si se usa en exceso.

Se observa un mejor rendimiento con un valor de gamma de 0.001, que se acerca al 87 % de *accuracy*. Continuaremos para ver si podemos conseguir un mejor resultado con la regresión logística.

En resumen, se comprueba que el clasificador SVM es más adecuado que Naïve Bayes para clasificación textual.

# Tercer clasificador: MaxEnt

Como siguiente clasificador probabilístico recurrimos a Máxima Entropía (MaxEnt).
En primer lugar, especificamos los parámetros a explorar en nuestro *grid search*.
"""

from sklearn.linear_model import LogisticRegression
max_ent_grid= {'penalty': ['l1','l2'], 'C': [0.001,0.01,0.1,1,10,100,1000]}
max_ent_cls = train_and_evaluate_classifier(train_text_features, encoded_y_train, LogisticRegression(), max_ent_grid)

print("Best estimator found by grid search:")
print(max_ent_cls.best_estimator_)

"""Seleccionamos el mejor clasificador y lo utilizamos para predecir las etiquetas del conjunto de test."""

print("Predicting sentiment on the test set")
y_pred = max_ent_cls.predict(test_text_features)

# Se deshace la transformación de las predicciones para obtener las predicciones como negativas o positivas
y_pred = le.inverse_transform(y_pred)

print("Classification Report")
print
print(classification_report(y_test, y_pred, target_names=["negative", "positive"]))
cm = confusion_matrix(y_test, y_pred, labels=["negative", "positive"])

plot_confusion_matrix(cm, classes=["negative", "positive"])

print("Final Accuracy")
print(accuracy_score(y_test, y_pred))

"""**Hemos realizado pruebas con distintos hiperparámetros, pero los resultados han sido los mismos o peores. Aquí encontramos una muestra de ejemplo:**"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

max_ent_grid = {
    'penalty': ['l1', 'l2'],
    'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],
    'solver': ['liblinear', 'newton-cg', 'lbfgs'],
    'max_iter': [100, 200]
}

max_ent_cls = train_and_evaluate_classifier(train_text_features, encoded_y_train, LogisticRegression(), max_ent_grid)

print("Best estimator found by grid search:")
print(max_ent_cls.best_estimator_)

print("Predicting sentiment on the test set")
y_pred = max_ent_cls.predict(test_text_features)

# Se deshace la transformación de las predicciones para obtener las predicciones como negativas o positivas
y_pred = le.inverse_transform(y_pred)

print("Classification Report")
print
print(classification_report(y_test, y_pred, target_names=["negative", "positive"]))
cm = confusion_matrix(y_test, y_pred, labels=["negative", "positive"])

plot_confusion_matrix(cm, classes=["negative", "positive"])

print("Final Accuracy")
print(accuracy_score(y_test, y_pred))

"""**Podemos concluir que:**

**El primer código especifica dos parámetros: 'penalty' (que controla la norma de penalización, puede ser 'l1' o 'l2') y 'C' (que regula la fuerza de la regularización).**

**El segundo código introduce dos parámetros adicionales:**
* **solver: Este hiperparámetro especifica el algoritmo que se utilizará para resolver el problema de optimización. En el primer bloque, no se especifica, por lo que se usa el valor predeterminado ('liblinear' para la versión actual de scikit-learn). En el segundo bloque, se está ajustando a tres solucionadores diferentes: 'liblinear', 'newton-cg', y 'lbfgs'. Aunque cada solucionador tiene sus propias características y es más apropiado para ciertos tipos de datos y problemas, en este caso no se ha observado ninguna mejora de *accuracy*.**
* **max_iter (número máximo de iteraciones): Este hiperparámetro establece el número máximo de iteraciones para la convergencia del algoritmo. Puede ser importante ajustarlo, especialmente si el modelo no converge con el número predeterminado de iteraciones. En el segundo bloque, se está explorando dos valores posibles: 100 y 200. Tampoco obtenemos una mejoría.**

**En ambos códigos, 'penalty' tiene los mismos valores ('l1' y 'l2') y 'C' especifica la misma serie de valores.**

**En el segundo código, se proporcionan valores específicos para 'solver' ('liblinear', 'newton-cg', 'lbfgs') y 'max_iter' (100, 200), que no están presentes en el primer código.**

**'penalty': Controla el tipo de regularización a aplicar ('l1' o 'l2').**
**'C': Inversa de la fuerza de regularización; valores más pequeños especifican una regularización más fuerte.**

**En esta tarea en concreto, el clasificador de máxima entropía supera mínimamente al SVM, obteniendo un *accuracy* del 86 %**

Este tipo de modelos son muy recomendables para clasificación textual. Además, son mucho más rápidos de entrenar que los SVM. La diferencia de *accuracy* no es tan amplia, por lo que deberíamos valorar qué merece más la pena, si recurrir a SVM o a máxima entropía.

### Cuarto clasificador: Random Forest

Random Forest es un algoritmo de aprendizaje automático supervisado que se utiliza tanto para tareas de clasificación como de regresión. Es una técnica de conjunto que combina múltiples árboles de decisión para tomar decisiones más precisas y estables.
A continuación ajustaremos una configuración de hiperparámetros algo pequeña, pero que puede servir para la tarea.
"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

# Definir hiperparámetros
rf_grid = {
    'n_estimators': [100, 200],
    'max_depth': [None, 10],
    'min_samples_split': [2, 5],
}

# Crear y ajustar el clasificador utilizando GridSearchCV
rf_cls = RandomForestClassifier()
grid_search_rf = GridSearchCV(rf_cls, rf_grid, cv=5)
grid_search_rf.fit(train_text_features, encoded_y_train)

# Imprimir los mejores hiperparámetros encontrados
print("Best hyperparameters found:")
print(grid_search_rf.best_params_)

# Obtener el mejor modelo
best_rf_cls = grid_search_rf.best_estimator_

# Realizar predicciones en el conjunto de prueba
y_pred = best_rf_cls.predict(test_text_features)

# Convertir las predicciones numéricas a etiquetas de texto
y_pred_labels = ["negative" if pred == 0 else "positive" for pred in y_pred]

# Imprimir el informe de clasificación
print("Classification Report")
print(classification_report(y_test, y_pred_labels, target_names=["negative", "positive"]))

# Calcular la matriz de confusión y mostrarla
cm = confusion_matrix(y_test, y_pred_labels, labels=["negative", "positive"])

plot_confusion_matrix(cm, classes=["negative", "positive"])

# Calcular y mostrar la precisión final
print("Final Accuracy")
print(accuracy_score(y_test, y_pred_labels))

"""El resultado a priori está bastante bien, por lo que realicé distintas pruebas con varios hiperparámetros, pero los resultados no mejoraron. Una muestra de prueba sería la siguiente:"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

# Definir hiperparámetros
rf_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10],
}

# Crear y ajustar el clasificador utilizando GridSearchCV
rf_cls = RandomForestClassifier()
grid_search_rf = GridSearchCV(rf_cls, rf_grid, cv=5)
grid_search_rf.fit(train_text_features, encoded_y_train)

# Imprimir los mejores hiperparámetros encontrados
print("Best hyperparameters found:")
print(grid_search_rf.best_params_)

# Obtener el mejor modelo
best_rf_cls = grid_search_rf.best_estimator_

# Realizar predicciones en el conjunto de prueba
y_pred = best_rf_cls.predict(test_text_features)

# Convertir las predicciones numéricas a etiquetas de texto
y_pred_labels = ["negative" if pred == 0 else "positive" for pred in y_pred]

# Imprimir el informe de clasificación
print("Classification Report")
print(classification_report(y_test, y_pred_labels, target_names=["negative", "positive"]))

# Calcular la matriz de confusión y mostrarla
cm = confusion_matrix(y_test, y_pred_labels, labels=["negative", "positive"])

plot_confusion_matrix(cm, classes=["negative", "positive"])

# Calcular y mostrar la precisión final
print("Final Accuracy")
print(accuracy_score(y_test, y_pred_labels))

"""**La primera prueba está explorando un espacio de hiperparámetros que incluye la cantidad de árboles en el bosque (n_estimators), la profundidad máxima de los árboles (max_depth), y el número mínimo de muestras requeridas para dividir un nodo (min_samples_split).**

**La segunda prueba tiene una cuadrícula de búsqueda de hiperparámetros más extensa que incluye parámetros adicionales como min_samples_leaf y bootstrap. Además, incluye más valores posibles para los hiperparámetros, proporcionando así una búsqueda más exhaustiva en el espacio de hiperparámetros**

**Los resultados no son mejores (no llega al 85 %), además de que la mejora ha sido mínima para el gasto computacional que tiene (que es bastante alto al proporcionar más hiperparámetros).**

### Quinto clasificador: XGBoost
"""

# Importa las bibliotecas necesarias
from xgboost import XGBClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from itertools import product  # Importa la función product desde itertools

# Definir la función para mostrar la matriz de confusión
def plot_confusion_matrix(cm, classes, title="Confusion Matrix"):
    import matplotlib.pyplot as plt
    import numpy as np

    plt.figure()
    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    fmt = 'd'
    thresh = cm.max() / 2.
    for i, j in product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.tight_layout()

# Entrenamiento y evaluación del modelo XGBoost
xgb_model = XGBClassifier()  # Puedes ajustar los hiperparámetros según sea necesario

# Entrena el modelo
xgb_model.fit(train_text_features, encoded_y_train)

print("Predicting on the test set")
y_pred = xgb_model.predict(test_features)

# Invierte la transformación de las predicciones para obtener las predicciones como "negative" o "positive"
y_pred = le.inverse_transform(y_pred)

# Muestra el informe de clasificación
print("Classification Report")
print(classification_report(y_test, y_pred, target_names=["negative", "positive"]))

# Matriz de confusión
cm = confusion_matrix(y_test, y_pred, labels=["negative", "positive"])

# Llama a la función para mostrar la matriz de confusión
plot_confusion_matrix(cm, classes=["negative", "positive"])

print("Final Accuracy")
print(accuracy_score(y_test, y_pred))

"""Hemos obtenido resultados mejores con otros clasificadores, por lo que en este caso, nos quedaríamos con los anteriores y no seguiríamos explorando.

### Sexto clasificador: Gradient Boosting
"""

# Importa las bibliotecas necesarias
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from itertools import product  # Importa la función product desde itertools

# Definir la función para mostrar la matriz de confusión
def plot_confusion_matrix(cm, classes, title="Confusion Matrix"):
    import matplotlib.pyplot as plt
    import numpy as np

    plt.figure()
    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    fmt = 'd'
    thresh = cm.max() / 2.
    for i, j in product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.tight_layout()

# Entrenamiento y evaluación del modelo Gradient Boosting
gb_model = GradientBoostingClassifier()  # Puedes ajustar los hiperparámetros según sea necesario

# Entrena el modelo
gb_model.fit(train_text_features, encoded_y_train)

print("Predicting on the test set")
y_pred = gb_model.predict(test_features)

# Invierte la transformación de las predicciones para obtener las predicciones como "negative" o "positive"
y_pred = le.inverse_transform(y_pred)

# Muestra el informe de clasificación
print("Classification Report")
print(classification_report(y_test, y_pred, target_names=["negative", "positive"]))

# Matriz de confusión
cm = confusion_matrix(y_test, y_pred, labels=["negative", "positive"])

# Llama a la función para mostrar la matriz de confusión
plot_confusion_matrix(cm, classes=["negative", "positive"])

print("Final Accuracy")
print(accuracy_score(y_test, y_pred))

"""Hemos obtenido resultados peores con este clasificador, por lo que tampoco profundizaremos en él, teniendo en cuenta que hemos obtenido bastante buenos resultados con otros.

Como hemos podido observar, el **segundo (SVM) y tercer clasificador (Logistic Regression)** han sido los que **mejores resultados han obtenido**. Por tanto, vamos a realizar pruebas con ambos para ver si es posible mejorar el *accuracy* obtenido.

### Pesado con TF-IDF

Ahora vamos a intentar mejorar el peso específico de cada término aplicando TF-IDF. Con un pesado más preciso, la representación de los datos es también más precisa, y por tanto debería mejorar los resultados del clasificador.
"""

from sklearn.feature_extraction.text import TfidfTransformer

X_train["all"] = X_train[['clean_review']].apply(lambda x: ''.join(x), axis=1)
X_test["all"] = X_test[['clean_review']].apply(lambda x: ''.join(x), axis=1)

count_vect = CountVectorizer(analyzer = "word")
train_features = count_vect.fit_transform(X_train['all'])
test_features = count_vect.transform(X_test['all'])

# Se utiliza L2 (norma euclídea) para normalizar los vectores resultantes
tfidf = TfidfTransformer(norm="l2")
train_text_tfidf_features = tfidf.fit_transform(train_features)
test_text_tfidf_features = tfidf.fit_transform(test_features)

"""**Probamos con SVM:**"""

svm_cls_tfidf = train_and_evaluate_classifier(train_text_tfidf_features, encoded_y_train, SVC(), svm_grid)

print("Best estimator found by grid search:")
print(svm_cls_tfidf.best_estimator_)

print("Predicting sentiment on the test set")
y_pred = svm_cls_tfidf.predict(test_text_tfidf_features)

# Se deshace la transformación de las predicciones para obtener las predicciones como negativas o positivas
y_pred = le.inverse_transform(y_pred)

print("Classification Report")
print
print(classification_report(y_test, y_pred, target_names=["negative", "positive"]))
cm = confusion_matrix(y_test, y_pred, labels=["negative", "positive"])

plot_confusion_matrix(cm, classes=["negative", "positive"])

print("Final Accuracy")
print(accuracy_score(y_test, y_pred))

"""Mostramos las 20 palabras más frecuentes de cada clase (*negative* y *positive*):"""

count_vect = CountVectorizer(analyzer="word") # set min_df = frequency
dataset['all'] = pd.concat([X_train, X_test], ignore_index=True)['clean_review']
negative_features = count_vect.fit_transform(dataset[np.array(pandas.concat([pd.concat([X_train, X_test], ignore_index=True), pd.concat([y_train, y_test], ignore_index=True)], axis=1).sentiment == "negative")]['all'])

negative_occ = np.asarray(negative_features.sum(axis=0)).ravel().tolist()

counts_df = pandas.DataFrame({'term': count_vect.get_feature_names_out(), 'occurrences': negative_occ})
counts_df.sort_values(by='occurrences', ascending=False).head(20)

real_features = count_vect.fit_transform(dataset[np.array(pandas.concat([pd.concat([X_train, X_test], ignore_index=True), pd.concat([y_train, y_test], ignore_index=True)], axis=1).sentiment == "positive")]['all'])
real_occ = np.asarray(real_features.sum(axis=0)).ravel().tolist()

counts_df = pandas.DataFrame({'term': count_vect.get_feature_names_out(), 'occurrences': real_occ})
counts_df.sort_values(by='occurrences', ascending=False).head(20)

"""Y ahora, las 20 features con mayor valor TF-IDF"""

transformer = TfidfTransformer()
negative_features = count_vect.fit_transform(dataset[np.array(pandas.concat([pd.concat([X_train, X_test], ignore_index=True), pd.concat([y_train, y_test], ignore_index=True)], axis=1).sentiment == "negative")]['all'])
transformed_weights = transformer.fit_transform(negative_features)

weights = np.asarray(transformed_weights.mean(axis=0)).ravel().tolist()
weights_df = pandas.DataFrame({'term': count_vect.get_feature_names_out(), 'weight': weights})
weights_df.sort_values(by='weight', ascending=False).head(20)

"""**Probamos con Máxima entropía**"""

max_ent_tfidf_cls = train_and_evaluate_classifier(train_text_tfidf_features, encoded_y_train, LogisticRegression(), max_ent_grid)

print("Best estimator found by grid search:")
print(max_ent_tfidf_cls.best_estimator_)

print("Predicting negative and positive reviews on the test set")
y_pred = max_ent_tfidf_cls.predict(test_text_tfidf_features)

# Se deshace la transformación de las predicciones para obtener las predicciones como negativas o positivas
y_pred = le.inverse_transform(y_pred)

print("Classification Report")
print
print(classification_report(y_test, y_pred, target_names=["negative", "positive"]))
cm = confusion_matrix(y_test, y_pred, labels=["negative", "positive"])

plot_confusion_matrix(cm, classes=["negative", "positive"])

print("Final Accuracy")
print(accuracy_score(y_test, y_pred))

"""Mostramos las 20 palabras más frecuentes de cada clase (*negative* y *positive*):"""

count_vect = CountVectorizer(analyzer="word") # set min_df = frequency
dataset['all'] = pd.concat([X_train, X_test], ignore_index=True)['clean_review']
negative_features = count_vect.fit_transform(dataset[np.array(pandas.concat([pd.concat([X_train, X_test], ignore_index=True), pd.concat([y_train, y_test], ignore_index=True)], axis=1).sentiment == "negative")]['all'])

negative_occ = np.asarray(negative_features.sum(axis=0)).ravel().tolist()

counts_df = pandas.DataFrame({'term': count_vect.get_feature_names_out(), 'occurrences': negative_occ})
counts_df.sort_values(by='occurrences', ascending=False).head(20)

real_features = count_vect.fit_transform(dataset[np.array(pandas.concat([pd.concat([X_train, X_test], ignore_index=True), pd.concat([y_train, y_test], ignore_index=True)], axis=1).sentiment == "positive")]['all'])
real_occ = np.asarray(real_features.sum(axis=0)).ravel().tolist()

counts_df = pandas.DataFrame({'term': count_vect.get_feature_names_out(), 'occurrences': real_occ})
counts_df.sort_values(by='occurrences', ascending=False).head(20)

"""Y ahora, las 20 features con mayor valor TF-IDF"""

transformer = TfidfTransformer()
negative_features = count_vect.fit_transform(dataset[np.array(pandas.concat([pd.concat([X_train, X_test], ignore_index=True), pd.concat([y_train, y_test], ignore_index=True)], axis=1).sentiment == "negative")]['all'])
transformed_weights = transformer.fit_transform(negative_features)

weights = np.asarray(transformed_weights.mean(axis=0)).ravel().tolist()
weights_df = pandas.DataFrame({'term': count_vect.get_feature_names_out(), 'weight': weights})
weights_df.sort_values(by='weight', ascending=False).head(20)

real_features = count_vect.fit_transform(dataset[np.array(pandas.concat([pd.concat([X_train, X_test], ignore_index=True), pd.concat([y_train, y_test], ignore_index=True)], axis=1).sentiment == "positive")]['all'])
transformed_weights = transformer.fit_transform(real_features)

weights = np.asarray(transformed_weights.mean(axis=0)).ravel().tolist()
weights_df = pandas.DataFrame({'term': count_vect.get_feature_names_out(), 'weight': weights})
weights_df.sort_values(by='weight', ascending=False).head(20)

"""Hemos conseguido una mejora del 1 % aplicando TF-IDF.

En tareas con textos cortos o en análisis de sentimiento con un conjunto reducido de palabras clave, enfoques simples como TF (frecuencia de aparición) o pesado binario pueden ser suficientes. Para textos más extensos y ricos en contenido, TF-IDF suele dar mejores resultados. En este caso, podemos ver una mejoría interesante con TF-IDF.

**Nos llama la atención que, aunque SVM hubiese obtenido mejores resultados como clasificador en las pruebas anteriores, con TF-IDF Logistic Regression funciona mejor. Por este motivo, hacemos la prueba con ambos clasificadores.**

### Selección de características

Vamos a llevar a cabo un proceso de selección de características utilizando el test estadístico **Chi-Cuadrado**. Este test nos ayuda a determinar si una característica específica tiene una correlación significativa con la clase final en nuestro conjunto de entrenamiento. Al aplicar este método, podemos retener las «n» características con los valores más altos de Chi-Cuadrado, lo que indica su relevancia en relación con la variable objetivo.

**SVM**
"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_selection import SelectKBest, chi2
from sklearn.svm import SVC

# Create a TfidfVectorizer and transform the text features
tfidf = TfidfVectorizer(norm="l2")
train_text_tfidf_features = tfidf.fit_transform(X_train.clean_review)
test_text_tfidf_features = tfidf.transform(X_test.clean_review)

# Get feature names using get_feature_names_out method
feature_names = tfidf.get_feature_names_out()

# Create a SelectKBest object for feature selection
ch2 = SelectKBest(chi2, k=5000)

# Fit and transform the training text TF-IDF features
train_text_tfidf_ch2_features = ch2.fit_transform(train_text_tfidf_features, y_train)

# Transform the test text TF-IDF features
test_text_tfidf_ch2_features = ch2.transform(test_text_tfidf_features)

# Get the selected feature names based on the indices of selected features
selected_feature_names = [feature_names[i] for i in ch2.get_support(indices=True)]

# Now, 'selected_feature_names' contains the names of the selected features

print("Features reduced from ", len(feature_names), "to", len(selected_feature_names))

max_ent_chi2_cls = train_and_evaluate_classifier(train_text_tfidf_ch2_features, encoded_y_train, SVC(), svm_grid)

print("Predicting negative and positive reviews on the test set")
y_pred = max_ent_chi2_cls.predict(test_text_tfidf_ch2_features)

# Se deshace la transformación de las predicciones para obtener las predicciones como positive o negative
y_pred = le.inverse_transform(y_pred)

print("Classification Report")
print
print(classification_report(y_test, y_pred, target_names=["negative", "positive"]))
cm = confusion_matrix(y_test, y_pred, labels=["negative", "positive"])

plot_confusion_matrix(cm, classes=["negative", "positive"])


print("Final Accuracy")
print(accuracy_score(y_test, y_pred))

"""**Máxima entropía**"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_selection import SelectKBest, chi2

# Create a TfidfVectorizer and transform the text features
tfidf = TfidfVectorizer(norm="l2")
train_text_tfidf_features = tfidf.fit_transform(X_train.clean_review)
test_text_tfidf_features = tfidf.transform(X_test.clean_review)

# Get feature names using get_feature_names_out method
feature_names = tfidf.get_feature_names_out()

# Create a SelectKBest object for feature selection
ch2 = SelectKBest(chi2, k=5000)

# Fit and transform the training text TF-IDF features
train_text_tfidf_ch2_features = ch2.fit_transform(train_text_tfidf_features, y_train)

# Transform the test text TF-IDF features
test_text_tfidf_ch2_features = ch2.transform(test_text_tfidf_features)

# Get the selected feature names based on the indices of selected features
selected_feature_names = [feature_names[i] for i in ch2.get_support(indices=True)]

# Now, 'selected_feature_names' contains the names of the selected features

print("Features reduced from ", len(feature_names), "to", len(selected_feature_names))

max_ent_chi2_cls = train_and_evaluate_classifier(train_text_tfidf_ch2_features, encoded_y_train, LogisticRegression(), max_ent_grid)

print("Predicting negative and positive reviews on the test set")
y_pred = max_ent_chi2_cls.predict(test_text_tfidf_ch2_features)

# Se deshace la transformación de las predicciones para obtener las predicciones como negativas o positivas
y_pred = le.inverse_transform(y_pred)

print("Classification Report")
print
print(classification_report(y_test, y_pred, target_names=["negative", "positive"]))
cm = confusion_matrix(y_test, y_pred, labels=["negative", "positive"])

plot_confusion_matrix(cm, classes=["negative", "positive"])


print("Final Accuracy")
print(accuracy_score(y_test, y_pred))

"""**Se obtiene el mismo resultado con Máxima entropía utilizando únicamente 5.000 características en lugar del conjunto completo de "features" (más de 30.000). Con SVM vemos que los resultados son peores. Nos quedamos con Máxima entropía**

**Por tanto, preferimos el modelo más pequeño (que incluye menos "features"). De esta manera podemos evitar también la sobreadaptación (overfitting) del modelo, la cuál puede ocurrir con mayor probabilidad a medida que el número de features utilizado es más alto. Además, conseguiremos entrenar el modelo de forma mucho más rápida.**

### Conteo de bigramas

En nuestra estrategia de procesamiento de texto, optaremos por emplear bigramas en lugar de unigramas. La inclusión de bigramas, que son secuencias de dos palabras consecutivas, tiende a enriquecer la representación semántica de los textos. Este enfoque puede capturar mejor las relaciones y contextos específicos entre las palabras, lo que a menudo resulta beneficioso para tareas de clasificación. Si bien los trigramas también son una opción, su mejora en comparación con los bigramas no suele ser tan notoria. Si el problema de clasificación lo requiere, se puede ir más allá (n-gramas).
"""

bigram_count_vect = CountVectorizer(analyzer = "word", ngram_range=(1,2))
train_bigram_features = bigram_count_vect.fit_transform(X_train.clean_review)
test_bigram_features = bigram_count_vect.transform(X_test.clean_review)

"""**SVM**"""

max_ent_bigram_cls = train_and_evaluate_classifier(train_bigram_features, encoded_y_train, SVC(), svm_grid)

print("Best estimator found by grid search:")
print(max_ent_bigram_cls.best_estimator_)

print("Predicting negative and positive reviews on the test set")
y_pred = max_ent_bigram_cls.predict(test_bigram_features)

# Se deshace la transformación de las predicciones para obtener las predicciones como negativas o positivas
y_pred = le.inverse_transform(y_pred)

print("Classification Report")
print
print(classification_report(y_test, y_pred, target_names=["negative", "positive"]))
cm = confusion_matrix(y_test, y_pred, labels=["negative", "positive"])

plot_confusion_matrix(cm, classes=["negative", "positive"])


print("Final Accuracy")
print(accuracy_score(y_test, y_pred))

"""**Máxima entropía**"""

max_ent_bigram_cls = train_and_evaluate_classifier(train_bigram_features, encoded_y_train, LogisticRegression(), max_ent_grid)

print("Best estimator found by grid search:")
print(max_ent_bigram_cls.best_estimator_)

print("Predicting negative and positive reviews on the test set")
y_pred = max_ent_bigram_cls.predict(test_bigram_features)

# Se deshace la transformación de las predicciones para obtener las predicciones como negativas o positivas
y_pred = le.inverse_transform(y_pred)

print("Classification Report")
print
print(classification_report(y_test, y_pred, target_names=["negative", "positive"]))
cm = confusion_matrix(y_test, y_pred, labels=["negative", "positive"])

plot_confusion_matrix(cm, classes=["negative", "positive"])


print("Final Accuracy")
print(accuracy_score(y_test, y_pred))

print("Predicting negative or positive reviews on the test set")
y_pred = max_ent_bigram_cls.predict(test_bigram_features)

# Se deshace la transformación de las predicciones para obtener las predicciones como negativas o positivas
y_pred = le.inverse_transform(y_pred)

print("Classification Report")
print
print(classification_report(y_test, y_pred, target_names=["negative", "positive"]))
cm = confusion_matrix(y_test, y_pred, labels=["negative", "positive"])

plot_confusion_matrix(cm, classes=["negative", "positive"])


print("Final Accuracy")
print(accuracy_score(y_test, y_pred))

"""**El modelo de bigramas mejora los resultados respecto al de unigramas (con pesado simple) en Máxima entropía, con SVM no. Aun así, continuaremos a ver si ahora se consigue una mejora.**

Hemos utilizado un pesado TF simple, ahora vamos a crear un modelo final utilizando bigramas y un pesado TF-IDF.
"""

tfidf = TfidfTransformer(norm="l2")
train_text_bigram_tfidf_features = tfidf.fit_transform(train_bigram_features)
test_text_bigram_tfidf_features = tfidf.fit_transform(test_bigram_features)

"""**SVM**"""

max_ent_tfidf_bigram_cls = train_and_evaluate_classifier(train_text_bigram_tfidf_features, encoded_y_train, SVC(), svm_grid)

print("Best estimator found by grid search:")
print(max_ent_tfidf_bigram_cls.best_estimator_)

print("Predicting negative or positive reviews on the test set")
y_pred = max_ent_tfidf_bigram_cls.predict(test_text_bigram_tfidf_features)

# Se deshace la transformación de las predicciones para obtener las predicciones como negativas o positivas
y_pred = le.inverse_transform(y_pred)

print("Classification Report")
print
print(classification_report(y_test, y_pred, target_names=["negative", "positive"]))
cm = confusion_matrix(y_test, y_pred, labels=["negative", "positive"])

plot_confusion_matrix(cm, classes=["negative", "positive"])


print("Final Accuracy")
print(accuracy_score(y_test, y_pred))

"""**Máxima entropía**"""

tfidf = TfidfTransformer(norm="l2")
train_text_bigram_tfidf_features = tfidf.fit_transform(train_bigram_features)
test_text_bigram_tfidf_features = tfidf.fit_transform(test_bigram_features)

max_ent_tfidf_bigram_cls = train_and_evaluate_classifier(train_text_bigram_tfidf_features, encoded_y_train, LogisticRegression(), max_ent_grid)

print("Best estimator found by grid search:")
print(max_ent_tfidf_bigram_cls.best_estimator_)

print("Predicting negative or positive reviews on the test set")
y_pred = max_ent_tfidf_bigram_cls.predict(test_text_bigram_tfidf_features)

# Se deshace la transformación de las predicciones para obtener las predicciones como negativas o positivas
y_pred = le.inverse_transform(y_pred)

print("Classification Report")
print
print(classification_report(y_test, y_pred, target_names=["negative", "positive"]))
cm = confusion_matrix(y_test, y_pred, labels=["negative", "positive"])

plot_confusion_matrix(cm, classes=["negative", "positive"])


print("Final Accuracy")
print(accuracy_score(y_test, y_pred))

"""**Tal y como esperábamos, el modelo que contiene todas las mejoras (bigramas y TF-IDF) obtiene los mejores resultados: un 88 %. SVM también ha mejorado sus resultados, acercándose bastante a Máxima entropía.**"""

def most_informative_feature_for_binary_classification(vectorizer, classifier, n=100):
    """
    See: https://stackoverflow.com/a/26980472

    Identify most important features if given a vectorizer and binary classifier. Set n to the number
    of weighted features you would like to show. (Note: current implementation merely prints and does not
    return top classes.)
    """

    class_labels = classifier.classes_
    feature_names = vectorizer.get_feature_names()
    topn_class1 = sorted(zip(feature_names, classifier.coef_[0]), key=lambda u: u[1])[:n]
    topn_class2 = sorted(zip(feature_names, classifier.coef_[0]), key=lambda u: -u[1])[:n]

    return topn_class1, topn_class2

def most_informative_feature_for_binary_classification(vectorizer, classifier, n):
    class_labels = classifier.classes_
    if len(class_labels) != 2:
        return None

    if isinstance(vectorizer, TfidfVectorizer):
        feature_names = vectorizer.get_feature_names_out()
    elif isinstance(vectorizer, CountVectorizer):
        feature_names = vectorizer.get_feature_names_out()

    topn_class1 = sorted(zip(feature_names, classifier.coef_[0]), key=lambda x: x[1], reverse=True)[:n]
    topn_class2 = sorted(zip(feature_names, classifier.coef_[0]), key=lambda x: x[1], reverse=False)[:n]

    return topn_class1, topn_class2

negative_features, positive_features = most_informative_feature_for_binary_classification(bigram_count_vect, max_ent_cls.best_estimator_, n=30)

top_negative_features = pandas.DataFrame(negative_features, columns=["feature", "score"])
top_negative_features.score = abs(top_negative_features.score)
plt.rc('xtick', labelsize=20)    # fontsize of the tick labels
top_negative_features.plot(x="feature", y="score", kind="bar", title="Most important features to identify negative sentiments")

top_real_features = pandas.DataFrame(real_features, columns=["feature", "score"])
plt.rc('xtick', labelsize=20)    # fontsize of the tick labels
top_real_features.plot(x="feature", y="score", kind="bar", title="Most important features to identify positive sentiments")

"""**Vamos a probar a continuación añadir Chi-Cuadrado a todo lo que hemos realizado anteriormente (solo con Máxima entropía), TF-IDF y bigramas, aunque como veremos, no se obtiene un mejor resultado.**"""

bigram_tfidf_chi2_vect = TfidfVectorizer(norm="l2", ngram_range=(1,2))
train_text_bigram_tfidf_chi2_features = bigram_tfidf_chi2_vect.fit_transform(X_train.clean_review)
test_text_bigram_tfidf_chi2_features = bigram_tfidf_chi2_vect.transform(X_test.clean_review)

ch2 = SelectKBest(chi2, k=25000)
train_text_bigram_tfidf_chi2_features = ch2.fit_transform(train_text_bigram_tfidf_chi2_features, y_train)
test_text_bigram_tfidf_chi2_features = ch2.transform(test_text_bigram_tfidf_chi2_features)

max_ent_bigram_tfidf_chi2_cls = train_and_evaluate_classifier(train_text_bigram_tfidf_chi2_features, encoded_y_train, LogisticRegression(), max_ent_grid)

print("Predicting negative or positive reviews on the test set")
y_pred = max_ent_bigram_tfidf_chi2_cls.predict(test_text_bigram_tfidf_chi2_features)

# Se deshace la transformación de las predicciones para obtener las predicciones como negativas o positivas
y_pred = le.inverse_transform(y_pred)

print("Classification Report")
print
print(classification_report(y_test, y_pred, target_names=["negative", "positive"]))
cm = confusion_matrix(y_test, y_pred, labels=["negative", "positive"])

plot_confusion_matrix(cm, classes=["negative", "positive"])

print("Final Accuracy")
print(accuracy_score(y_test, y_pred))

"""## Modelo final

**Se observa que con Chi-Cuadrado no mejora, por lo que al final la mejor opción sería utilizar una Máxima entropía con TF-IDF y bigramas. Al usar Chi-Cuadrado se eliminaban columnas que eran necesarias para el modelo.**

**Lo volvemos a presentar a continuación.**
"""

tfidf = TfidfTransformer(norm="l2")
train_text_bigram_tfidf_features = tfidf.fit_transform(train_bigram_features)
test_text_bigram_tfidf_features = tfidf.fit_transform(test_bigram_features)

max_ent_tfidf_bigram_cls = train_and_evaluate_classifier(train_text_bigram_tfidf_features, encoded_y_train, LogisticRegression(), max_ent_grid)

print("Best estimator found by grid search:")
print(max_ent_tfidf_bigram_cls.best_estimator_)

print("Predicting negative or positive reviews on the test set")
y_pred = max_ent_tfidf_bigram_cls.predict(test_text_bigram_tfidf_features)

# Se deshace la transformación de las predicciones para obtener las predicciones como negativas o positivas
y_pred = le.inverse_transform(y_pred)

print("Classification Report")
print
print(classification_report(y_test, y_pred, target_names=["negative", "positive"]))
cm = confusion_matrix(y_test, y_pred, labels=["negative", "positive"])

plot_confusion_matrix(cm, classes=["negative", "positive"])


print("Final Accuracy")
print(accuracy_score(y_test, y_pred))

"""# Conclusión

Nos hemos enfrentado a un desafío de clasificación textual centrado en reseñas positivas y negativas. Durante la exploración del conjunto de datos, identificamos la presencia destacada de *stopwords* y variantes de palabras, lo que motivó la aplicación de estrategias de eliminación de *stopwords* y normalización mediante lematización y *stemming*. Este proceso permitió reducir la complejidad de la matriz «document-term» y encontrar relaciones más significativas entre los datos.

En la fase inicial de modelado, implementamos diversos algoritmos, incluyendo Naïve Bayes, SVM, modelos de máxima entropía, Random Forest, XGBoost y Gradient Boosting. Utilizamos herramientas de sklearn para realizar validación cruzada, optimización y evaluación de los modelos. Como era de esperar, los SVM y, en particular, los modelos de máxima entropía demostraron ser los más efectivos para nuestra tarea.

Posteriormente, nos centramos en estrategias avanzadas de Procesamiento del Lenguaje Natural (NLP) para mejorar la clasificación:

*   TF-IDF: Evolucionamos desde una representación simple basada en la frecuencia de palabras hacia modelos de máxima entropía que utilizan la ponderación TF-IDF. Este enfoque buscaba capturar de manera más precisa la importancia relativa de los términos en los documentos.
*   Selección de características: Aplicamos el test de Chi-Cuadrado para reducir la dimensionalidad del modelo, enfocándonos en las características más relevantes y significativas.
*   Bigramas: Ampliamos nuestro enfoque inicial en unigramas a modelos de máxima entropía basados en bigramas. Este enriquecimiento nos permitió capturar relaciones más complejas entre pares de palabras consecutivas

**Los resultados más destacados se obtuvieron con modelos de máxima entropía basados en bigramas, utilizando ponderación TF-IDF. Estos modelos alcanzaron una precisión del 88 %, indicando una clasificación sólida y efectiva.**

# Redes Neuronales

En una primera aproximación a la utilización de modelos basados en redes neuronales para la clasificación de textos, nos vamos a limitar a usar redes neuronales densas, en las que todas las salidas de una capa están conectadas con todas las entradas de la siguiente capa.

Como se ha comentado en los contenidos teóricos de la asignatura, para construir nuestros modelos se utilizará [Keras](https://keras.io/), una API de alto nivel para redes neuronales. Vamos a importar de esta librería todos los tipos de capas y utilidades que vamos a necesitar.
"""

!pip install --upgrade keras

"""Se han mostrado problemas a la hora de cargar #from keras.utils.np_utils import to_categorical y from tensorflow.python.keras.models import Input, por lo que se ha ajustado en el código."""

# Importamos capas densas, capas de embeddings y capas para el aplanado de dimensiones
from keras.layers import Dense, Embedding, Flatten, GlobalMaxPool1D

# Importamos las herramientas para generar el modelo y trabajar con las entradas
from keras.models import Sequential, Model
from tensorflow.python.keras.models import Input

# Utilidades para el preprocesado de textos: tokenizador y rellenado de secuencias
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences

from sklearn.model_selection import train_test_split

# Herramienta para la gestión de las etiquetas de salida
from tensorflow.keras.utils import to_categorical

# Herramienta para implementar Early Stopping (criterios de parada del entrenamiento)
from keras.callbacks import EarlyStopping

# Utilidad para el trabajo con embeddings preentrenados
from keras.initializers import Constant

# Con esta función podremos ver cómo evoluciona el entrenamiento y la validación de los modelos.

import matplotlib.pyplot as plt
plt.style.use('ggplot')

def plot_history(history):
    acc = history.history['acc']
    val_acc = history.history['val_acc']
    loss = history.history['loss']
    val_loss = history.history['val_loss']
    x = range(1, len(acc) + 1)

    plt.figure(figsize=(12, 5))
    plt.subplot(1, 2, 1)
    plt.plot(x, acc, 'b', label='Training acc')
    plt.plot(x, val_acc, 'r', label='Validation acc')
    plt.title('Training and validation accuracy')
    plt.legend()
    plt.subplot(1, 2, 2)
    plt.plot(x, loss, 'b', label='Training loss')
    plt.plot(x, val_loss, 'r', label='Validation loss')
    plt.title('Training and validation loss')
    plt.legend()

"""Definición de los parámetros iniciales de la red:"""

#Número de épocas (iteraciones globales)
epochs = 40
#Dimensión de la capa de embeddings
emb_dim = 50
#Tamaño de batch (lote)
batch_size = 32
#Número de palabras tenidas en cuenta (reducción del vocabulario)
n_most_common_words = 50000
#Máxima longitud del documento
max_len = 512

"""En una primera instancia, nos enfocaremos en mantener las mismas características que se emplearon en las pruebas previas. En este contexto, cada documento se describirá mediante un vector que utiliza la ponderación TF-IDF. Este vector, cuya dimensión coincide con el tamaño del vocabulario, presentará valores distintos de cero únicamente en los índices asociados a las palabras presentes en el documento."""

from sklearn.feature_extraction.text import TfidfTransformer

X_train["all"] = X_train[['clean_review']].apply(lambda x: ''.join(x), axis=1)
X_test["all"] = X_test[['clean_review']].apply(lambda x: ''.join(x), axis=1)

count_vect = CountVectorizer(analyzer = "word")
train_features = count_vect.fit_transform(X_train['all'])
test_features = count_vect.transform(X_test['all'])

# Se utiliza L2 (norma euclídea) para normalizar los vectores resultantes
tfidf = TfidfTransformer(norm="l2")
train_text_tfidf_features = tfidf.fit_transform(train_features)
test_text_tfidf_features = tfidf.fit_transform(test_features)

"""Además, vamos a utilizar la función "to_categorical" para transformar nuestra salida en un formato entendible por Keras para una tarea de clasificación."""

Y_train = to_categorical(encoded_y_train, 2)
Y_test = to_categorical(le.transform(y_test.values), 2)

print("Dimensión de los vectores:", train_text_tfidf_features.shape[1])

"""En cuanto a la representación de cada documento, se emplea un vector de 31 106 dimensiones, correspondiente al tamaño del vocabulario.

Ahora, procederemos a la construcción del modelo en Keras. Dada la simplicidad de nuestro enfoque, optaremos por la API secuencial de Keras. Aunque para estructuras más complejas exista la opción de la API funcional de Keras, que permite la creación de modelos con múltiples entradas, salidas y capas personalizadas.

En este contexto, nos limitaremos a capas densas para la red neuronal. En cada capa, se debe especificar el número de neuronas (definiendo así el tamaño de salida) y la función de activación. Para todas las capas, utilizaremos la función Softmax. En la capa inicial, también se especificará el tamaño de la entrada, que en este caso coincide con el tamaño del vocabulario.

Finalmente, se invocará el método «compile» para configurar el optimizador (en este caso, «Adam»), las métricas de evaluación (optaremos por la precisión, dado que las clases están equilibradas) y la función de pérdida. Dada la naturaleza de nuestro problema de clasificación binaria, seleccionaremos la entropía cruzada binaria como función de pérdida.

Un vistazo al resumen del modelo, obtenido mediante el método «summary», proporciona una visión completa de la arquitectura de la red, incluyendo todos los parámetros implicados.
"""

model = Sequential()
model.add(Dense(128, input_dim=train_text_tfidf_features.shape[1], activation='softmax'))
model.add(Dense(64, activation='softmax'))
model.add(Dense(2, activation='softmax'))
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])
print(model.summary())

"""Utilizamos una primera capa con 128 neuronas que recibe toda la entrada (vectores de 31 106 dimensiones). Por lo tanto, los parámetros a aprender en esta capa son 31 106 x 128 (pesos) + 128 (biases) = 3 981 696 parámetros.

La siguiente capa recibe 128 entradas y tiene 64 salidas, por lo tanto, hay 128 x 64 + 64 = 8 256 parámetros a aprender en esta capa.

Finalmente, la última capa recibe 64 entradas y genera 2 salidas. En este caso, hay 64 x 2 + 2 = 130 parámetros a aprender.

En resumen, el total de parámetros entrenables para el modelo es 3 981 696 (primera capa) + 8 256 (segunda capa) + 130 (tercera capa) = 3 990 082 parámetros.

A continuación, procederemos con el entrenamiento del modelo. Para ello, llamaremos a la función «fit», convirtiendo nuestras características a un array y especificando las salidas esperadas para cada instancia de entrenamiento. También indicaremos el número de épocas (iteraciones) y el tamaño del lote de instancias (batch size). Asimismo, reservaremos un porcentaje de instancias para la validación del modelo, lo que nos permitirá monitorear posibles casos de sobreajuste. Una vez completado el entrenamiento, utilizaremos la función «plot_history» para visualizar el progreso del mismo.
"""

history = model.fit(train_text_tfidf_features.toarray().astype(float), Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.2)
plot_history(history)

"""Podemos observar en el gráfico cómo la precisión de entrenamiento y validación converge hasta cierto punto, indicando que la red puede no estar mejorando significativamente. A partir de la iteración 23, notamos que la función de pérdida de validación deja de disminuir y se separa de la de entrenamiento. Esta observación sugiere la posibilidad de que esté ocurriendo overfitting durante el entrenamiento del modelo.

La presencia de este fenómeno se hace evidente en el gráfico, donde la precisión de validación estanca su mejora, y la brecha entre las curvas de pérdida de entrenamiento y validación se amplía después de la iteración 23.

Para obtener una comprensión más detallada del rendimiento de nuestro modelo, llevaremos a cabo pruebas en instancias de prueba. En este caso, ajustaremos las características de prueba para que sean adecuadas y comprensibles para el modelo.


"""

predict_x=model.predict(test_text_tfidf_features.toarray().astype(float))
y_pred=np.argmax(predict_x,axis=1)

# Se deshace la transformación de las predicciones para obtener las predicciones como negativas o positivas
y_pred = le.inverse_transform(y_pred)

print("Classification Report")
print
print(classification_report(y_test, y_pred, target_names=["negative", "positive"]))
cm = confusion_matrix(y_test, y_pred, labels=["negative", "positive"])

plot_confusion_matrix(cm, classes=["negative", "positive"])

print("Final Accuracy")
print(accuracy_score(y_test, y_pred))

"""Hemos conseguido un accuracy final muy similar al que conseguíamos con los clasificadores de máxima entropía, **casi un 88 %**.

Hemos identificado que el modelo anterior exhibe signos de sobreajuste durante su entrenamiento. Para abordar este problema, podemos emplear diversas técnicas, entre las cuales destaca la estrategia de parada temprana, también conocida como «Early Stopping». Esta técnica implica establecer un criterio de detención que permita interrumpir el entrenamiento antes de completar todas las «epochs» o iteraciones especificadas.

En este contexto, implementaremos Early Stopping de la siguiente manera: si la función de pérdida en el conjunto de validación deja de disminuir durante al menos 7 iteraciones consecutivas, detendremos el entrenamiento. Este enfoque nos brinda la capacidad de evitar el sobreajuste al detener el proceso de entrenamiento cuando se observa un estancamiento en la mejora de la pérdida en el conjunto de validación.
"""

model = Sequential()
model.add(Dense(128, input_dim=train_text_tfidf_features.shape[1], activation='softmax'))
model.add(Dense(64, activation='softmax'))
model.add(Dense(2, activation='softmax'))
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])
print(model.summary())

history = model.fit(train_text_tfidf_features.toarray().astype(float), Y_train, epochs=epochs,0 batch_size=batch_size,validation_split=0.2, callbacks=[EarlyStopping(monitor='val_loss', mode='min', patience=7, min_delta=0.01)])
plot_history(history)

predict_x=model.predict(test_text_tfidf_features.toarray().astype(float))
y_pred=np.argmax(predict_x,axis=1)

# Se deshace la transformación de las predicciones para obtener las predicciones como negativas o positivas
y_pred = le.inverse_transform(y_pred)

print("Classification Report")
print
print(classification_report(y_test, y_pred, target_names=["negative", "positive"]))
cm = confusion_matrix(y_test, y_pred, labels=["negative", "positive"])

plot_confusion_matrix(cm, classes=["negative", "positive"])

print("Final Accuracy")
print(accuracy_score(y_test, y_pred))

"""Podemos observar en la gráfica que el entrenamiento se detiene en la iteración 21, cuando el valor de pérdida en validación comienza a ascender. A pesar de esta interrupción temprana, la precisión final obtenida se mantiene en niveles similares.

Durante las primeras iteraciones, la pérdida y la precisión mejoran tanto en el conjunto de entrenamiento como en el de validación. Sin embargo, a partir de la iteración 21, la pérdida de validación deja de disminuir y se desvía de la de entrenamiento. Este comportamiento sugiere la presencia de cierto sobreajuste.

La estrategia de «Early Stopping» ha sido efectiva en este caso, ya que ha permitido detener el entrenamiento antes de que el modelo se sobreajuste a los datos de entrenamiento. La implementación de esta técnica ayuda a mejorar la generalización del modelo y a evitar el deterioro del rendimiento en datos no vistos.

**Vamos a realizar pruebas para ver si es posible mejorar lo que tenemos de momento**

Por no añadir todas las pruebas realizadas, voy a comentar los hiperparámetros que se han probado y que no han resultado del todo satisfactorios.

**Número de épocas (Epochs):** Se han probado distintas épocas como 20 y 40, auqnue la que mejores resultados ha obtenido ha sido la de 30.

**Dimensión de la capa de embeddings (emb_dim):** Las pruebas con dimensiones de 50 y 150 no mostraron mejoras notables en la capacidad del modelo para capturar relaciones semánticas y sintácticas. La dimensión más eficaz fue 100.

**Tamaño de batch (batch_size):** Experimentar con diferentes tamaños de lote no condujo a mejoras sustanciales en la convergencia del modelo ni en su estabilidad.

**Número de palabras consideradas (n_most_common_words):** Reducir este número a 10,000 no resultó en una mejora sustancial en la complejidad del modelo ni en la información relevante capturada. Por tanto, podemos afirmar que no nos ha servido reducir el vocabulario en este caso.

**Máxima longitud (max_len):** La elección de una longitud máxima de 512 no condujo a mejoras significativas en la capacidad del modelo para procesar información más extensa, ha sido más efectiva la longitud de 256.

Aunque se hayan realizado numerosas pruebas, los mejores resultados se han obtenido a partir de los siguientes hiperparámetros:
"""

# Número de épocas (iteraciones globales)
epochs = 30

# Dimensión de la capa de embeddings
emb_dim = 100

# Tamaño de batch (lote)
batch_size = 32

# Número de palabras tenidas en cuenta (reducción del vocabulario)
n_most_common_words = 50000

# Máxima longitud del documento
max_len = 256

from sklearn.feature_extraction.text import TfidfTransformer

X_train["all"] = X_train[['clean_review']].apply(lambda x: ''.join(x), axis=1)
X_test["all"] = X_test[['clean_review']].apply(lambda x: ''.join(x), axis=1)

count_vect = CountVectorizer(analyzer = "word")
train_features = count_vect.fit_transform(X_train['all'])
test_features = count_vect.transform(X_test['all'])

# Se utiliza L2 (norma euclídea) para normalizar los vectores resultantes
tfidf = TfidfTransformer(norm="l2")
train_text_tfidf_features = tfidf.fit_transform(train_features)
test_text_tfidf_features = tfidf.fit_transform(test_features)

Y_train = to_categorical(encoded_y_train, 2)
Y_test = to_categorical(le.transform(y_test.values), 2)

print("Dimensión de los vectores:", train_text_tfidf_features.shape[1])

"""**Funciones de activación (activation):** Experimentar con «relu» y «sigmoid» no produjo diferencias sustanciales en el rendimiento del modelo. La elección de funciones de activación no resultó en mejoras notables en la capacidad predictiva del modelo. Aunque «relu» se suele usar en capas ocultas y «sigmoid» en la capa de salida para problemas de clasificación binaria, en este caso, no han sido lo suficientemente efectivas.

La configuración actual con «softmax» en las capas ocultas y «adam» como optimizador ha mostrado ser más adecuada. «Softmax» se usa frecuentemente en la capa de salida para problemas de clasificación multiclase, mientras que «adam» es un optimizador que adapta su tasa de aprendizaje durante el entrenamiento, demostrando ser bastante eficaz.
"""

model = Sequential()
model.add(Dense(128, input_dim=train_text_tfidf_features.shape[1], activation='softmax'))
model.add(Dense(64, activation='softmax'))
model.add(Dense(2, activation='softmax'))
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])
print(model.summary())

history = model.fit(train_text_tfidf_features.toarray().astype(float), Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.2, callbacks=[EarlyStopping(monitor='val_loss', mode='min', patience=7, min_delta=0.01)])
plot_history(history)

predict_x=model.predict(test_text_tfidf_features.toarray().astype(float))
y_pred=np.argmax(predict_x,axis=1)

# Se deshace la transformación de las predicciones para obtener las predicciones como negativas o positivas
y_pred = le.inverse_transform(y_pred)

print("Classification Report")
print
print(classification_report(y_test, y_pred, target_names=["negative", "positive"]))
cm = confusion_matrix(y_test, y_pred, labels=["negative", "positive"])

plot_confusion_matrix(cm, classes=["negative", "positive"])

print("Final Accuracy")
print(accuracy_score(y_test, y_pred))

"""## Word embeddings

En este primer acercamiento a las redes neuronales, hemos empleado directamente el vector de características generado a partir de las palabras presentes en los documentos como entrada. Sin embargo, el tamaño considerable del vocabulario y del conjunto de entrenamiento suele hacer que esta aproximación sea poco eficiente y difícil de manejar. Es en este contexto donde entran en juego los *word embeddings*, que posibilitan la representación de palabras en un espacio vectorial más denso, generalmente con dimensiones que oscilan entre 50 y 300.

Para adecuar nuestros datos de entrada y poder introducirlos en la red, necesitamos reformatearlos como secuencias de tokens. Para llevar a cabo este proceso, se emplean funciones proporcionadas por Keras. En primer lugar, utilizamos la función text_to_sequences dentro de la clase Tokenizer para convertir el texto original en secuencias de tokens.

Una vez completada esta transformación, obtenemos la longitud del vocabulario sumándole una unidad al tamaño del índice de palabras proporcionado por el tokenizador. Este ajuste es necesario debido a que el índice 0 se encuentra reservado.
"""

tokenizer = Tokenizer()
tokenizer.fit_on_texts(X_train.clean_review)
sequences_train = tokenizer.texts_to_sequences(X_train.clean_review)
sequences_test = tokenizer.texts_to_sequences(X_test.clean_review)
vocab_size = len(tokenizer.word_index) + 1

print("Tamaño del vocabulario: ", vocab_size)

"""Como podemos observar, el tamaño del vocabulario generado por el Tokenizador de Keras es muy similar al utilizado en experimentos anteriores.

Al emplear la representación mencionada, nos enfrentamos a un desafío clave: la longitud de cada frase varía, mientras que la red requiere que todas las entradas tengan el mismo tamaño. En el experimento previo, esta variabilidad no era un problema, ya que introducíamos como entrada un vector del tamaño de todo el vocabulario. Sin embargo, en esta etapa, la entrada consistirá en la propia frase.

Para abordar esta variabilidad en la longitud de las frases, empleamos la función pad_sequences. Esta función realiza la operación de relleno (*padding*) en todas las frases, asegurando que tengan la misma longitud. La longitud objetivo está previamente definida en la variable "max_len".
"""

train_text_nn = pad_sequences(sequences_train, maxlen=max_len)
test_text_nn = pad_sequences(sequences_test, maxlen=max_len)

Y_train = to_categorical(encoded_y_train, 2)
Y_test = to_categorical(le.transform(y_test.values), 2)

"""En esta etapa, procederemos a construir nuestro modelo de Deep Learning, incorporando las siguientes capas:

Capa de embeddings: En lugar de emplear los tokens originales, generaremos una representación vectorial densa de los datos de entrada, utilizando un tamaño de embedding de 50 dimensiones.

Capas densas intermedias: Estas capas serán responsables de procesar la representación vectorial generada por la capa de embeddings y extraer características relevantes.

Capa densa final: Esta capa actuará como el clasificador final, operando sobre la representación proporcionada por la última capa densa para determinar a qué clase pertenece cada nuevo documento de entrada.

La introducción de la capa de embeddings implica que cada documento se representará con una matriz en lugar de un vector de características. Esta matriz contendrá 512 elementos (el tamaño máximo de documento que hemos indicado), y cada uno de ellos será un vector denso de 50 elementos, representando una palabra. Dado que las capas densas requieren un vector de características como entrada, será necesario "aplanar" esta matriz para transformarla en un vector. Esta tarea se puede realizar mediante la capa "Flatten", que convierte un tensor de cualquier forma en un tensor unidimensional. Además, al emplear esta capa, es esencial especificar el parámetro "input_length" en la capa de embedding, indicando el tamaño máximo de documento que se utilizará.
"""

#Número de épocas (iteraciones globales)
epochs = 40
#Dimensión de la capa de embeddings
emb_dim = 50
#Tamaño de batch (lote)
batch_size = 32
#Número de palabras tenidas en cuenta (reducción del vocabulario)
n_most_common_words = 50000
#Máxima longitud del documento
max_len = 512

model = Sequential()
model.add(Embedding(input_dim=vocab_size,
                           output_dim=emb_dim,
                           input_length=train_text_nn.shape[1]))
model.add(Flatten())
model.add(Dense(64, activation='softmax'))
model.add(Dense(2, activation='softmax'))
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])
print(model.summary())

history = model.fit(train_text_nn, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.2, callbacks=[EarlyStopping(monitor='val_loss', mode='min', patience=7, min_delta=0.01)])
plot_history(history)

"""De forma similar al experimento anterior, observamos cómo el entrenamiento y la validación convergen a un valor de accuracy, y además lo hacen más rápidamente que antes."""

predict_x=model.predict(test_text_nn)
y_pred=np.argmax(predict_x,axis=1)

# Se deshace la transformación de las predicciones para obtener las predicciones como negativas o positivas
y_pred = le.inverse_transform(y_pred)

print("Classification Report")
print
print(classification_report(y_test, y_pred, target_names=["negative", "positive"]))
cm = confusion_matrix(y_test, y_pred, labels=["negative", "positive"])

plot_confusion_matrix(cm, classes=["negative", "positive"])

print("Final Accuracy")
print(accuracy_score(y_test, y_pred))

"""La accuracy final que se consigue es algo menor que en el experimento anterior.

**Pruebas de red neuronal de embeddings**

**A continuación presento distintos hiperparámetros que se han probado, aunque como prueba solo veremos uno de los ejemplos, ya que de ninguna forma se ha logrado mejorar los resultados (se ha obtenido una accuracy alrededor del 85 %.**

Al introducir una **capa adicional** en el modelo, se buscó mejorar su capacidad de representación, aunque esta modificación conlleva el riesgo de sobreajuste. La adición de la capa de dropout tiene como objetivo mitigar este problema, ya que apaga aleatoriamente algunas neuronas durante el entrenamiento, evitando que el modelo dependa demasiado de un conjunto específico de neuronas y, por lo tanto, mejorando la generalización.

La inclusión de la **regularización L2** se usó con el propósito penalizar pesos grandes, proporcionando una medida adicional para prevenir el sobreajuste.

Se experimentó con dos **tasas de aprendizaje (0.001 y 0.01)** utilizando el optimizador Adam. Sin embargo, los resultados anteriores demostraron ser más efectivos.

Aunque estas modificaciones se realizaron con la intención de mejorar la capacidad de generalización del modelo, los resultados anteriores sin estas adiciones demostraron ser más efectivos para la tarea específica y el conjunto de datos en consideración. Es un recordatorio de que, en ocasiones, una arquitectura más simple puede ser más beneficiosa, especialmente cuando se trabaja con conjuntos de datos bastante limpios.
"""

from keras.regularizers import l2
from keras.layers import Embedding, Flatten, Dense, Dropout
from keras.optimizers import Adam

model = Sequential()
model.add(Embedding(input_dim=vocab_size,
                   output_dim=emb_dim,
                   input_length=train_text_nn.shape[1]))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dense(64, activation='relu'))
model.add(Dense(2, activation='sigmoid'))

# Compilar el modelo
model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['acc'])

history = model.fit(train_text_nn, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.2, callbacks=[EarlyStopping(monitor='val_loss', mode='min', patience=7, min_delta=0.01)])
plot_history(history)

predict_x=model.predict(test_text_nn)
y_pred=np.argmax(predict_x,axis=1)

# Se deshace la transformación de las predicciones para obtener las predicciones como negativas o positivas
y_pred = le.inverse_transform(y_pred)

print("Classification Report")
print
print(classification_report(y_test, y_pred, target_names=["negative", "positive"]))
cm = confusion_matrix(y_test, y_pred, labels=["negative", "positive"])

plot_confusion_matrix(cm, classes=["negative", "positive"])

print("Final Accuracy")
print(accuracy_score(y_test, y_pred))

"""## Embeddings preentrenados

En el experimento anterior, intentamos aprender representaciones basadas en embeddings directamente de nuestros datos de entrenamiento. Sin embargo, dado que nuestro conjunto de datos no es lo suficientemente grande para entrenar embeddings efectivos, es más recomendable utilizar embeddings preentrenados.

En lugar de aprender embeddings desde cero, optaremos por emplear los embeddings proporcionados por Glove. Estos embeddings están preentrenados en conjuntos de datos masivos y capturan relaciones semánticas entre palabras. En este experimento, seleccionaremos los embeddings de 50 dimensiones (glove.6B.50d.txt) para integrarlos en nuestro modelo.

Tras la descarga de los embeddings, los cargamos para tener una representación vectorial para cada palabra.
"""

import os

embeddings_index = {}
# Colab
f = open('/content/drive/MyDrive/glove.6B.50d.txt')

for line in f:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs

f.close()

print('Found %s word vectors.' % len(embeddings_index))
word_index = tokenizer.word_index
print('Found %s unique tokens.' % len(word_index))

"""Tenemos un total de 400000 palabras almacenadas en nuestro modelo de embeddings, mientras que nuestro vocabulario se compone de 31130 tokens distintos. Vamos a generar una matriz que introduciremos a la capa de embeddings, la cuál contendrá el embedding correspondiente a cada palabra en el vocabulario. En caso de que la palabra no se encuentre en el modelo de embeddings de Glove, generaremos un vector aleatorio para representarla."""

print('Found %s word vectors.' % len(embeddings_index))
word_index = tokenizer.word_index
print('Found %s unique tokens.' % len(word_index))

num_words = min(n_most_common_words, len(word_index)) + 1

embedding_matrix = np.zeros((num_words, emb_dim))

for word, i in word_index.items():
    if i > n_most_common_words:
        continue
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        # Si la palabra existe añadimos su vector a la matriz
        embedding_matrix[i] = embedding_vector
    else:
        # Si no existe, se le asigna un vector aleatorio
        embedding_matrix[i] = np.random.randn(emb_dim)

"""Necesitamos modificar la capa de embeddings para incluir los embeddings Glove. Utilizando la matriz generada anteriormente, cargamos los valores de los embeddings preentrenados. Además, el parámetro "trainable" nos permite decidir si estos parámetros siguen siendo modificables (trainable = True) o lo mantenemos fijos durante todo el entrenamiento (trainable = False)."""

model = Sequential()
model.add(Embedding(num_words,
                    emb_dim,
                    embeddings_initializer=Constant(embedding_matrix),
                    input_length=train_text_nn.shape[1],
                    trainable=True))
model.add(Flatten())
model.add(Dense(64, activation='softmax'))
model.add(Dense(2, activation='softmax'))
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])
print(model.summary())

history = model.fit(train_text_nn, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.2, callbacks=[EarlyStopping(monitor='val_loss', mode='min', patience=7, min_delta=0.01)])
plot_history(history)

predict_x=model.predict(test_text_nn)
y_pred=np.argmax(predict_x,axis=1)

# Se deshace la transformación de las predicciones para obtener las predicciones como negativas o positivas
y_pred = le.inverse_transform(y_pred)

print("Classification Report")
print
print(classification_report(y_test, y_pred, target_names=["negative", "positive"]))
cm = confusion_matrix(y_test, y_pred, labels=["negative", "positive"])

plot_confusion_matrix(cm, classes=["negative", "positive"])

print("Final Accuracy")
print(accuracy_score(y_test, y_pred))

"""El modelo basado en embeddings preentrenados está funcionando bastante peor que el modelo anterior. Esto puede deberse, entre otras cosas, a que estamos utilizando tokens a los que se ha realizado un proceso de stemming (por tanto no son palabras propiamente dichas). Por ello, es probable que haya muchos tokens que no existan en el modelo de embedding preentrenado. A continuación vamos a pre-procesar el texto de forma similar a la anterior, pero sin incluir el paso de *stemming* en el mismo:"""

from nltk.stem import *
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords

import re

import nltk
nltk.download('stopwords')
nltk.download('wordnet')

def process_text(raw_text):
    #Consideramos únicamente letras utilizando una expresión regular
    letters_only = re.sub("[^a-zA-Z]", " ",raw_text)
    #Convertimos todo a minúsculas
    words = letters_only.lower().split()

    #Eliminamos las stopwords
    stops = set(stopwords.words("english"))
    not_stop_words = [w for w in words if not w in stops]

    #Lematización
    wordnet_lemmatizer = WordNetLemmatizer()
    lemmatized = [wordnet_lemmatizer.lemmatize(word) for word in not_stop_words]

    return( " ".join( lemmatized ))

dataset['clean_review_2'] = dataset['review'].apply(lambda x: process_text(x))
dataset.head()

X_train = dataset[0:len(training_set)][["id", "clean_review_2"]]
y_train = dataset[0:len(training_set)][["sentiment"]]
X_test = dataset[len(training_set):len(dataset)][["id", "clean_review_2"]]
y_test = dataset[len(training_set):len(dataset)][["sentiment"]]

tokenizer = Tokenizer()
tokenizer.fit_on_texts(X_train.clean_review_2)
sequences_train = tokenizer.texts_to_sequences(X_train.clean_review_2)
sequences_test = tokenizer.texts_to_sequences(X_test.clean_review_2)
vocab_size = len(tokenizer.word_index) + 1

print("Tamaño del vocabulario:", vocab_size)

print('Found %s word vectors.' % len(embeddings_index))
word_index = tokenizer.word_index
print('Found %s unique tokens.' % len(word_index))

num_words = min(n_most_common_words, len(word_index)) + 1

embedding_matrix = np.zeros((num_words, emb_dim))

for word, i in word_index.items():
    if i > n_most_common_words:
        continue
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        # Si la palabra existe añadimos su vector a la matriz
        embedding_matrix[i] = embedding_vector
    else:
        # Si no existe, se le asigna un vector aleatorio
        embedding_matrix[i] = np.random.randn(emb_dim)

train_text_nn = pad_sequences(sequences_train, maxlen=max_len)
test_text_nn = pad_sequences(sequences_test, maxlen=max_len)

Y_train = to_categorical(encoded_y_train, 2)
Y_test = to_categorical(le.transform(y_test.values), 2)

model = Sequential()
model.add(Embedding(num_words,
                    emb_dim,
                    embeddings_initializer=Constant(embedding_matrix),
                    input_length=train_text_nn.shape[1],
                    trainable=True))
model.add(Flatten())
model.add(Dense(64, activation='softmax'))
model.add(Dense(2, activation='softmax'))
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])
print(model.summary())

history = model.fit(train_text_nn, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.2, callbacks=[EarlyStopping(monitor='val_loss', mode='min', patience=7, min_delta=0.01)])
plot_history(history)

predict_x=model.predict(test_text_nn)
y_pred=np.argmax(predict_x,axis=1)

# Se deshace la transformación de las predicciones para obtener las predicciones como negativas o positivas
y_pred = le.inverse_transform(y_pred)

print("Classification Report")
print
print(classification_report(y_test, y_pred, target_names=["negative", "positive"]))
cm = confusion_matrix(y_test, y_pred, labels=["negative", "positive"])

plot_confusion_matrix(cm, classes=["negative", "positive"])

print("Final Accuracy")
print(accuracy_score(y_test, y_pred))

"""Aunque el rendimiento mejora, parece que no conseguimos obtener resultados similares utilizando embeddings preentrenados, por lo que no continuaremos investigando esta rama.

# Conclusiones finales

Hemos observado que el uso de **embeddings de palabras específicos** para nuestra tarea, es decir, los ***word embeddings*** que hemos entrenado directamente en nuestros datos, ha demostrado **ser más efectivo que el uso de *embeddings* preentrenados** en esta tarea de reseñas negativas y positivas.

Los ***embeddings* preentrenados**, aunque pueden ser útiles para tareas generales del lenguaje natural, no han sido ajustados específicamente a las peculiaridades de nuestro conjunto de datos. **Nuestro material es único** en su contenido y contexto, lo que hace que **los *embeddings* generales no capturen completamente las características para la tarea en cuestión.**

A pesar de que el uso de *embeddings* preentrenados en tareas de procesamiento de lenguaje natural sea común, **los datos muestras que los word embeddings personalizados proporcionaron una accuracy del 88 %, superando en un 11 % los resultados obtenidos con los preentrenados.**

Sin embargo, como información más relevante, **los algoritmos de aprendizaje automático clásicos también alcanzaron una precisión del 88 % en nuestra tarea de clasificación de texto.** Este resultado nos lleva a reflexionar sobre la eficacia de los métodos tradicionales en comparación con las redes neuronales en situaciones específicas. **Mientras que en muchas ocasiones se suele recurrir a las redes neuronales, los métodos clásicos de aprendizaje automático pueden seguir siendo una opción sólida y eficiente en algunos casos, demostrando que la elección del enfoque depende en última instancia de la naturaleza única de los datos y la tarea.**

# Dificultades

En esta práctica he tenido la oportunidad de explorar distintos algoritmos y redes neuronales, por lo que me he enfrentado a **diversas dificultades** que me han ayudado a comprender mejor el contenido. A pesar de mis esfuerzos, **la brecha entre los resultados actuales y los deseados sigue siendo bastante grande**. En un principio pensé que sería más sencillo mejorar los resultados, pero tras hacer numerosas pruebas, me di cuenta de que esto no era así.

Una de las **áreas** en las que he invertido **mucho tiempo y esfuerzo es la optimización de los hiperparámetros**. He probado **distintas combinaciones, ajustando cuidadosamente valores como tasas de aprendizaje, dimensiones de capas y funciones de activación en el caso de redes neuronales**. Sin embargo, a pesar de estos esfuerzos, **no he logrado obtener los resultados deseados** (que no necesariamente son «malos», sino que quería obtener un *accuracy* más alto **texto en negrita**).

La dificultad principal que encontraba era el **gran gasto computacional de realizar algunas pruebas**. También el empeoramiento que muchas veces encontraba tras probar con distintas opciones.

Esperaba que al probar diferentes configuraciones, sería posible encontrar una combinación que impulsara significativamente el rendimiento. Sin embargo, hasta ahora, la mejora no ha alcanzado el nivel de impacto que quería alcanzar.

Estas dificultades me han llevado a reflexionar sobre la **complejidad de la optimización de algoritmos y redes aplicado al análisis de sentimientos**. Llevar a cabo esta tarea de forma satisfactoria no es tan sencillo como pudiese parecer.
"""