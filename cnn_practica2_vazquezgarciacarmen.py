# -*- coding: utf-8 -*-
"""Practica2_VazquezGarciaCarmen.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1do_pFLfZnS4-mYrnU6dmzmxsQckVVEfV

#**Práctica 2 de Redes neuronales para el Procesamiento del Lenguaje Natural**

#Tema 2: Redes convolucionales para PLN


### Alumna: Carmen Vázquez García

Durante esta práctica, **se utilizan los materiales aportados para la primera práctica**. Por este motivo, **los apartado de carga, visualización y preprocesamiento de los datos son los mismos que en la práctica anterior**.

Para comenzar se comienza con la carga de *dataset* y un análisis de los datos que encontramos, visualizando los posibles problemas, soluciones encontradas, niveles de balanceo...

Luego se procede a la limpieza y preprocesado del texto, para poder dejar el texto preparado para que sea procoesado por los sistemas de aprendizaje automático que se utilizarán posteriormente.

Tras ello, se recurre a estos algoritmos de aprendizaje automático clásico. En este caso, se recurre a una amplia variedad de algoritmos, entre los que se explorarán distintos parámetros para la obtención de mejores resultados.
Para finalizar, se pondrá el foco en los sistemas basados en redes neuronales, donde se explorarán distintos hiperparámetros para la mejora de estas.

En primer lugar se dejarán las librerías necesarias preparadas para su uso, como se muestra a continuación:
"""

# Commented out IPython magic to ensure Python compatibility.
# Librerias necesarias
# Matplotlib conf
import matplotlib.pyplot as plt
# %matplotlib inline
params = {'legend.fontsize': 'x-large',
          'figure.figsize': (15, 5),
         'axes.labelsize': 'x-large',
         'axes.titlesize':'x-large',
         'xtick.labelsize': 40,
         'ytick.labelsize': 40
}
plt.rcParams.update(params)
# Seaborn conf
import seaborn as sns
sns.set(style='darkgrid')
sns.set_palette(sns.color_palette("Blues"))

import sys

# Procesado de datos
import pandas
import numpy as np
import operator

# Modelos
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer
from sklearn.feature_selection import SelectKBest, chi2

from sklearn.metrics import classification_report, accuracy_score
from sklearn.metrics import confusion_matrix

import warnings
warnings.filterwarnings('ignore')

"""## *Dataset*

Para comenzar con el tratamiento de datos, es necesario cargarlos y prepararlos para el entrenamiento. En este caso, como se indica en la práctica, no se utilizarán los datos en local. Recurriremos a los contenidos especificando rutas relativas y no absolutas.
"""

# Mediante estos comandos podemos enlazar nuestro notebook en Colab con nuestro almacenamiento en Google Drive
from google.colab import drive
drive.mount('/content/drive/')

# Cargando el dataset mediante rutas relativas (Colab)
training_set = pandas.read_csv("/content/drive/MyDrive/train_reviews.csv", quotechar='"', header=0, sep=",")
test_set = pandas.read_csv("/content/drive/My Drive/test_reviews.csv", quotechar='"', header=0, sep=",")

"""En este caso no se nos han proporcionado las etiquetas del conjunto de test. Pero más adelante recurriremos a otra forma para poder evaluar nuestros modelos.

Para saber cómo debemos trabajar con el *dataset*, es esencial visualizarlo. Por tanto, a continuación inspeccionamos el conjunto de entrenamiento y el de test.
"""

# Inspeccionamos el aspecto del conjunto de entrenamiento
training_set.head(30)

# Inspeccionamos el aspecto del conjunto de test
test_set.head(30)

"""Como se ha observado, el texto en general está bastante limpio. Aun así, encontramos algunas etiquetas que se deben eliminar, como algunas etiquetas. Esto lo realizaremos una vez hayamos observado que en la columna `sentiment` no hay datos distintos a *negative* y *positive*, paso que veremos a continuación."""

accepted_labels = {"negative", "positive"}
rows_with_problems = training_set[training_set.sentiment.isin(accepted_labels) == False]
rows_with_problems

"""Por lo que hemos podido observar, no se muestra ningún resultado que no sea *negative* o *positive*. Por tanto, no hay ninguna
información en la columna `sentiment` que no sean estos dos valores. Esto deja claro que no tenemos que modificar nada de esta columna.

Tras haber confirmado que no existe ninguna problemática con la columna `sentiment`, procedemos con la limpieza de la columna `review` de ambos conjuntos. Anteriormente vimos que lo único que ensuciaba el contenido eran algunas etiquetas (en este caso, saltos de línea), por lo que las sustituiremos por un espacio.
"""

# Sustituimos las etiquetas de salto de página en la columna 'review' por un espacio
training_set['review'] = training_set['review'].str.replace(r'<br\s*/?>', ' ')

# Creamos un archivo nuevo con el dataframe limpio
training_set.to_csv('training_set_limpio.csv', index=False)

# Inspeccionamos el aspecto del conjunto de entrenamiento limpio
training_set.head(30)

"""A continuación, realizamos exactamente el mismo proceso con la columna `review` de test_set."""

# Reemplazamos las etiquetas de salto de página por un espacio en la columna 'review' del test_set
test_set['review'] = test_set['review'].str.replace(r'<br\s*/?>', ' ')

# Crear un archivo nuevo con el dataframe limpio del test_set
test_set.to_csv('test_set_limpio.csv', index=False)

# Inspeccionamos el aspecto del conjunto de entrenamiento limpio
test_set.head(30)

"""Al volver a visualizar la columna `sentiment` hemos podido observar que las etiquetas se han sustituido por espacios, por lo que ahora podemos juntar ambos sets en un mismo *dataset* para facilitar el preprocesado."""

dataset = pandas.concat([training_set,test_set])

"""# Inspección de los datos

Comenzaremos por observar la distribución de las dos clases (*negative* y *positive*) para ver si existe algún desbalanceo de datos.
"""

from collections import Counter
sns.countplot(data=training_set, x=training_set.sentiment, order=[x for x, count in sorted(Counter(training_set.sentiment).items(), key=lambda x: -x[1])], palette="Blues_r")
plt.xticks(rotation=90)

"""Como hemos podido observar, no hay ningún tipo de desbalanceo. Por tanto, no nos tenemos que preocupar por clases desbalanceadas.

Para seguir con la inspección de datos, vamos a agrupar por sentimiento (positivo o negativo) y concatenar por cada grupo en una sola cadena de texto. Luego, utilizaremos un tokenizador para dividir estas cadenas de texto en palabras. Tras esto, se cuenta la frecuencia de cada palabra y se muestran las 25 palabras más comunes en un gráfico de barras para *negative* y *positive*.
"""

import pandas as pd
from matplotlib import interactive

# Agrupamos los textos por clases
df = pd.DataFrame({"review": training_set.review, "sentiment": training_set.sentiment})
grouped = df.groupby(["sentiment"]).apply(lambda x: x["review"].sum())
grouped_df = pd.DataFrame({"sentiment": grouped.index, "review": grouped.values})

# Tokenizamos las frases utilizando el espacio en blanco como separador
from nltk.tokenize import WhitespaceTokenizer
tokenizer = WhitespaceTokenizer()

for ii, review in enumerate(grouped_df.review):
    pd.DataFrame(tokenizer.tokenize(review)).apply(pd.value_counts).head(25).plot(kind="bar")
    plt.title(grouped_df.sentiment[ii], fontsize=20)
    plt.xticks(fontsize=15)
    interactive(True)
    plt.show()

"""Al observar las palabras más repetidas, lo que se podía esperar era encontrar términos representativos que nos ayudaran a distinguir entre reseñas positivas y negativas. Sin embargo, notamos que hay conjuntos de palabras muy similares que aportan poca información en ambas categorías, las ***stopwords***. Su presencia afecta negativamente el rendimiento del clasificador, por lo que es necesario eliminarlas. De hecho, debido a estas palabras, nos cuesta percibir la diferencia entre reseñas negativas y positivas, ya que prácticamente lo único que encontramos son estas palabras que no aportan información relevante.

También notamos que algunas palabras se consideran diferentes simplemente por la diferencia de mayúsculas y minúsculas, como *The* y *the*. Para abordar este problema, debemos **normalizar** las palabras.

Además, encontramos variantes de un mismo verbo, como *are* e *is*. Para lidiar con esto, podemos aplicar técnicas como el ***stemming*** o la **lematización** para reducir las palabras a su forma base y asegurarnos de que se traten de manera coherente en el análisis de texto.

## Preprocesado y preparación de datos


Comenzaremos con un preprocesado básico del *pipeline* de PLN
 - Extracción de palabras
 - Lematización para evitar que palabras con la misma raíz se repitan
 - *Stemming* para agrupar palabras con el mismo significado
 - Eliminación de *stopwords*

Para ello, vamos a crear la función **process_text** que engloba todo el preprocesado del texto. En este caso, utilizaremos la librería NLTK.
"""

from nltk.stem import *
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords

import re

import nltk
nltk.download('stopwords')
nltk.download('wordnet')

def process_text(raw_text):
    # Consideramos únicamente letras utilizando una expresión regular
    letters_only = re.sub("[^a-zA-Z]", " ",raw_text)
    # Convertimos todo a minúsculas
    words = letters_only.lower().split()

    # Eliminamos las stopwords
    stops = set(stopwords.words("english"))
    not_stop_words = [w for w in words if not w in stops]

    # Lematización
    wordnet_lemmatizer = WordNetLemmatizer()
    lemmatized = [wordnet_lemmatizer.lemmatize(word) for word in not_stop_words]

    # Stemming
    stemmer = PorterStemmer()
    stemmed = [stemmer.stem(word) for word in lemmatized]

    return( " ".join( stemmed ))

"""Ahora que disponemos de la función, la aplicamos al contenido textual del dataset: `review`. Para mantener el contenido original, creamos una nueva columna."""

dataset['clean_review'] = dataset['review'].apply(lambda x: process_text(x))
dataset.head()

"""A continuación, visualizamos lo que tenemos hasta el momento:"""

df = pd.DataFrame({"review": dataset[0:len(training_set)].clean_review, "sentiment": dataset[0:len(training_set)].sentiment})
grouped = df.groupby(["sentiment"]).apply(lambda x: x["review"].sum())
grouped_df = pd.DataFrame({"sentiment": grouped.index, "review": grouped.values})

from nltk.tokenize import WhitespaceTokenizer
tokenizer = WhitespaceTokenizer()

for ii, text in enumerate(grouped_df.review):
    pd.DataFrame(tokenizer.tokenize(text)).apply(pd.value_counts).head(25).plot(kind="bar")
    plt.title(grouped_df.sentiment[ii], fontsize=20)
    plt.xticks(fontsize=15)
    interactive(True)
    plt.show()

"""Como se puede apreciar en ambos gráficos, ha cambiado bastante. Hemos eliminado las *stopwords*, por lo que las palabras que observamos ahora son mucho más representativas. Además, se aprecia un uso más distintivo de algunas palabras entre las reseñas positivas y negativas, ya que antes debido a las *stopwords* el contenido era prácticamente el mismo.

Por ejemplo, aunque en las reseñas negativas también encontramos el adjetivo *good*, podemos ver que hay bastante uso también de *bad*, como podíamos esperar en la reseña negativa. Sin embargo, en el caso de las reseñas positivas, no encotramos *bad*. Lo que predomina es el adjetivo *good*, al ser reseñas positivas.

**Para continuar, vamos a divir nuestro *dataset* de nuevo para obtener el conjunto de entrenamiento y el de test, que se ha realizado de la siguiente forma:**

**X_train:** Se coge desde el inicio (índice 0) hasta el índice que representa el final del conjunto de entrenamiento (denotado como len(training_set)). Este subconjunto de datos consiste en las columnas `id` y `clean_review` del conjunto de entrenamiento, que se utilizan como características de entrada para el modelo.

**y_train:** Al igual que X_train, se toma una porción del *dataset* desde el inicio hasta el final del conjunto de entrenamiento, pero esta vez se extraen solo las etiquetas de clase `sentiment` correspondientes a las reseñas en el conjunto de entrenamiento.

**X_test:** Para el conjunto de prueba, se toma una porción de dataset que va desde el final del conjunto de entrenamiento (índice len(training_set)) hasta el final de *dataset*. Esto incluye las mismas columnas que en X_train, pero corresponden a las reseñas del conjunto de prueba.

**y_test:** Al igual que X_test, se coge desde el final del conjunto de entrenamiento hasta el final del *dataset* y se extraen las etiquetas de clase `sentiment` correspondientes a las reseñas en el conjunto de prueba.
"""

X_train = dataset[0:len(training_set)][["id", "clean_review"]]
y_train = dataset[0:len(training_set)][["sentiment"]]
X_test = dataset[len(training_set):len(dataset)][["id", "clean_review"]]
y_test = dataset[len(training_set):len(dataset)][["sentiment"]]

# Hot encoding para sentiment
from sklearn import preprocessing
le = preprocessing.LabelEncoder()
le.fit(y_train.sentiment.values)
target_sentiment = le.classes_
encoded_y_train = le.transform(y_train.sentiment.values)

"""**Modelo de espacio vectorial**

Vamos a aplicar un modelo de espacio vectorial para representar documentos de texto. Utilizaremos la técnica de Bolsa de Palabras (BoW), que implica convertir el texto en vectores numéricos para facilitar su procesamiento por algoritmos de clasificación.

En este enfoque:


1.   Asignamos un identificador a cada palabra en nuestro conjunto de datos.
2.   Creamos una matriz llamada «document-term» utilizando CountVectorizer de sklearn. Esta matriz, denotada como X[i, j], representa la frecuencia de ocurrencia de cada palabra w en cada documento d. Cada fila i corresponde a un documento, y cada columna j a una palabra en el diccionario.

Al aplicar CountVectorizer a las reseñas limpias, obtenemos las características necesarias para ambos conjuntos de clases, negativas y positivas. Este enfoque nos permite convertir el contenido textual en información numérica estructurada que se pueda aprovechar por algoritmos de clasificación.
"""

count_vect = CountVectorizer(analyzer = "word") # set min_df = frequency

train_features = count_vect.fit_transform(X_train.clean_review)
test_features = count_vect.transform(X_test.clean_review)

"""Aunque el tamaño del vocabulario es muy grande, podemos inspeccionar hasta cierto punto los vectores que obtenemos tras realizar este paso:"""

# Observemos el vocabulario
print('Longitud del vocabulario: ')
print(len(count_vect.vocabulary_))
print()
print("Observemos el vocabulario:")
print(count_vect.vocabulary_)
print()

"""Transformamos uno de los datos de entrada en el vector de palabras que se va a utilizar como entrada en nuestros experimentos:"""

# Ejemplo original (primer título)
print("Original:")
print(dataset.review.head()[0])
print()

# Ejemplo tras el preprocesado
print("Preprocesado:")
print(X_train.clean_review[0])
print()

# Convertimos el primer título
v0 = count_vect.transform([X_train.clean_review[0]]).toarray()[0]
print('Convertido: ')
print(v0)
print()

# Es demasiado grande para observarlo, imprimimos su longitud
print('Longitud del vector: ')
print(len(v0))
print()

# ¿Cuántas palabras contiene la frase?
print('Número de palabras: ')
print(np.sum(v0))
print()

# ¿En qué posiciones del vector se encuentran las palabras?
result = np.where(v0 == 1)
print("Índices de las palabras")
print(result[0], sep='\n')
print()

# Recuperemos la frase original
print('Original:')
print(count_vect.inverse_transform(v0.reshape(1, -1)))
print()

"""Se observa cómo, partiendo de la frase original «People tried to make me believe that the premise [...]», tras el preprocesado se obtienen los tokens «peopl tri make believ premis [...]» (se ha normalizado, eliminado *stopwords* y se ha aplicado lematización y *stemming* a las palabras).

El vector resultante tras la transformación se compone de 31106 valores (tantos como palabras en el vocabulario), de las cuáles 271 serán 1, y el resto 0. Al tener tantas componentes ni siquiera podemos observar dónde se localizan los valores 1, ya que se trata de un vector one-hot muy disperso. Se pueden recuperar fácilmente las palabras originales para comprobar que el vector efectivamente representa la frase original.
Ahora extraemos las *features* del texto limpio de las reseñas.
"""

train_text_features = count_vect.fit_transform(X_train.clean_review)
test_text_features = count_vect.transform(X_test.clean_review)

"""## Pruebas con CNN"""

# Importamos capas densas, capas de embeddings y capas para el aplanado de dimensiones
from keras.layers import Dense, Dropout, Embedding, Flatten, GlobalMaxPool1D, GlobalMaxPooling1D, MaxPooling1D, Conv1D

# Importamos las herramientas para generar el modelo y trabajar con las entradas
from keras.models import Sequential, Model
from tensorflow.python.keras.models import Input
import tensorflow as tf

# Utilidades para el preprocesado de textos: tokenizador y rellenado de secuencias
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences

from sklearn.model_selection import train_test_split

# Herramienta para la gestión de las etiquetas de salida
from tensorflow.keras.utils import to_categorical

# Herramienta para implementar Early Stopping (criterios de parada del entrenamiento)
from keras.callbacks import EarlyStopping

# Utilidad para el trabajo con embeddings preentrenados
from keras.initializers import Constant

# Importamos la función pad_sequences para realizar el rellenado de secuencias.
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Importamos la clase Tokenizer para convertir textos en secuencias de números.
from tensorflow.keras.preprocessing.text import Tokenizer

# Importamos la función to_categorical para convertir etiquetas de clase en matrices binarias.
from tensorflow.keras.utils import to_categorical

# Importamos la clase LabelEncoder para codificar etiquetas de clase en números.
from sklearn.preprocessing import LabelEncoder

# Importamos la clase EarlyStopping para detener el entrenamiento temprano si ciertos criterios se cumplen.
from tensorflow.keras.callbacks import EarlyStopping

# Importamos la librería nltk para procesamiento de lenguaje natural.
import nltk

# Importamos la lista de palabras de detención (stopwords) de nltk.
from nltk.corpus import stopwords

# Importamos la función word_tokenize para dividir el texto en palabras.
from nltk.tokenize import word_tokenize

# Importamos la cadena (string) para trabajar con operaciones de cadena (aunque no se utiliza directamente en este código).
import string

# Con esta función podremos ver cómo evoluciona el entrenamiento y la validación de los modelos.

import matplotlib.pyplot as plt
plt.style.use('ggplot')

def plot_history(history):
    acc = history.history['acc']
    val_acc = history.history['val_acc']
    loss = history.history['loss']
    val_loss = history.history['val_loss']
    x = range(1, len(acc) + 1)

    plt.figure(figsize=(12, 5))
    plt.subplot(1, 2, 1)
    plt.plot(x, acc, 'b', label='Training acc')
    plt.plot(x, val_acc, 'r', label='Validation acc')
    plt.title('Training and validation accuracy')
    plt.legend()
    plt.subplot(1, 2, 2)
    plt.plot(x, loss, 'b', label='Training loss')
    plt.plot(x, val_loss, 'r', label='Validation loss')
    plt.title('Training and validation loss')
    plt.legend()

# Herramienta para la gestión de las etiquetas de salida
from tensorflow.keras.utils import to_categorical

Y_train = to_categorical(encoded_y_train, 2)
Y_test = to_categorical(le.transform(y_test.values), 2)

print("Dimensión de los vectores:", train_text_features.shape[1])

def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    import itertools
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')
    plt.figure()
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.show()

"""Una vez ya se han cargado las herramientas (librerías y código) necesarias para poder llevar a cabo las pruebas con las redes CNN, se realizan pruebas.
En primer lugar, se comienza con BoW.

## BoW

En BoW, cada documento se representa como un vector que cuenta la frecuencia de cada palabra en un conjunto predefinido de palabras (vocabulario). El código realiza el ajuste del vectorizador (count_vect.fit_transform) en los datos de entrenamiento (X_train.clean_review) y luego transforma tanto los datos de entrenamiento como los de prueba (count_vect.transform). El resultado será una representación numérica de los datos de texto en función de la frecuencia de las palabras en el vocabulario.

Es importante mencionar que el BoW no considera el orden de las palabras en el texto, solo la frecuencia de las palabras. **Se realizarán pruebas, pero para conservar el orden de las palabras, es más interesante el uso de Word Embeddings (Word2Vec, GloVe...).**

Para crear una red neuronal convolucional (CNN) aplicada a tareas de análisis de sentimientos, el orden de las palabras es importante. Por tanto, **las CNN para NLP suelen funcionar mejor considerando la secuencia de palabras en el texto**, ya que de esta forma se capturan patrones locales o n-gramas importantes para comprender el significado y la estructura del lenguaje.
Aun así, como ya se había mencionado previamente, **se probarán distintos métodos de representación.**

Para la primerar prueba he definido **dos capas convolucionales** (Conv1D) con **funciones de activación ReLU**, seguidas de **capas de pooling** (MaxPooling1D). Esto permite a la red aprender patrones locales y realizar un submuestreo para reducir la dimensionalidad.

También he añadido capas de **Dropout** después de las **capas de pooling**. Mediante el Dropout se intenta prevenir el sobreajuste al apagar aleatoriamente algunas neuronas durante el entrenamiento.

Tras esto, he añadido una **capa de aplanado Flatten** para transformar los datos de la salida de las capas convolucionales y de pooling en un formato que pueda servir a las capas densas.

He continuado con dos **capas densas** (Dense) para aprender representaciones más complejas y no lineales a partir de las características extraídas por las capas convolucionales.

Para finalizar, he compilado el modelo utilizando el **optimizador Adam y la
función de pérdida binary_crossentropy**. **Además, he implementado Early Stopping** para detener el entrenamiento si no se observa mejora en la métrica de pérdida en un número determinado de épocas. Así se busca prevenir el sobreajuste.
"""

from keras.models import Sequential
from keras.layers import Conv1D, GlobalMaxPooling1D, Dense

# Número de épocas
epochs = 20
# Tamaño de batch
batch_size = 32

# Definir una arquitectura de CNN más robusta con Early Stopping
model = Sequential()

# Capas convolucionales y de pooling
model.add(Conv1D(filters=32, kernel_size=5, activation='relu', input_shape=(train_text_features.shape[1], 1)))
model.add(MaxPooling1D(pool_size=2))
model.add(Dropout(0.25))

# Añadir más capas convolucionales y de pooling para aprender patrones más complejos
model.add(Conv1D(filters=64, kernel_size=5, activation='relu'))
model.add(MaxPooling1D(pool_size=2))
model.add(Dropout(0.25))

# Capa de aplanado
model.add(Flatten())

# Capas densas
model.add(Dense(units=128, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(units=1, activation='sigmoid'))

# Compilar el modelo
optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)
model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['acc'])

# Definir Early Stopping
early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

# Resumen del modelo
model.summary()

# Definir EarlyStopping
early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

# Entrenar el modelo y almacenar el historial en la variable history
history = model.fit(train_text_features.toarray().astype(float), encoded_y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2, callbacks=[early_stopping])

# Llamar a la función plot_history con el historial
plot_history(history)

predict_x=model.predict(test_text_features.toarray().astype(float))
y_pred = (predict_x > 0.5).astype(int)

# Se deshace la transformación de las predicciones para obtener las predicciones como negativas o positivas
y_pred = le.inverse_transform(y_pred)

print("Classification Report")
print
print(classification_report(y_test, y_pred, target_names=["negative", "positive"]))
cm = confusion_matrix(y_test, y_pred, labels=["negative", "positive"])

plot_confusion_matrix(cm, classes=["negative", "positive"])

print("Final Accuracy")
print(accuracy_score(y_test, y_pred))

"""Tras muchas pruebas con distintas CNN, no he conseguido lograr unos resultados muy buenos. **He probado muchos métodos para evitar el sobreajuste y no ha sido posible, esta CNN ha sido la que ha obtenido mejores resultados de todas las que he probado.**

## TF-IDF
"""

from sklearn.feature_extraction.text import TfidfTransformer

X_train["all"] = X_train[['clean_review']].apply(lambda x: ''.join(x), axis=1)
X_test["all"] = X_test[['clean_review']].apply(lambda x: ''.join(x), axis=1)

count_vect = CountVectorizer(analyzer = "word", max_features=10000)
train_features = count_vect.fit_transform(X_train['all'])
test_features = count_vect.transform(X_test['all'])

# Se utiliza L2 (norma euclídea) para normalizar los vectores resultantes
tfidf = TfidfTransformer(norm="l2")
train_text_tfidf_features = tfidf.fit_transform(train_features)
test_text_tfidf_features = tfidf.fit_transform(test_features)

from tensorflow.keras.utils import to_categorical

Y_train = to_categorical(encoded_y_train, 2)
Y_test = to_categorical(le.transform(y_test.values), 2)

print("Dimensión de los vectores:", train_text_tfidf_features.shape[1])

"""En esta implementación, he decidido probar una **CNN más simple**, ya que he tenido **muchos problemas con Google Colab a la hora de realizar la práctica (entrenamientos muy lentos y problemas con la RAM)**.

En primer lugar, he configurado la arquitectura que consta de una capa convolucional (Conv1D) con 32 filtros y un tamaño de una capa convolucional (Conv1D) con 32 filtros y un tamaño de kernel de 3.

Después de la capa convolucional, se ha agregado una capa de pooling (MaxPooling1D) con un tamaño de pool de 2 para reducir la dimensionalidad. He introducido una capa de aplanado (Flatten)y se han añadido dos capas densas (Dense).

La primera capa densa tiene 64 unidades y utiliza la función de activación ReLU. Además, se ha añadido una regularización L2 con un parámetro de 0.01 para controlar el sobreajuste. Se ha introducido una capa de Dropout con una tasa del 50% después de la primera capa densa, con el objetivo de regularizar el modelo y, una vez más, prevenir el sobreajuste.

La última capa densa tiene 2 unidades con la función de activación softmax. El modelo se ha compilado utilizando el optimizador Adam y la función de pérdida binary_crossentropy.
"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout
from tensorflow.keras.regularizers import l2

model = Sequential()

# Capa Conv1D con kernel_size=3 y filters=32
model.add(Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(train_text_tfidf_features.shape[1], 1)))
model.add(MaxPooling1D(pool_size=2))

# Aplanar la salida de la capa convolucional
model.add(Flatten())

# Capas densas con Dropout y regularización L2
model.add(Dense(units=64, activation='relu', kernel_regularizer=l2(0.01)))
model.add(Dropout(0.5))
model.add(Dense(units=2, activation='softmax'))

# Compilar el modelo
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])

# Imprimir el resumen del modelo
print(model_cnn.summary())

# Definir el objeto EarlyStopping
early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

history = model_cnn.fit(train_text_tfidf_features.toarray().astype(float), Y_train, epochs=40, batch_size=32, validation_split=0.2)
plot_history(history)

predict_x=model_cnn.predict(test_text_tfidf_features.toarray().astype(float))
y_pred=np.argmax(predict_x,axis=1)

# Se deshace la transformación de las predicciones para obtener las predicciones como negativas o positivas
y_pred = le.inverse_transform(y_pred)

print("Classification Report")
print
print(classification_report(y_test, y_pred, target_names=["negative", "positive"]))
cm = confusion_matrix(y_test, y_pred, labels=["negative", "positive"])

plot_confusion_matrix(cm, classes=["negative", "positive"])

print("Final Accuracy")
print(accuracy_score(y_test, y_pred))

"""**Se han obtenido un resultado muy similar al anterior.** Además de este modelo, se ha probado cambiando algunos hiperparámetros y añadiendo alguna capa convolucional más. Aun así, no ha habido cambios importantes, por lo que he decidido dejar exclusivamente esta prueba.

**No he realizado pruebas con TF ni con TFD-IDF con selección de características mediante chi2, ya que considero más interesante explorar y evaluar el rendimiento utilizando Word Embeddings. En el contexto de redes neuronales convolucionales (CNN), es más interesante recurrir a Word Embeddings.**

## Word embeddings

## Embeddings
"""

# Importamos capas densas, capas de embeddings y capas para el aplanado de dimensiones
from keras.layers import Dense, Embedding, Flatten, GlobalMaxPool1D

# Importamos las herramientas para generar el modelo y trabajar con las entradas
from keras.models import Sequential, Model
from tensorflow.python.keras.models import Input

# Utilidades para el preprocesado de textos: tokenizador y rellenado de secuencias
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences

from sklearn.model_selection import train_test_split

# Herramienta para la gestión de las etiquetas de salida
from tensorflow.keras.utils import to_categorical

# Herramienta para implementar Early Stopping (criterios de parada del entrenamiento)
from keras.callbacks import EarlyStopping

# Utilidad para el trabajo con embeddings preentrenados
from keras.initializers import Constant

# Número de épocas (iteraciones globales)
epochs = 40

# Dimensión de la capa de embeddings
emb_dim = 100

# Tamaño de batch (lote)
batch_size = 32

# Número de palabras tenidas en cuenta (reducción del vocabulario)
n_most_common_words = 50000

# Máxima longitud del documento
max_len = 256

tokenizer = Tokenizer()
tokenizer.fit_on_texts(X_train.clean_review)
sequences_train = tokenizer.texts_to_sequences(X_train.clean_review)
sequences_test = tokenizer.texts_to_sequences(X_test.clean_review)
vocab_size = len(tokenizer.word_index) + 1

print("Tamaño del vocabulario: ", vocab_size)

train_text_nn = pad_sequences(sequences_train, maxlen=max_len)
test_text_nn = pad_sequences(sequences_test, maxlen=max_len)

Y_train = to_categorical(encoded_y_train, 2)
Y_test = to_categorical(le.transform(y_test.values), 2)

from keras.callbacks import EarlyStopping
from keras.models import Sequential
from keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense, Dropout

model = Sequential()

# Capa de Embedding
model.add(Embedding(input_dim=vocab_size,
                    output_dim=emb_dim,
                    input_length=train_text_nn.shape[1]))

# Capas Convolutivas con dropout para regularización
model.add(Conv1D(filters=32, kernel_size=3, activation='relu'))
model.add(MaxPooling1D(pool_size=2))
model.add(Dropout(0.5))

model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))
model.add(MaxPooling1D(pool_size=2))
model.add(Dropout(0.5))

# Capa de Aplanado
model.add(Flatten())

# Capas Densas con dropout para regularización
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.5))

# Capa de Salida
model.add(Dense(2, activation='softmax'))

# Compilar el modelo
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])

# Imprimir resumen del modelo
print(model.summary())

history = model.fit(train_text_nn, Y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2, callbacks=[EarlyStopping(monitor='val_loss', mode='min', patience=7, min_delta=0.01)])
plot_history(history)

predict_x=model.predict(test_text_nn)
y_pred=np.argmax(predict_x,axis=1)

# Se deshace la transformación de las predicciones para obtener las predicciones como negativas o positivas
y_pred = le.inverse_transform(y_pred)

print("Classification Report")
print
print(classification_report(y_test, y_pred, target_names=["negative", "positive"]))
cm = confusion_matrix(y_test, y_pred, labels=["negative", "positive"])

plot_confusion_matrix(cm, classes=["negative", "positive"])

print("Final Accuracy")
print(accuracy_score(y_test, y_pred))

from keras.callbacks import EarlyStopping
from keras.models import Sequential
from keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization

# Crear modelo CNN mejorado
model = Sequential()

# Capa de Embedding
model.add(Embedding(input_dim=vocab_size, output_dim=emb_dim, input_length=train_text_nn.shape[1]))

# Capas Convolutivas con Batch Normalization y Dropout para regularización
model.add(Conv1D(filters=64, kernel_size=5, activation='relu'))
model.add(BatchNormalization())
model.add(MaxPooling1D(pool_size=2))
model.add(Dropout(0.4))

model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))
model.add(BatchNormalization())
model.add(MaxPooling1D(pool_size=2))
model.add(Dropout(0.4))

# Capa de Aplanado
model.add(Flatten())

# Capas Densas con Batch Normalization y Dropout para regularización
model.add(Dense(128, activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.4))

# Capa de Salida
model.add(Dense(2, activation='softmax'))

# Compilar el modelo con tasa de aprendizaje reducida
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])

# Imprimir resumen del modelo
print(model.summary())

history = model.fit(train_text_nn, Y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2, callbacks=[EarlyStopping(monitor='val_loss', mode='min', patience=7, min_delta=0.01)])
plot_history(history)

predict_x=model.predict(test_text_nn)
y_pred=np.argmax(predict_x,axis=1)

# Se deshace la transformación de las predicciones para obtener las predicciones como negativas o positivas
y_pred = le.inverse_transform(y_pred)

print("Classification Report")
print
print(classification_report(y_test, y_pred, target_names=["negative", "positive"]))
cm = confusion_matrix(y_test, y_pred, labels=["negative", "positive"])

plot_confusion_matrix(cm, classes=["negative", "positive"])

print("Final Accuracy")
print(accuracy_score(y_test, y_pred))

"""Se han realizado distintas pruebas con distintos hiperparámetros pero no he logrado de ninguna forma mejorar el modelo.

## Embeddings preentrenados
"""

import os

embeddings_index = {}
# Colab
f = open('/content/drive/MyDrive/glove.6B.50d.txt')

for line in f:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs

f.close()

print('Found %s word vectors.' % len(embeddings_index))
word_index = tokenizer.word_index
print('Found %s unique tokens.' % len(word_index))

num_words = min(n_most_common_words, len(word_index)) + 1

embedding_matrix = np.zeros((num_words, emb_dim))

for word, i in word_index.items():
    if i > n_most_common_words:
        continue
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        # Si la palabra existe añadimos su vector a la matriz
        embedding_matrix[i] = embedding_vector
    else:
        # Si no existe, se le asigna un vector aleatorio
        embedding_matrix[i] = np.random.randn(emb_dim)

# Herramienta para implementar Early Stopping (criterios de parada del entrenamiento)
from keras.callbacks import EarlyStopping

from keras.models import Sequential
from keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense, Dropout
from keras.callbacks import EarlyStopping

model = Sequential()
model.add(Embedding(num_words,
                    emb_dim,
                    embeddings_initializer=Constant(embedding_matrix),
                    input_length=train_text_nn.shape[1],
                    trainable=True))

# Añadir capas convolucionales
model.add(Conv1D(filters=32, kernel_size=3, activation='relu'))
model.add(MaxPooling1D(pool_size=2))
model.add(Dropout(0.5))  # Añadir dropout para reducir el sobreajuste

model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))
model.add(MaxPooling1D(pool_size=2))
model.add(Dropout(0.5))  # Añadir dropout para reducir el sobreajuste

# Aplanar la salida de las capas convolucionales
model.add(Flatten())

# Capas densas con activación ReLU y dropout para reducir el sobreajuste
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.5))

model.add(Dense(2, activation='softmax'))

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])

print(model.summary())

history = model.fit(train_text_nn, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.2, callbacks=[EarlyStopping(monitor='val_loss', mode='min', patience=7, min_delta=0.01)])
plot_history(history)

predict_x=model.predict(test_text_nn)
y_pred=np.argmax(predict_x,axis=1)

# Se deshace la transformación de las predicciones para obtener las predicciones como negativas o positivas
y_pred = le.inverse_transform(y_pred)

print("Classification Report")
print
print(classification_report(y_test, y_pred, target_names=["negative", "positive"]))
cm = confusion_matrix(y_test, y_pred, lcabels=["negative", "positive"])

plot_confusion_matrix(cm, classes=["negative", "positive"])

print("Final Accuracy")
print(accuracy_score(y_test, y_pred))

from keras.callbacks import EarlyStopping
from keras.models import Sequential
from keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization
from keras.initializers import Constant
from keras.regularizers import l2

model = Sequential()

# Capa de Embedding
model.add(Embedding(num_words,
                    emb_dim,
                    embeddings_initializer=Constant(embedding_matrix),
                    input_length=train_text_nn.shape[1],
                    trainable=True))

# Añadir capas convolucionales
model.add(Conv1D(filters=32, kernel_size=3, activation='relu'))
model.add(BatchNormalization())
model.add(MaxPooling1D(pool_size=2))
model.add(Dropout(0.4))  # Ajustar la tasa de dropout

model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))
model.add(BatchNormalization())
model.add(MaxPooling1D(pool_size=2))
model.add(Dropout(0.4))  # Ajustar la tasa de dropout

# Aplanar la salida de las capas convolucionales
model.add(Flatten())

# Capas densas con activación ReLU y regularización L2
model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.01)))
model.add(Dropout(0.4))  # Ajustar la tasa de dropout

# Capa de salida
model.add(Dense(2, activation='softmax'))

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])

print(model.summary())

history = model.fit(train_text_nn, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.2, callbacks=[EarlyStopping(monitor='val_loss', mode='min', patience=7, min_delta=0.01)])
plot_history(history)

predict_x=model.predict(test_text_nn)
y_pred=np.argmax(predict_x,axis=1)

# Se deshace la transformación de las predicciones para obtener las predicciones como negativas o positivas
y_pred = le.inverse_transform(y_pred)

print("Classification Report")
print
print(classification_report(y_test, y_pred, target_names=["negative", "positive"]))
cm = confusion_matrix(y_test, y_pred, labels=["negative", "positive"])

plot_confusion_matrix(cm, classes=["negative", "positive"])

print("Final Accuracy")
print(accuracy_score(y_test, y_pred))

"""# Visualización CNN"""

import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Conv1D, GlobalMaxPooling1D, Embedding, Flatten, Dense
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.utils import to_categorical
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.callbacks import EarlyStopping
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import string

# Descargar el recurso 'punkt' y la lista de stopwords de NLTK
nltk.download('punkt')
nltk.download('stopwords')

# Obtener la lista de stopwords
stop_words = set(stopwords.words('english'))

# Función para realizar preprocesamiento de texto
def preprocess_text(text):
    text = text.lower()
    # Eliminar signos de puntuación
    text = text.translate(str.maketrans('', '', string.punctuation))
    # Tokenizar el texto
    tokens = word_tokenize(text)
    # Eliminar stopwords
    tokens = [word for word in tokens if word not in stop_words]
    return ' '.join(tokens)

# Ajustar el tokenizer en el conjunto de entrenamiento
tokenizer = Tokenizer()
tokenizer.fit_on_texts(X_train.clean_review)

# Obtener las secuencias de texto
sequences_train = tokenizer.texts_to_sequences(X_train.clean_review)
sequences_test = tokenizer.texts_to_sequences(X_test.clean_review)

# Parámetros de configuración:
max_len = 50          # Corta los textos después de esta cantidad de palabras
embedding_dims = 50   # Dimensiones del embedding
filters = 20          # Número de filtros en la capa de convolución
kernel_size = 3       # Tamaño de los filtros en la capa de convolución
n_gram_size = 3       # Tamaño del n-grama

# Padding de secuencias
train_text_nn = pad_sequences(sequences_train, maxlen=max_len)
test_text_nn = pad_sequences(sequences_test, maxlen=max_len)

# Codificar las etiquetas
le = LabelEncoder()
encoded_y_train = le.fit_transform(y_train)
encoded_y_test = le.transform(y_test)

Y_train = to_categorical(encoded_y_train, 2)
Y_test = to_categorical(encoded_y_test, 2)

# Construir el modelo de Keras:
print('Construyendo el modelo...')
input_ = Input(shape=(max_len,))
embedding = Embedding(len(tokenizer.word_index) + 1, embedding_dims)(input_)
conv_layer = Conv1D(filters, kernel_size, activation='relu')(embedding)
pooled_layer = GlobalMaxPooling1D()(conv_layer)
flatten_layer = Flatten()(pooled_layer)  # Agregar capa Flatten
dense_layer = Dense(32, activation='relu')(flatten_layer)
output_layer = Dense(2, activation='softmax')(dense_layer)

model = Model(inputs=input_, outputs=output_layer)
model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])
model.summary()

# Crear un modelo intermedio que devuelva las activaciones de la capa de convolución:
activation_model = Model(inputs=model.input, outputs=model.layers[2].output)  # Usar la capa de convolución (índice 2)

# Entrenar el modelo
epochs = 10
batch_size = 32

history = model.fit(train_text_nn, Y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2, callbacks=[EarlyStopping(monitor='val_loss', mode='min', patience=7, min_delta=0.01)])

def visualize_ngram_activations(model, data, word_index, n_gram_size):
    activations = model.predict(data)

    # Obtener el mapeo de índice a palabra:
    reverse_word_index = {value: key for (key, value) in word_index.items()}

    # Obtener n-gramas de 3 palabras incluyendo índices nulos o fuera del rango
    all_indices = data[0]
    n_grams = [' '.join(reverse_word_index.get(idx, '') for idx in all_indices[i:i+n_gram_size]) for i in range(0, len(all_indices) - n_gram_size + 1, 1)]

    # Crear un gráfico para todas las activaciones:
    plt.figure(figsize=(15, 6))
    for i, filter_activations in enumerate(activations[0].T):
        activations_slice = filter_activations[:len(n_grams)]
        plt.plot(activations_slice, label=f'Filtro {i}')

    # Configurar el eje X con n-gramas y sus índices (sin mostrar el índice)
    x_ticks_labels = [f'{n_grams[i]}' if all_indices[i] != 0 else '' for i in range(len(n_grams))]
    plt.xticks(range(len(n_grams)), x_ticks_labels, rotation=45, ha='right')

# Introducir una nueva oración y realizar preprocesamiento
custom_sentence = "Despite the talented, marvelous and interesting cast, this film falls flat with a weak plot and uninspired moments that fail to captivate the audience."
preprocessed_sentence = preprocess_text(custom_sentence)

# Tokenizar la nueva oración y realizar preprocesamiento
tokenizer = Tokenizer()
tokenizer.fit_on_texts([preprocessed_sentence])
sequences_custom_sentence = tokenizer.texts_to_sequences([preprocessed_sentence])
sample_custom_sentence = pad_sequences(sequences_custom_sentence, maxlen=max_len)

# Visualizar las activaciones para la nueva oración con n-gramas
visualize_ngram_activations(activation_model, sample_custom_sentence, tokenizer.word_index, n_gram_size)

import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Conv1D, GlobalMaxPooling1D, Embedding, Flatten, Dense
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.utils import to_categorical
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.callbacks import EarlyStopping
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import string

# Descargar el recurso 'punkt' y la lista de stopwords de NLTK
nltk.download('punkt')
nltk.download('stopwords')

# Obtener la lista de stopwords
stop_words = set(stopwords.words('english'))

# Función para realizar preprocesamiento de texto
def preprocess_text(text):
    text = text.lower()
    # Eliminar signos de puntuación
    text = text.translate(str.maketrans('', '', string.punctuation))
    # Tokenizar el texto
    tokens = word_tokenize(text)
    # Eliminar stopwords
    tokens = [word for word in tokens if word not in stop_words]
    return ' '.join(tokens)

# Ajustar el tokenizer en el conjunto de entrenamiento
tokenizer = Tokenizer()
tokenizer.fit_on_texts(X_train.clean_review)

# Obtener las secuencias de texto
sequences_train = tokenizer.texts_to_sequences(X_train.clean_review)
sequences_test = tokenizer.texts_to_sequences(X_test.clean_review)

# Parámetros de configuración:
max_len = 50          # Corta los textos después de esta cantidad de palabras
embedding_dims = 50   # Dimensiones del embedding
filters = 20          # Número de filtros en la capa de convolución
kernel_size = 3       # Tamaño de los filtros en la capa de convolución
n_gram_size = 3       # Tamaño del n-grama

# Padding de secuencias
train_text_nn = pad_sequences(sequences_train, maxlen=max_len)
test_text_nn = pad_sequences(sequences_test, maxlen=max_len)

# Codificar las etiquetas
le = LabelEncoder()
encoded_y_train = le.fit_transform(y_train)
encoded_y_test = le.transform(y_test)

Y_train = to_categorical(encoded_y_train, 2)
Y_test = to_categorical(encoded_y_test, 2)

# Construir el modelo de Keras:
print('Construyendo el modelo...')
input_ = Input(shape=(max_len,))
embedding = Embedding(len(tokenizer.word_index) + 1, embedding_dims)(input_)
conv_layer = Conv1D(filters, kernel_size, activation='relu')(embedding)
pooled_layer = GlobalMaxPooling1D()(conv_layer)
flatten_layer = Flatten()(pooled_layer)  # Agregar capa Flatten
dense_layer = Dense(32, activation='relu')(flatten_layer)
output_layer = Dense(2, activation='softmax')(dense_layer)

model = Model(inputs=input_, outputs=output_layer)
model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])
model.summary()

# Crear un modelo intermedio que devuelva las activaciones de la capa de convolución:
activation_model = Model(inputs=model.input, outputs=model.layers[2].output)  # Usar la capa de convolución (índice 2)

# Entrenar el modelo
epochs = 10
batch_size = 32

history = model.fit(train_text_nn, Y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2, callbacks=[EarlyStopping(monitor='val_loss', mode='min', patience=7, min_delta=0.01)])

import matplotlib.pyplot as plt
import numpy as np
from itertools import zip_longest

# Definir la función para visualizar las activaciones de las capas
def visualize_activations(model, data, word_index, n_gram_size):
    activations = model.predict(data)

    # Obtener el mapeo de índice a palabra:
    reverse_word_index = {value: key for (key, value) in word_index.items()}

    # Obtener n-gramas de 3 palabras incluyendo índices nulos o fuera del rango
    all_indices = data[0]
    n_grams = [' '.join(reverse_word_index.get(idx, '') for idx in all_indices[i:i+n_gram_size]) for i in range(0, len(all_indices) - n_gram_size + 1, 1)]

    # Crear un gráfico para todas las activaciones:
    plt.figure(figsize=(15, 6))
    for i, filter_activations in enumerate(activations[0].T):
        activations_slice = filter_activations[:len(n_grams)]
        plt.plot(activations_slice, label=f'Filtro {i}')

    # Configurar el eje X con n-gramas y sus índices (sin mostrar el índice)
    x_ticks_labels = [f'{n_grams[i]}' if all_indices[i] != 0 else '' for i in range(len(n_grams))]
    plt.xticks(range(len(n_grams)), x_ticks_labels, rotation=45, ha='right')

    # Configurar el gráfico
    plt.title('Activaciones de todos los filtros')
    plt.xlabel('N-gramas (Índices)')
    plt.ylabel('Activación')
    plt.legend(loc='upper left', bbox_to_anchor=(1, 1))  # Mover la leyenda fuera del área del gráfico
    plt.tight_layout()  # Ajustar la disposición del gráfico
    plt.show()

# Introducir una nueva oración y realizar preprocesamiento
custom_sentence = "Despite the talented, marvelous and interesting cast, this film falls flat with a weak plot and uninspired moments that fail to captivate the audience."
preprocessed_sentence = preprocess_text(custom_sentence)

# Tokenizar la nueva oración y realizar preprocesamiento
tokenizer = Tokenizer()
tokenizer.fit_on_texts([preprocessed_sentence])
sequences_custom_sentence = tokenizer.texts_to_sequences([preprocessed_sentence])
sample_custom_sentence = pad_sequences(sequences_custom_sentence, maxlen=max_len)

# Visualizar las activaciones para la nueva oración con n-gramas
visualize_activations(activation_model, sample_custom_sentence, tokenizer.word_index, n_gram_size)

import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Conv1D, GlobalMaxPooling1D, Embedding, Flatten, Dense
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.utils import to_categorical
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.callbacks import EarlyStopping
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import string

# Descargar el recurso 'punkt' y la lista de stopwords de NLTK
nltk.download('punkt')
nltk.download('stopwords')

# Obtener la lista de stopwords
stop_words = set(stopwords.words('english'))

# Función para realizar preprocesamiento de texto
def preprocess_text(text):
    text = text.lower()
    # Eliminar signos de puntuación
    text = text.translate(str.maketrans('', '', string.punctuation))
    # Tokenizar el texto
    tokens = word_tokenize(text)
    # Eliminar stopwords
    tokens = [word for word in tokens if word not in stop_words]
    return ' '.join(tokens)

# Ajustar el tokenizer en el conjunto de entrenamiento
tokenizer = Tokenizer()
tokenizer.fit_on_texts(X_train.clean_review)

# Obtener las secuencias de texto
sequences_train = tokenizer.texts_to_sequences(X_train.clean_review)
sequences_test = tokenizer.texts_to_sequences(X_test.clean_review)

# Parámetros de configuración:
max_len = 50          # Corta los textos después de esta cantidad de palabras
embedding_dims = 50   # Dimensiones del embedding
filters = 20          # Número de filtros en la capa de convolución
kernel_size = 3       # Tamaño de los filtros en la capa de convolución
n_gram_size = 3       # Tamaño del n-grama

# Padding de secuencias
train_text_nn = pad_sequences(sequences_train, maxlen=max_len)
test_text_nn = pad_sequences(sequences_test, maxlen=max_len)

# Codificar las etiquetas
le = LabelEncoder()
encoded_y_train = le.fit_transform(y_train)
encoded_y_test = le.transform(y_test)

Y_train = to_categorical(encoded_y_train, 2)
Y_test = to_categorical(encoded_y_test, 2)

# Construir el modelo de Keras:
print('Construyendo el modelo...')
input_ = Input(shape=(max_len,))
embedding = Embedding(len(tokenizer.word_index) + 1, embedding_dims)(input_)
conv_layer = Conv1D(filters, kernel_size, activation='relu')(embedding)
pooled_layer = GlobalMaxPooling1D()(conv_layer)
flatten_layer = Flatten()(pooled_layer)  # Agregar capa Flatten
dense_layer = Dense(32, activation='relu')(flatten_layer)
output_layer = Dense(2, activation='softmax')(dense_layer)

model = Model(inputs=input_, outputs=output_layer)
model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])
model.summary()

# Crear un modelo intermedio que devuelva las activaciones de la capa de convolución:
activation_model = Model(inputs=model.input, outputs=model.layers[2].output)  # Usar la capa de convolución (índice 2)

# Entrenar el modelo
epochs = 10
batch_size = 32

history = model.fit(train_text_nn, Y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2, callbacks=[EarlyStopping(monitor='val_loss', mode='min', patience=7, min_delta=0.01)])

import matplotlib.pyplot as plt
import numpy as np
from itertools import zip_longest

# Definir la función para visualizar las activaciones de las capas
def visualize_activations(model, data, word_index, n_gram_size):
    activations = model.predict(data)

    # Obtener el mapeo de índice a palabra:
    reverse_word_index = {value: key for (key, value) in word_index.items()}

    # Obtener n-gramas de 3 palabras incluyendo índices nulos o fuera del rango
    all_indices = data[0]
    n_grams = [' '.join(reverse_word_index.get(idx, '') for idx in all_indices[i:i+n_gram_size]) for i in range(0, len(all_indices) - n_gram_size + 1, 1)]

    # Crear un gráfico para todas las activaciones:
    plt.figure(figsize=(15, 6))
    for i, filter_activations in enumerate(activations[0].T):
        activations_slice = filter_activations[:len(n_grams)]
        plt.plot(activations_slice, label=f'Filtro {i}')

    # Configurar el eje X con n-gramas y sus índices (sin mostrar el índice)
    x_ticks_labels = [f'{n_grams[i]}' if all_indices[i] != 0 else '' for i in range(len(n_grams))]
    plt.xticks(range(len(n_grams)), x_ticks_labels, rotation=45, ha='right')

    # Configurar el gráfico
    plt.title('Activaciones de todos los filtros')
    plt.xlabel('N-gramas (Índices)')
    plt.ylabel('Activación')
    plt.legend(loc='upper left', bbox_to_anchor=(1, 1))  # Mover la leyenda fuera del área del gráfico
    plt.tight_layout()  # Ajustar la disposición del gráfico
    plt.show()

# Introducir una nueva oración y realizar preprocesamiento
custom_sentence = "This movie is a gem, featuring powerful performances and memorable scenes."
preprocessed_sentence = preprocess_text(custom_sentence)

# Tokenizar la nueva oración y realizar preprocesamiento
tokenizer = Tokenizer()
tokenizer.fit_on_texts([preprocessed_sentence])
sequences_custom_sentence = tokenizer.texts_to_sequences([preprocessed_sentence])
sample_custom_sentence = pad_sequences(sequences_custom_sentence, maxlen=max_len)

# Visualizar las activaciones para la nueva oración con n-gramas
visualize_activations(activation_model, sample_custom_sentence, tokenizer.word_index, n_gram_size)

"""# Resultados obtenidos

**Las gráficas muestran la activación de distintos filtros o características en respuesta a diferentes n-gramas en un contexto de análisis de sentimiento.** El eje horizontal está etiquetado como "N-gramas (índices)" y muestra una serie de n-gramas, que son fragmentos de texto. El eje vertical está etiquetado como "Activación" y varía de 0 a 0.5.

La gráfica muestra múltiples líneas, cada una representando un filtro diferente, y cómo su activación varía en respuesta a los distintos n-gramas. Algunos n-gramas provocan una activación más alta que otros, ya que en nuestro modelo hay n-gramas que son más relevantes y la característica está siendo detectada por el filtro correspondiente.

Por ejemplo, se puede observar claramente que claramente hay **ciertos n-gramas como "marvelous interesting cast film" que tienen picos de activación más altos que otros.** Esto podría sugerir que **estos n-gramas son más influyentes en la determinación del sentimiento de un texto, al menos según los filtros que se visualizan.**

**El análisis de estas activaciones ayuda a comprender qué palabras o secuencias de palabras son más significativas para el modelo al realizar análisis de sentimiento y cómo cada filtro contribuye a la interpretación final del sentimiento del texto.**

Los picos de activación en una gráfica como esta representan niveles elevados de respuesta de los filtros del modelo a ciertos n-gramas. **Estos modelos están diseñados para captar emociones negativas y positivas, por tanto, si un filtro está diseñado para detectar entusiasmo o alegría, un pico de activación cuando este filtro procesa el n-grama "marvelous interesting" podría sugerir que el modelo considera estas palabras como indicativas de un sentimiento positivo.** Del mismo modo, **si un filtro detecta decepción o crítica, un pico de activación para n-gramas que incluyan palabras asociadas a estos sentimientos indicaría una inclinación hacia un sentimiento negativo.**

**Estos filtros se entrenan para reconocer patrones en los datos que son indicativos de estas características.** Por ejemplo, algunos filtros podrían estar especializados en identificar emociones positivas a partir de la presencia de palabras como "excelente" o "maravilloso", mientras que otros podrían enfocarse en detectar palabras que actúan como intensificadores, como "muy" o "extremadamente", que pueden modificar la intensidad del sentimiento expresado.

Las activaciones altas en ciertos filtros pueden influir fuertemente en la clasificación final, especialmente si esos filtros están asociados con características que el modelo considera muy relevantes para el sentimiento. En la fase de aprendizaje, el modelo ajusta los pesos de las conexiones entre filtros y la capa de salida para optimizar la clasificación del sentimiento. La correlación entre la activación de filtros y la clasificación final depende de cómo el modelo ha sido entrenado y de qué tan bien cada filtro contribuye a la precisión general del modelo.

# Dificultades

He tenido muchas dificultades a la hora de llevar a cabo esta práctica. Aunque he buscado información y me he centrado en la explicación del capítulo 5.4.1 como se comentaba para realizar la práctica, siento que esto no ha sido suficiente.

No he sido capaz de crear modelos de calidad, en gran parte porque Google Colab ponía muchos impedimentos (sesiones sin GPU, problemática con la RAM...). Por tanto, no he podido optimizar los modelos tanto como me hubiese gustado. En una futura práctica si se solicita algo de mayor envergadura, no sé si con Google Colab sería suficiente.

Con respecto a la visualización de la CNN, me ha resultado muy complejo. Siento que no he sido capaz de dar con la clave, necesito buscar más material. Al no venir de una carrera técnica, se me ha dificultado esta práctica. La guía me fue muy útil en la primera práctica.

**Lo que más me ha costado ha sido la activación de las capas según la oración con la que probaba,** creo que no lo he hecho de la mejor forma ni de la más eficiente. Aun así, he intentado representarlo como he podido.

A continuación, adjunto una gran cantidad de pruebas que he llevado a cabo para acercarme al resultado obtenido:

### Lista de pruebas para la visualización
"""

# Crear un modelo intermedio que devuelva las activaciones de la capa de convolución:
activation_model = Model(inputs=model.input, outputs=model.layers[1].output)

# Función para visualizar las activaciones:
def visualize_activations(model, data, word_index):
    activations = model.predict(data)
    # Selecciona una muestra para visualizar:
    sample_activations = activations[0]

    # Obtener el mapeo de índice a palabra:
    reverse_word_index = {value: key for (key, value) in word_index.items()}

    # Para cada filtro, visualizar las activaciones:
    for i, filter_activations in enumerate(sample_activations.T):
        plt.figure(i+1)
        plt.title(f'Activaciones del filtro {i}')
        plt.plot(filter_activations, label=f'Filtro {i}')
        plt.xticks(range(len(filter_activations)), [reverse_word_index.get(word - 3, '?') for word in data[0]], rotation='vertical')
        plt.xlabel('Palabras')
        plt.ylabel('Activación')
        plt.legend()
        plt.show()

# Visualizar las activaciones para el ejemplo seleccionado:
word_index = imdb.get_word_index()
sample_text = x_test[0:1]  # Selecciona un ejemplo para visualizar
visualize_activations(activation_model, sample_text, word_index)

import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Conv1D, GlobalMaxPooling1D, Embedding, Flatten, Dense
from tensorflow.keras.datasets import imdb
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer

# Parámetros de configuración:
max_features = 10000  # Número de palabras a considerar como características
max_len = 500         # Corta los textos después de esta cantidad de palabras
embedding_dims = 50   # Dimensiones del embedding
filters = 20          # Número de filtros en la capa de convolución
kernel_size = 3       # Tamaño de los filtros en la capa de convolución

# Cargar el conjunto de datos de IMDB y preparar los datos de entrada:
print('Cargando datos...')
(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)
x_train = pad_sequences(x_train, maxlen=max_len)
x_test = pad_sequences(x_test, maxlen=max_len)

# Construir el modelo de Keras:
print('Construyendo el modelo...')
input_ = Input(shape=(max_len,))
embedding = Embedding(max_features, embedding_dims)(input_)
conv_layer = Conv1D(filters, kernel_size, activation='relu')(embedding)
pooled_layer = GlobalMaxPooling1D()(conv_layer)
flatten_layer = Flatten()(pooled_layer)  # Agregar capa Flatten
dense_layer = Dense(32, activation='relu')(flatten_layer)
output_layer = Dense(2, activation='softmax')(dense_layer)

model = Model(inputs=input_, outputs=output_layer)
model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])
model.summary()

# Crear un modelo intermedio que devuelva las activaciones de la capa de convolución:
activation_model = Model(inputs=model.input, outputs=model.layers[2].output)  # Usar la capa de convolución (índice 2)

# Función para visualizar las activaciones:
def visualize_activations(model, data, word_index):
    activations = model.predict(data)

    # Obtener el mapeo de índice a palabra:
    reverse_word_index = {value: key for (key, value) in word_index.items()}

    # Crear un gráfico para todas las activaciones:
    plt.figure(figsize=(15, 6))
    for i, filter_activations in enumerate(activations[0].T):
        plt.plot(filter_activations, label=f'Filtro {i}')

    # Configurar el eje X con palabras
    plt.xticks(range(len(data[0])), [reverse_word_index.get(word, '?') for word in data[0]], rotation='vertical', ha='left')

    # Configurar el gráfico
    plt.title('Activaciones de todos los filtros')
    plt.xlabel('Palabras')
    plt.ylabel('Activación')
    plt.legend()
    plt.show()

# Convertir las frases en secuencias de palabras
sequences_example1 = tokenizer.texts_to_sequences([example1])
sequences_example2 = tokenizer.texts_to_sequences([example2])

# Ajustar la longitud de la secuencia
sample_text1 = pad_sequences(sequences_example1, maxlen=max_len)
sample_text2 = pad_sequences(sequences_example2, maxlen=max_len)

# Visualizar las activaciones para los ejemplos de prueba
visualize_activations(activation_model, sample_text1, tokenizer.word_index)
visualize_activations(activation_model, sample_text2, tokenizer.word_index)

import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Conv1D, GlobalMaxPooling1D, Embedding, Flatten, Dense
from tensorflow.keras.datasets import imdb
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer

# Parámetros de configuración:
max_features = 10000  # Número de palabras a considerar como características
max_len = 500         # Corta los textos después de esta cantidad de palabras
embedding_dims = 50   # Dimensiones del embedding
filters = 20          # Número de filtros en la capa de convolución
kernel_size = 3       # Tamaño de los filtros en la capa de convolución

# Cargar el conjunto de datos de IMDB y preparar los datos de entrada:
print('Cargando datos...')
(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)
x_train = pad_sequences(x_train, maxlen=max_len)
x_test = pad_sequences(x_test, maxlen=max_len)

# Construir el modelo de Keras:
print('Construyendo el modelo...')
input_ = Input(shape=(max_len,))
embedding = Embedding(max_features, embedding_dims)(input_)
conv_layer = Conv1D(filters, kernel_size, activation='relu')(embedding)
pooled_layer = GlobalMaxPooling1D()(conv_layer)
flatten_layer = Flatten()(pooled_layer)  # Agregar capa Flatten
dense_layer = Dense(32, activation='relu')(flatten_layer)
output_layer = Dense(2, activation='softmax')(dense_layer)

model = Model(inputs=input_, outputs=output_layer)
model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])
model.summary()

# Crear un modelo intermedio que devuelva las activaciones de la capa de convolución:
activation_model = Model(inputs=model.input, outputs=model.layers[2].output)  # Usar la capa de convolución (índice 2)

# Función para visualizar las activaciones:
def visualize_activations(model, data, word_index):
    activations = model.predict(data)

    # Obtener el mapeo de índice a palabra:
    reverse_word_index = {value: key for (key, value) in word_index.items()}

    # Crear un gráfico para todas las activaciones:
    plt.figure(figsize=(15, 6))
    for i, filter_activations in enumerate(activations[0].T):
        plt.plot(filter_activations, label=f'Filtro {i}')

    # Configurar el eje X con palabras
    plt.xticks(rotation=45, ha='right')  # Rotar las etiquetas del eje X y alinearlas a la derecha

    # Configurar el gráfico
    plt.title('Activaciones de todos los filtros')
    plt.xlabel('Palabras')
    plt.ylabel('Activación')
    plt.legend(loc='upper left', bbox_to_anchor=(1, 1))  # Mover la leyenda fuera del área del gráfico
    plt.tight_layout()  # Ajustar la disposición del gráfico
    plt.show()

# Convertir las frases en secuencias de palabras
sequences_example1 = tokenizer.texts_to_sequences([example1])
sequences_example2 = tokenizer.texts_to_sequences([example2])

# Ajustar la longitud de la secuencia
sample_text1 = pad_sequences(sequences_example1, maxlen=max_len)
sample_text2 = pad_sequences(sequences_example2, maxlen=max_len)

# Visualizar las activaciones para los ejemplos de prueba
visualize_activations(activation_model, sample_text1, tokenizer.word_index)
visualize_activations(activation_model, sample_text2, tokenizer.word_index)

import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Conv1D, GlobalMaxPooling1D, Embedding, Flatten, Dense
from tensorflow.keras.datasets import imdb
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer

# Parámetros de configuración:
max_features = 10000  # Número de palabras a considerar como características
max_len = 50          # Corta los textos después de esta cantidad de palabras
embedding_dims = 50   # Dimensiones del embedding
filters = 20          # Número de filtros en la capa de convolución
kernel_size = 3       # Tamaño de los filtros en la capa de convolución
n_gram_size = 3       # Tamaño del n-grama

# Cargar el conjunto de datos de IMDB y preparar los datos de entrada:
print('Cargando datos...')
(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)
x_train = pad_sequences(x_train, maxlen=max_len)
x_test = pad_sequences(x_test, maxlen=max_len)

# Construir el modelo de Keras:
print('Construyendo el modelo...')
input_ = Input(shape=(max_len,))
embedding = Embedding(max_features, embedding_dims)(input_)
conv_layer = Conv1D(filters, kernel_size, activation='relu')(embedding)
pooled_layer = GlobalMaxPooling1D()(conv_layer)
flatten_layer = Flatten()(pooled_layer)  # Agregar capa Flatten
dense_layer = Dense(32, activation='relu')(flatten_layer)
output_layer = Dense(2, activation='softmax')(dense_layer)

model = Model(inputs=input_, outputs=output_layer)
model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])
model.summary()

# Crear un modelo intermedio que devuelva las activaciones de la capa de convolución:
activation_model = Model(inputs=model.input, outputs=model.layers[2].output)  # Usar la capa de convolución (índice 2)

# Función para visualizar las activaciones con n-gramas:
def visualize_ngram_activations(model, data, word_index, n_gram_size):
    activations = model.predict(data)

    # Obtener el mapeo de índice a palabra:
    reverse_word_index = {value: key for (key, value) in word_index.items()}

    # Obtener n-gramas de 3 palabras
    n_grams = [' '.join(reverse_word_index.get(idx, '?') for idx in data[0][i:i+n_gram_size]) for i in range(len(data[0]) - n_gram_size + 1)]

    # Crear un gráfico para todas las activaciones:
    plt.figure(figsize=(15, 6))
    for i, filter_activations in enumerate(activations[0].T):
        plt.plot(filter_activations, label=f'Filtro {i}')

    # Configurar el eje X con n-gramas
    plt.xticks(range(len(n_grams)), n_grams, rotation=45, ha='right')

    # Configurar el gráfico
    plt.title('Activaciones de todos los filtros')
    plt.xlabel('N-gramas')
    plt.ylabel('Activación')
    plt.legend(loc='upper left', bbox_to_anchor=(1, 1))  # Mover la leyenda fuera del área del gráfico
    plt.tight_layout()  # Ajustar la disposición del gráfico
    plt.show()

# Introducir una nueva reseña
new_review = "This is a fantastic movie with great performances and an engaging plot."

# Tokenizar la nueva reseña
tokenizer = Tokenizer()
tokenizer.fit_on_texts([new_review])
sequences_new_review = tokenizer.texts_to_sequences([new_review])
sample_new_review = pad_sequences(sequences_new_review, maxlen=max_len)

# Visualizar las activaciones para la nueva reseña con n-gramas
visualize_ngram_activations(activation_model, sample_new_review, tokenizer.word_index, n_gram_size)

import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Conv1D, GlobalMaxPooling1D, Embedding, Flatten, Dense
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import string
import nltk

# Descargar el recurso 'punkt' y la lista de stopwords de NLTK
nltk.download('punkt')
nltk.download('stopwords')

# Obtener la lista de stopwords
stop_words = set(stopwords.words('english'))

# Función para realizar preprocesamiento de texto
def preprocess_text(text):
    # Convertir a minúsculas
    text = text.lower()
    # Eliminar signos de puntuación
    text = text.translate(str.maketrans('', '', string.punctuation))
    # Tokenizar el texto
    tokens = word_tokenize(text)
    # Eliminar stopwords
    tokens = [word for word in tokens if word not in stop_words]
    return ' '.join(tokens)

# Parámetros de configuración:
max_features = 10000  # Número de palabras a considerar como características
max_len = 50          # Corta los textos después de esta cantidad de palabras
embedding_dims = 50   # Dimensiones del embedding
filters = 20          # Número de filtros en la capa de convolución
kernel_size = 3       # Tamaño de los filtros en la capa de convolución
n_gram_size = 3       # Tamaño del n-grama

# Construir el modelo de Keras:
# Construir el modelo de Keras:
print('Construyendo el modelo...')
input_ = Input(shape=(max_len,))
embedding = Embedding(len(tokenizer.word_index) + 1, embedding_dims)(input_)
conv_layer = Conv1D(filters, kernel_size, activation='relu')(embedding)
pooled_layer = GlobalMaxPooling1D()(conv_layer)
flatten_layer = Flatten()(pooled_layer)  # Agregar capa Flatten
dense_layer = Dense(32, activation='relu')(flatten_layer)
output_layer = Dense(2, activation='softmax')(dense_layer)

model = Model(inputs=input_, outputs=output_layer)
model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])
model.summary()

# Función para visualizar las activaciones con n-gramas:
def visualize_ngram_activations(model, data, word_index, n_gram_size):
    activations = model.predict(data)

    # Obtener el mapeo de índice a palabra:
    reverse_word_index = {value: key for (key, value) in word_index.items()}

    # Obtener n-gramas de 3 palabras de la oración
    n_grams = [' '.join(reverse_word_index.get(idx, '?') for idx in data[0][i:i+n_gram_size]) for i in range(2, len(data[0]) - n_gram_size + 1, n_gram_size)]

    # Crear un gráfico para todas las activaciones:
    plt.figure(figsize=(15, 6))
    for i, filter_activations in enumerate(activations[0].T):
        activations_slice = filter_activations[:len(n_grams)]
        plt.plot(activations_slice, label=f'Filtro {i}')

    # Configurar el eje X con n-gramas
    plt.xticks(range(len(n_grams)), n_grams, rotation=45, ha='right')

    # Configurar el gráfico
    plt.title('Activaciones de todos los filtros')
    plt.xlabel('N-gramas')
    plt.ylabel('Activación')
    plt.legend(loc='upper left', bbox_to_anchor=(1, 1))  # Mover la leyenda fuera del área del gráfico
    plt.tight_layout()  # Ajustar la disposición del gráfico
    plt.show()

# Introducir una nueva oración y realizar preprocesamiento
custom_sentence = "This is a very bad film with meaningless moments and dull scenes, although the actors are not horrendous."
preprocessed_sentence = preprocess_text(custom_sentence)

# Tokenizar la nueva oración y realizar preprocesamiento
tokenizer = Tokenizer()
tokenizer.fit_on_texts([preprocessed_sentence])
sequences_custom_sentence = tokenizer.texts_to_sequences([preprocessed_sentence])
sample_custom_sentence = pad_sequences(sequences_custom_sentence, maxlen=max_len)

# Visualizar las activaciones para la nueva oración con n-gramas
visualize_ngram_activations(activation_model, sample_custom_sentence, tokenizer.word_index, n_gram_size)

import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Conv1D, GlobalMaxPooling1D, Embedding, Flatten, Dense
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import string
import nltk

# Descargar el recurso 'punkt' y la lista de stopwords de NLTK
nltk.download('punkt')
nltk.download('stopwords')

# Obtener la lista de stopwords
stop_words = set(stopwords.words('english'))

# Función para realizar preprocesamiento de texto
def preprocess_text(text):
    # Convertir a minúsculas
    text = text.lower()
    # Eliminar signos de puntuación
    text = text.translate(str.maketrans('', '', string.punctuation))
    # Tokenizar el texto
    tokens = word_tokenize(text)
    # Eliminar stopwords
    tokens = [word for word in tokens if word not in stop_words]
    return ' '.join(tokens)

# Parámetros de configuración:
max_features = 10000  # Número de palabras a considerar como características
max_len = 50          # Corta los textos después de esta cantidad de palabras
embedding_dims = 50   # Dimensiones del embedding
filters = 20          # Número de filtros en la capa de convolución
kernel_size = 3       # Tamaño de los filtros en la capa de convolución
n_gram_size = 3       # Tamaño del n-grama

# Construir el modelo de Keras:
print('Construyendo el modelo...')
input_ = Input(shape=(max_len,))
embedding = Embedding(max_features, embedding_dims)(input_)
conv_layer = Conv1D(filters, kernel_size, activation='relu')(embedding)
pooled_layer = GlobalMaxPooling1D()(conv_layer)
flatten_layer = Flatten()(pooled_layer)  # Agregar capa Flatten
dense_layer = Dense(32, activation='relu')(flatten_layer)
output_layer = Dense(2, activation='softmax')(dense_layer)

model = Model(inputs=input_, outputs=output_layer)
model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])
model.summary()

def visualize_ngram_activations(model, data, word_index, n_gram_size):
    activations = model.predict(data)

    # Obtener el mapeo de índice a palabra:
    reverse_word_index = {value: key for (key, value) in word_index.items()}

    # Obtener n-gramas de 3 palabras excluyendo índices nulos o fuera del rango
    valid_indices = [idx for idx in data[0] if idx != 0 and idx in reverse_word_index]
    n_grams = [' '.join(reverse_word_index[idx] for idx in valid_indices[i:i+n_gram_size]) for i in range(0, len(valid_indices) - n_gram_size + 1, n_gram_size)]

    # Crear un gráfico para todas las activaciones:
    plt.figure(figsize=(15, 6))
    for i, filter_activations in enumerate(activations[0].T):
        activations_slice = filter_activations[:len(n_grams)]
        plt.plot(activations_slice, label=f'Filtro {i}')

    # Configurar el eje X con n-gramas
    plt.xticks(range(len(n_grams)), n_grams, rotation=45, ha='right')

    # Configurar el gráfico
    plt.title('Activaciones de todos los filtros')
    plt.xlabel('N-gramas')
    plt.ylabel('Activación')
    plt.legend(loc='upper left', bbox_to_anchor=(1, 1))  # Mover la leyenda fuera del área del gráfico
    plt.tight_layout()  # Ajustar la disposición del gráfico
    plt.show()

# Introducir una nueva oración y realizar preprocesamiento
custom_sentence = "This is a very bad film with meaningless moments and dull scenes, although the actors are not horrendous."
preprocessed_sentence = preprocess_text(custom_sentence)

# Tokenizar la nueva oración y realizar preprocesamiento
tokenizer = Tokenizer()
tokenizer.fit_on_texts([preprocessed_sentence])
sequences_custom_sentence = tokenizer.texts_to_sequences([preprocessed_sentence])
sample_custom_sentence = pad_sequences(sequences_custom_sentence, maxlen=max_len)

# Visualizar las activaciones para la nueva oración con n-gramas
visualize_ngram_activations(activation_model, sample_custom_sentence, tokenizer.word_index, n_gram_size)

import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Conv1D, GlobalMaxPooling1D, Embedding, Flatten, Dense
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import string
import nltk

# Descargar el recurso 'punkt' y la lista de stopwords de NLTK
nltk.download('punkt')
nltk.download('stopwords')

# Obtener la lista de stopwords
stop_words = set(stopwords.words('english'))

# Función para realizar preprocesamiento de texto
def preprocess_text(text):
    # Convertir a minúsculas
    text = text.lower()
    # Eliminar signos de puntuación
    text = text.translate(str.maketrans('', '', string.punctuation))
    # Tokenizar el texto
    tokens = word_tokenize(text)
    # Eliminar stopwords
    tokens = [word for word in tokens if word not in stop_words]
    return ' '.join(tokens)

# Parámetros de configuración:
max_features = 10000  # Número de palabras a considerar como características
max_len = 50          # Corta los textos después de esta cantidad de palabras
embedding_dims = 50   # Dimensiones del embedding
filters = 20          # Número de filtros en la capa de convolución
kernel_size = 3       # Tamaño de los filtros en la capa de convolución
n_gram_size = 3       # Tamaño del n-grama

# Construir el modelo de Keras:
print('Construyendo el modelo...')
input_ = Input(shape=(max_len,))
embedding = Embedding(max_features, embedding_dims)(input_)
conv_layer = Conv1D(filters, kernel_size, activation='relu')(embedding)
pooled_layer = GlobalMaxPooling1D()(conv_layer)
flatten_layer = Flatten()(pooled_layer)  # Agregar capa Flatten
dense_layer = Dense(32, activation='relu')(flatten_layer)
output_layer = Dense(2, activation='softmax')(dense_layer)

model = Model(inputs=input_, outputs=output_layer)
model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])
model.summary()

def visualize_ngram_activations(model, data, word_index, n_gram_size):
    activations = model.predict(data)

    # Obtener el mapeo de índice a palabra:
    reverse_word_index = {value: key for (key, value) in word_index.items()}

    # Obtener n-gramas de 3 palabras excluyendo índices nulos o fuera del rango
    valid_indices = [idx for idx in data[0] if idx != 0 and idx in reverse_word_index]
    n_grams = [' '.join(reverse_word_index[idx] for idx in valid_indices[i:i+n_gram_size]) for i in range(0, len(valid_indices) - n_gram_size + 1, 1)]

    # Crear un gráfico para todas las activaciones:
    plt.figure(figsize=(15, 6))
    for i, filter_activations in enumerate(activations[0].T):
        activations_slice = filter_activations[:len(n_grams)]
        plt.plot(activations_slice, label=f'Filtro {i}')

    # Configurar el eje X con n-gramas
    plt.xticks(range(len(n_grams)), n_grams, rotation=45, ha='right')

    # Configurar el gráfico
    plt.title('Activaciones de todos los filtros')
    plt.xlabel('N-gramas')
    plt.ylabel('Activación')
    plt.legend(loc='upper left', bbox_to_anchor=(1, 1))  # Mover la leyenda fuera del área del gráfico
    plt.tight_layout()  # Ajustar la disposición del gráfico
    plt.show()

# Introducir una nueva oración y realizar preprocesamiento
custom_sentence = "This is a very bad film with meaningless moments and dull scenes, although the actors are not horrendous."
preprocessed_sentence = preprocess_text(custom_sentence)

# Tokenizar la nueva oración y realizar preprocesamiento
tokenizer = Tokenizer()
tokenizer.fit_on_texts([preprocessed_sentence])
sequences_custom_sentence = tokenizer.texts_to_sequences([preprocessed_sentence])
sample_custom_sentence = pad_sequences(sequences_custom_sentence, maxlen=max_len)

# Visualizar las activaciones para la nueva oración con n-gramas
visualize_ngram_activations(activation_model, sample_custom_sentence, tokenizer.word_index, n_gram_size)

def visualize_ngram_activations(model, data, word_index, n_gram_size):
    activations = model.predict(data)

    # Obtener el mapeo de índice a palabra:
    reverse_word_index = {value: key for (key, value) in word_index.items()}

    # Obtener n-gramas de 3 palabras excluyendo índices nulos o fuera del rango
    valid_indices = [idx for idx in data[0] if idx != 0 and idx in reverse_word_index]
    n_grams = [' '.join(reverse_word_index[idx] for idx in valid_indices[i:i+n_gram_size]) for i in range(0, len(valid_indices) - n_gram_size + 1, 1)]

    # Crear un gráfico para todas las activaciones:
    plt.figure(figsize=(15, 6))
    for i, filter_activations in enumerate(activations[0].T):
        activations_slice = filter_activations[:len(n_grams)]
        plt.plot(activations_slice, label=f'Filtro {i}')

    # Configurar el eje X con n-gramas
    plt.xticks(range(len(n_grams)), n_grams, rotation=45, ha='right')

    # Configurar el gráfico
    plt.title('Activaciones de todos los filtros')
    plt.xlabel('N-gramas')
    plt.ylabel('Activación')
    plt.legend(loc='upper left', bbox_to_anchor=(1, 1))  # Mover la leyenda fuera del área del gráfico
    plt.tight_layout()  # Ajustar la disposición del gráfico
    plt.show()

# Visualizar las activaciones para la nueva oración con n-gramas
visualize_ngram_activations(activation_model, sample_custom_sentence, tokenizer.word_index, n_gram_size)

def visualize_ngram_activations(model, data, word_index, n_gram_size):
    activations = model.predict(data)

    # Obtener el mapeo de índice a palabra:
    reverse_word_index = {value: key for (key, value) in word_index.items()}

    # Obtener n-gramas de 3 palabras incluyendo índices nulos o fuera del rango
    all_indices = data[0]
    n_grams = [' '.join(reverse_word_index.get(idx, '') for idx in all_indices[i:i+n_gram_size]) for i in range(0, len(all_indices) - n_gram_size + 1, 1)]

    # Crear un gráfico para todas las activaciones:
    plt.figure(figsize=(15, 6))
    for i, filter_activations in enumerate(activations[0].T):
        activations_slice = filter_activations[:len(n_grams)]
        plt.plot(activations_slice, label=f'Filtro {i}')

    # Configurar el eje X con n-gramas y sus índices
    x_ticks_labels = [f'{n_grams[i]} ({all_indices[i]})' for i in range(len(n_grams))]
    plt.xticks(range(len(n_grams)), x_ticks_labels, rotation=45, ha='right')

    # Configurar el gráfico
    plt.title('Activaciones de todos los filtros')
    plt.xlabel('N-gramas (Índices)')
    plt.ylabel('Activación')
    plt.legend(loc='upper left', bbox_to_anchor=(1, 1))  # Mover la leyenda fuera del área del gráfico
    plt.tight_layout()  # Ajustar la disposición del gráfico
    plt.show()

# Visualizar las activaciones para la nueva oración con n-gramas
visualize_ngram_activations(activation_model, sample_custom_sentence, tokenizer.word_index, n_gram_size)

def visualize_ngram_activations(model, data, word_index, n_gram_size):
    activations = model.predict(data)

    # Obtener el mapeo de índice a palabra:
    reverse_word_index = {value: key for (key, value) in word_index.items()}

    # Obtener n-gramas de 3 palabras incluyendo índices nulos o fuera del rango
    all_indices = data[0]
    n_grams = [' '.join(reverse_word_index.get(idx, '') for idx in all_indices[i:i+n_gram_size]) for i in range(0, len(all_indices) - n_gram_size + 1, 1)]

    # Crear un gráfico para todas las activaciones:
    plt.figure(figsize=(15, 6))
    for i, filter_activations in enumerate(activations[0].T):
        activations_slice = filter_activations[:len(n_grams)]
        plt.plot(activations_slice, label=f'Filtro {i}')

    # Configurar el eje X con n-gramas y sus índices
    x_ticks_labels = [f'{n_grams[i]} ({all_indices[i]})' if all_indices[i] != 0 else '' for i in range(len(n_grams))]
    plt.xticks(range(len(n_grams)), x_ticks_labels, rotation=45, ha='right')

    # Configurar el gráfico
    plt.title('Activaciones de todos los filtros')
    plt.xlabel('N-gramas (Índices)')
    plt.ylabel('Activación')
    plt.legend(loc='upper left', bbox_to_anchor=(1, 1))  # Mover la leyenda fuera del área del gráfico
    plt.tight_layout()  # Ajustar la disposición del gráfico
    plt.show()

# Visualizar las activaciones para la nueva oración con n-gramas
visualize_ngram_activations(activation_model, sample_custom_sentence, tokenizer.word_index, n_gram_size)

def visualize_ngram_activations(model, data, word_index, n_gram_size):
    activations = model.predict(data)

    # Obtener el mapeo de índice a palabra:
    reverse_word_index = {value: key for (key, value) in word_index.items()}

    # Obtener n-gramas de 3 palabras incluyendo índices nulos o fuera del rango
    all_indices = data[0]
    n_grams = [' '.join(reverse_word_index.get(idx, '') for idx in all_indices[i:i+n_gram_size]) for i in range(0, len(all_indices) - n_gram_size + 1, 1)]

    # Crear un gráfico para todas las activaciones:
    plt.figure(figsize=(15, 6))
    for i, filter_activations in enumerate(activations[0].T):
        activations_slice = filter_activations[:len(n_grams)]
        plt.plot(activations_slice, label=f'Filtro {i}')

    # Configurar el eje X con n-gramas y sus índices (sin mostrar el índice)
    x_ticks_labels = [f'{n_grams[i]}' if all_indices[i] != 0 else '' for i in range(len(n_grams))]
    plt.xticks(range(len(n_grams)), x_ticks_labels, rotation=45, ha='right')


    # Configurar el gráfico
    plt.title('Activaciones de todos los filtros')
    plt.xlabel('N-gramas (Índices)')
    plt.ylabel('Activación')
    plt.legend(loc='upper left', bbox_to_anchor=(1, 1))  # Mover la leyenda fuera del área del gráfico
    plt.tight_layout()  # Ajustar la disposición del gráfico
    plt.show()

# Visualizar las activaciones para la nueva oración con n-gramas
visualize_ngram_activations(activation_model, sample_custom_sentence, tokenizer.word_index, n_gram_size)

def visualize_ngram_activations(model, data, word_index, n_gram_size):
    activations = model.predict(data)

    # Obtener el mapeo de índice a palabra:
    reverse_word_index = {value: key for (key, value) in word_index.items()}

    # Obtener n-gramas de 3 palabras incluyendo índices nulos o fuera del rango
    all_indices = data[0]
    n_grams = [' '.join(reverse_word_index.get(idx, '') for idx in all_indices[i:i+n_gram_size]) for i in range(0, len(all_indices) - n_gram_size + 1, 1)]

    # Crear un gráfico para todas las activaciones:
    plt.figure(figsize=(15, 6))
    for i, filter_activations in enumerate(activations[0].T):
        activations_slice = filter_activations[:len(n_grams)]
        plt.plot(activations_slice, label=f'Filtro {i}')

    # Configurar el eje X con n-gramas y sus índices (sin mostrar el índice)
    x_ticks_labels = [f'{n_grams[i]}' if all_indices[i] != 0 else '' for i in range(len(n_grams))]
    plt.xticks(range(len(n_grams)), x_ticks_labels, rotation=45, ha='right')

    # Configurar el gráfico
    plt.title('Activaciones de todos los filtros')
    plt.xlabel('N-gramas (Índices)')
    plt.ylabel('Activación')
    plt.legend(loc='upper left', bbox_to_anchor=(1, 1))  # Mover la leyenda fuera del área del gráfico
    plt.tight_layout(w_pad=15.0)  # Ajustar la disposición del gráfico con espacio adicional
    plt.show()

# Visualizar las activaciones para la nueva oración con n-gramas
visualize_ngram_activations(activation_model, sample_custom_sentence, tokenizer.word_index, n_gram_size)

def visualize_ngram_activations(model, data, word_index, n_gram_size):
    activations = model.predict(data)

    # Obtener el mapeo de índice a palabra:
    reverse_word_index = {value: key for (key, value) in word_index.items()}

    # Obtener n-gramas de 3 palabras incluyendo índices nulos o fuera del rango
    all_indices = data[0]
    n_grams = [' '.join(reverse_word_index.get(idx, '') for idx in all_indices[i:i+n_gram_size]) for i in range(0, len(all_indices) - n_gram_size + 1, 1)]

    # Crear un gráfico para todas las activaciones:
    plt.figure(figsize=(20, 6))  # Aumentar el tamaño de la figura
    for i, filter_activations in enumerate(activations[0].T):
        activations_slice = filter_activations[:len(n_grams)]
        plt.plot(activations_slice, label=f'Filtro {i}')

    # Configurar el eje X con n-gramas y sus índices (sin mostrar el índice)
    x_ticks_labels = [f'{n_grams[i]}' if all_indices[i] != 0 else '' for i in range(len(n_grams))]
    plt.xticks(range(len(n_grams)), x_ticks_labels, rotation=45, ha='right')

    # Configurar el gráfico
    plt.title('Activaciones de todos los filtros')
    plt.xlabel('N-gramas (Índices)')
    plt.ylabel('Activación')
    plt.legend(loc='upper left', bbox_to_anchor=(1, 1))  # Mover la leyenda fuera del área del gráfico
    plt.tight_layout()  # Ajustar la disposición del gráfico
    plt.show()

# Visualizar las activaciones para la nueva oración con n-gramas
visualize_ngram_activations(activation_model, sample_custom_sentence, tokenizer.word_index, n_gram_size)

def visualize_ngram_activations(model, data, word_index, n_gram_size):
    activations = model.predict(data)

    # Obtener el mapeo de índice a palabra:
    reverse_word_index = {value: key for (key, value) in word_index.items()}

    # Obtener n-gramas de 3 palabras incluyendo índices nulos o fuera del rango
    all_indices = data[0]
    n_grams = [' '.join(reverse_word_index.get(idx, '') for idx in all_indices[i:i+n_gram_size]) for i in range(0, len(all_indices) - n_gram_size + 1, 1)]

    # Crear un gráfico para todas las activaciones:
    plt.figure(figsize=(15, 6))
    for i, filter_activations in enumerate(activations[0].T):
        activations_slice = filter_activations[:len(n_grams)]

        # Solo mostrar valores de activación para los n-gramas, y 0 para índices nulos o fuera de rango
        activations_slice = [activation if idx not in [0, -1] else 0 for idx, activation in zip(all_indices, activations_slice)]

        plt.plot(activations_slice, label=f'Filtro {i}')

    # Configurar el eje X con n-gramas y sin mostrar índices
    x_ticks_labels = [f'{n_grams[i]}' if all_indices[i] != 0 else '' for i in range(len(n_grams))]
    plt.xticks(range(len(n_grams)), x_ticks_labels, rotation=45, ha='right')

    # Configurar el gráfico
    plt.title('Activaciones de todos los filtros')
    plt.xlabel('N-gramas (Índices)')
    plt.ylabel('Activación')
    plt.legend(loc='upper left', bbox_to_anchor=(1, 1))  # Mover la leyenda fuera del área del gráfico
    plt.tight_layout()  # Ajustar la disposición del gráfico
    plt.show()

# Visualizar las activaciones para la nueva oración con n-gramas
visualize_ngram_activations(activation_model, sample_custom_sentence, tokenizer.word_index, n_gram_size)

def visualize_ngram_activations(model, data, word_index, n_gram_size):
    activations = model.predict(data)

    # Obtener el mapeo de índice a palabra:
    reverse_word_index = {value: key for (key, value) in word_index.items()}

    # Obtener n-gramas de 3 palabras incluyendo índices nulos o fuera del rango
    all_indices = data[0]
    n_grams = [' '.join(reverse_word_index.get(idx, '') for idx in all_indices[i:i+n_gram_size]) for i in range(0, len(all_indices) - n_gram_size + 1, 1)]

    # Crear un gráfico para todas las activaciones:
    plt.figure(figsize=(15, 6))
    for i, filter_activations in enumerate(activations[0].T):
        activations_slice = filter_activations[:len(n_grams)]

        # Solo mostrar valores de activación para los n-gramas, y 0 para índices nulos o fuera de rango
        activations_slice = [activation if idx not in [0, -1] else None for idx, activation in zip(all_indices, activations_slice)]

        plt.plot(activations_slice, label=f'Filtro {i}')

    # Configurar el eje X con n-gramas y sin mostrar índices
    x_ticks_labels = [f'{n_grams[i]}' if all_indices[i] != 0 else '' for i in range(len(n_grams))]
    x_positions = [i for i, label in enumerate(x_ticks_labels) if label != '']

    plt.xticks(x_positions, x_ticks_labels, rotation=45, ha='right')

    # Configurar el gráfico
    plt.title('Activaciones de todos los filtros')
    plt.xlabel('N-gramas (Índices)')
    plt.ylabel('Activación')
    plt.legend(loc='upper left', bbox_to_anchor=(1, 1))  # Mover la leyenda fuera del área del gráfico
    plt.tight_layout()  # Ajustar la disposición del gráfico
    plt.show()

# Visualizar las activaciones para la nueva oración con n-gramas
visualize_ngram_activations(activation_model, sample_custom_sentence, tokenizer.word_index, n_gram_size)

def visualize_ngram_activations(model, data, word_index, n_gram_size):
    activations = model.predict(data)

    # Obtener el mapeo de índice a palabra:
    reverse_word_index = {value: key for (key, value) in word_index.items()}

    # Obtener n-gramas de 3 palabras incluyendo índices nulos o fuera del rango
    all_indices = data[0]
    n_grams = [' '.join(reverse_word_index.get(idx, '') for idx in all_indices[i:i+n_gram_size]) for i in range(0, len(all_indices) - n_gram_size + 1, 1)]

    # Crear un gráfico para todas las activaciones:
    plt.figure(figsize=(15, 6))
    for i, filter_activations in enumerate(activations[0].T):
        activations_slice = filter_activations[:len(n_grams)]

        # Solo mostrar valores de activación para los n-gramas, y 0 para índices nulos o fuera de rango
        activations_slice = [activation if idx not in [0, -1] else None for idx, activation in zip(all_indices, activations_slice)]

        plt.plot(activations_slice, label=f'Filtro {i}')

    # Configurar el eje X con n-gramas y sus índices
    x_positions = [i for i in range(1, len(n_grams) + 1)]
    plt.xticks(x_positions, n_grams, rotation=45, ha='right')

    # Configurar el gráfico
    plt.title('Activaciones de todos los filtros')
    plt.xlabel('N-gramas (Índices)')
    plt.ylabel('Activación')
    plt.legend(loc='upper left', bbox_to_anchor=(1, 1))  # Mover la leyenda fuera del área del gráfico
    plt.tight_layout()  # Ajustar la disposición del gráfico
    plt.show()

# Visualizar las activaciones para la nueva oración con n-gramas
visualize_ngram_activations(activation_model, sample_custom_sentence, tokenizer.word_index, n_gram_size)