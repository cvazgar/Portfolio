# -*- coding: utf-8 -*-
"""Practicas4y5_VazquezGarcia_Carmen.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HqyrGyo596pCFrc9OoWBwoghwNRrqRVd

#**Prácticas 4 y 5 de Redes neuronales para el Procesamiento del Lenguaje Natural**

#Tema 4 Transformers
#Tema 5 Modelos contextuales para PLN

### Alumna: Carmen Vázquez García

Durante esta práctica, **se utilizan los materiales aportados para la primera práctica**. Por este motivo, **los apartado de carga, visualización y preprocesamiento de los datos son los mismos que en la práctica anterior**.

Para comenzar se comienza con la carga de *dataset* y un análisis de los datos que encontramos, visualizando los posibles problemas, soluciones encontradas, niveles de balanceo...

Luego se procede a la limpieza y preprocesado del texto, para poder dejar el texto preparado para que sea procoesado por los sistemas de aprendizaje automático que se utilizarán posteriormente.

Tras ello, se recurre a estos algoritmos de aprendizaje automático clásico. En este caso, se recurre a una amplia variedad de algoritmos, entre los que se explorarán distintos parámetros para la obtención de mejores resultados.
Para finalizar, se pondrá el foco en los sistemas basados en redes neuronales, donde se explorarán distintos hiperparámetros para la mejora de estas.

En primer lugar se dejarán las librerías necesarias preparadas para su uso, como se muestra a continuación:
"""

# Commented out IPython magic to ensure Python compatibility.
# Librerias necesarias
# Matplotlib conf
import matplotlib.pyplot as plt
# %matplotlib inline
params = {'legend.fontsize': 'x-large',
          'figure.figsize': (15, 5),
         'axes.labelsize': 'x-large',
         'axes.titlesize':'x-large',
         'xtick.labelsize': 40,
         'ytick.labelsize': 40
}
plt.rcParams.update(params)
# Seaborn conf
import seaborn as sns
sns.set(style='darkgrid')
sns.set_palette(sns.color_palette("Blues"))

import sys

# Procesado de datos
import pandas
import numpy as np
import operator

# Modelos
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer
from sklearn.feature_selection import SelectKBest, chi2

from sklearn.metrics import classification_report, accuracy_score
from sklearn.metrics import confusion_matrix

import warnings
warnings.filterwarnings('ignore')

"""## *Dataset*

Para comenzar con el tratamiento de datos, es necesario cargarlos y prepararlos para el entrenamiento. En este caso, como se indica en la práctica, no se utilizarán los datos en local. Recurriremos a los contenidos especificando rutas relativas y no absolutas.
"""

# Mediante estos comandos podemos enlazar nuestro notebook en Colab con nuestro almacenamiento en Google Drive
from google.colab import drive
drive.mount('/content/drive/')

# Cargando el dataset mediante rutas relativas (Colab)
training_set = pandas.read_csv("/content/drive/MyDrive/train_reviews.csv", quotechar='"', header=0, sep=",")
test_set = pandas.read_csv("/content/drive/My Drive/test_reviews.csv", quotechar='"', header=0, sep=",")

"""En este caso no se nos han proporcionado las etiquetas del conjunto de test. Pero más adelante recurriremos a otra forma para poder evaluar nuestros modelos.

Para saber cómo debemos trabajar con el *dataset*, es esencial visualizarlo. Por tanto, a continuación inspeccionamos el conjunto de entrenamiento y el de test.
"""

# Inspeccionamos el aspecto del conjunto de entrenamiento
training_set.head(30)

# Inspeccionamos el aspecto del conjunto de test
test_set.head(30)

"""Como se ha observado, el texto en general está bastante limpio. Aun así, encontramos algunas etiquetas que se deben eliminar, como algunas etiquetas. Esto lo realizaremos una vez hayamos observado que en la columna `sentiment` no hay datos distintos a *negative* y *positive*, paso que veremos a continuación."""

accepted_labels = {"negative", "positive"}
rows_with_problems = training_set[training_set.sentiment.isin(accepted_labels) == False]
rows_with_problems

"""Por lo que hemos podido observar, no se muestra ningún resultado que no sea *negative* o *positive*. Por tanto, no hay ninguna
información en la columna `sentiment` que no sean estos dos valores. Esto deja claro que no tenemos que modificar nada de esta columna.

Tras haber confirmado que no existe ninguna problemática con la columna `sentiment`, procedemos con la limpieza de la columna `review` de ambos conjuntos. Anteriormente vimos que lo único que ensuciaba el contenido eran algunas etiquetas (en este caso, saltos de línea), por lo que las sustituiremos por un espacio.
"""

# Sustituimos las etiquetas de salto de página en la columna 'review' por un espacio
training_set['review'] = training_set['review'].str.replace(r'<br\s*/?>', ' ')

# Creamos un archivo nuevo con el dataframe limpio
training_set.to_csv('training_set_limpio.csv', index=False)

# Inspeccionamos el aspecto del conjunto de entrenamiento limpio
training_set.head(30)

"""A continuación, realizamos exactamente el mismo proceso con la columna `review` de test_set."""

# Reemplazamos las etiquetas de salto de página por un espacio en la columna 'review' del test_set
test_set['review'] = test_set['review'].str.replace(r'<br\s*/?>', ' ')

# Crear un archivo nuevo con el dataframe limpio del test_set
test_set.to_csv('test_set_limpio.csv', index=False)

# Inspeccionamos el aspecto del conjunto de entrenamiento limpio
test_set.head(30)

"""Al volver a visualizar la columna `sentiment` hemos podido observar que las etiquetas se han sustituido por espacios, por lo que ahora podemos juntar ambos sets en un mismo *dataset* para facilitar el preprocesado."""

dataset = pandas.concat([training_set,test_set])

"""# Inspección de los datos

Comenzaremos por observar la distribución de las dos clases (*negative* y *positive*) para ver si existe algún desbalanceo de datos.
"""

from collections import Counter
sns.countplot(data=training_set, x=training_set.sentiment, order=[x for x, count in sorted(Counter(training_set.sentiment).items(), key=lambda x: -x[1])], palette="Blues_r")
plt.xticks(rotation=90)

"""Como hemos podido observar, no hay ningún tipo de desbalanceo. Por tanto, no nos tenemos que preocupar por clases desbalanceadas.

Para seguir con la inspección de datos, vamos a agrupar por sentimiento (positivo o negativo) y concatenar por cada grupo en una sola cadena de texto. Luego, utilizaremos un tokenizador para dividir estas cadenas de texto en palabras. Tras esto, se cuenta la frecuencia de cada palabra y se muestran las 25 palabras más comunes en un gráfico de barras para *negative* y *positive*.
"""

import pandas as pd
from matplotlib import interactive

# Agrupamos los textos por clases
df = pd.DataFrame({"review": training_set.review, "sentiment": training_set.sentiment})
grouped = df.groupby(["sentiment"]).apply(lambda x: x["review"].sum())
grouped_df = pd.DataFrame({"sentiment": grouped.index, "review": grouped.values})

# Tokenizamos las frases utilizando el espacio en blanco como separador
from nltk.tokenize import WhitespaceTokenizer
tokenizer = WhitespaceTokenizer()

for ii, review in enumerate(grouped_df.review):
    pd.DataFrame(tokenizer.tokenize(review)).apply(pd.value_counts).head(25).plot(kind="bar")
    plt.title(grouped_df.sentiment[ii], fontsize=20)
    plt.xticks(fontsize=15)
    interactive(True)
    plt.show()

"""Al observar las palabras más repetidas, lo que se podía esperar era encontrar términos representativos que nos ayudaran a distinguir entre reseñas positivas y negativas. Sin embargo, notamos que hay conjuntos de palabras muy similares que aportan poca información en ambas categorías, las ***stopwords***. Su presencia afecta negativamente el rendimiento del clasificador, por lo que es necesario eliminarlas. De hecho, debido a estas palabras, nos cuesta percibir la diferencia entre reseñas negativas y positivas, ya que prácticamente lo único que encontramos son estas palabras que no aportan información relevante.

También notamos que algunas palabras se consideran diferentes simplemente por la diferencia de mayúsculas y minúsculas, como *The* y *the*. Para abordar este problema, debemos **normalizar** las palabras.

Además, encontramos variantes de un mismo verbo, como *are* e *is*. Para lidiar con esto, podemos aplicar técnicas como el ***stemming*** o la **lematización** para reducir las palabras a su forma base y asegurarnos de que se traten de manera coherente en el análisis de texto.

## Preprocesado y preparación de datos


Comenzaremos con un preprocesado básico del *pipeline* de PLN
 - Extracción de palabras
 - Lematización para evitar que palabras con la misma raíz se repitan
 - *Stemming* para agrupar palabras con el mismo significado
 - Eliminación de *stopwords*

Para ello, vamos a crear la función **process_text** que engloba todo el preprocesado del texto. En este caso, utilizaremos la librería NLTK.
"""

from nltk.stem import *
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords

import re

import nltk
nltk.download('stopwords')
nltk.download('wordnet')

def process_text(raw_text):
    # Consideramos únicamente letras utilizando una expresión regular
    letters_only = re.sub("[^a-zA-Z]", " ",raw_text)
    # Convertimos todo a minúsculas
    words = letters_only.lower().split()

    # Eliminamos las stopwords
    stops = set(stopwords.words("english"))
    not_stop_words = [w for w in words if not w in stops]

    # Lematización
    wordnet_lemmatizer = WordNetLemmatizer()
    lemmatized = [wordnet_lemmatizer.lemmatize(word) for word in not_stop_words]

    # Stemming
    stemmer = PorterStemmer()
    stemmed = [stemmer.stem(word) for word in lemmatized]

    return( " ".join( stemmed ))

"""Ahora que disponemos de la función, la aplicamos al contenido textual del dataset: `review`. Para mantener el contenido original, creamos una nueva columna."""

dataset['clean_review'] = dataset['review'].apply(lambda x: process_text(x))
dataset.head()

"""A continuación, visualizamos lo que tenemos hasta el momento:"""

df = pd.DataFrame({"review": dataset[0:len(training_set)].clean_review, "sentiment": dataset[0:len(training_set)].sentiment})
grouped = df.groupby(["sentiment"]).apply(lambda x: x["review"].sum())
grouped_df = pd.DataFrame({"sentiment": grouped.index, "review": grouped.values})

from nltk.tokenize import WhitespaceTokenizer
tokenizer = WhitespaceTokenizer()

for ii, text in enumerate(grouped_df.review):
    pd.DataFrame(tokenizer.tokenize(text)).apply(pd.value_counts).head(25).plot(kind="bar")
    plt.title(grouped_df.sentiment[ii], fontsize=20)
    plt.xticks(fontsize=15)
    interactive(True)
    plt.show()

"""Como se puede apreciar en ambos gráficos, ha cambiado bastante. Hemos eliminado las *stopwords*, por lo que las palabras que observamos ahora son mucho más representativas. Además, se aprecia un uso más distintivo de algunas palabras entre las reseñas positivas y negativas, ya que antes debido a las *stopwords* el contenido era prácticamente el mismo.

Por ejemplo, aunque en las reseñas negativas también encontramos el adjetivo *good*, podemos ver que hay bastante uso también de *bad*, como podíamos esperar en la reseña negativa. Sin embargo, en el caso de las reseñas positivas, no encotramos *bad*. Lo que predomina es el adjetivo *good*, al ser reseñas positivas.

**Para continuar, vamos a divir nuestro *dataset* de nuevo para obtener el conjunto de entrenamiento y el de test, que se ha realizado de la siguiente forma:**

**X_train:** Se coge desde el inicio (índice 0) hasta el índice que representa el final del conjunto de entrenamiento (denotado como len(training_set)). Este subconjunto de datos consiste en las columnas `id` y `clean_review` del conjunto de entrenamiento, que se utilizan como características de entrada para el modelo.

**y_train:** Al igual que X_train, se toma una porción del *dataset* desde el inicio hasta el final del conjunto de entrenamiento, pero esta vez se extraen solo las etiquetas de clase `sentiment` correspondientes a las reseñas en el conjunto de entrenamiento.

**X_test:** Para el conjunto de prueba, se toma una porción de dataset que va desde el final del conjunto de entrenamiento (índice len(training_set)) hasta el final de *dataset*. Esto incluye las mismas columnas que en X_train, pero corresponden a las reseñas del conjunto de prueba.

**y_test:** Al igual que X_test, se coge desde el final del conjunto de entrenamiento hasta el final del *dataset* y se extraen las etiquetas de clase `sentiment` correspondientes a las reseñas en el conjunto de prueba.
"""

X_train = dataset[0:len(training_set)][["id", "review"]]
y_train = dataset[0:len(training_set)][["sentiment"]]
X_test = dataset[len(training_set):len(dataset)][["id", "review"]]
y_test = dataset[len(training_set):len(dataset)][["sentiment"]]

# Hot encoding para sentiment
from sklearn import preprocessing
le = preprocessing.LabelEncoder()
le.fit(y_train.sentiment.values)
target_sentiment = le.classes_
encoded_y_train = le.transform(y_train.sentiment.values)

# Con esta función podremos ver cómo evoluciona el entrenamiento y la validación de los modelos.

import matplotlib.pyplot as plt
plt.style.use('ggplot')

def plot_history(history):
    acc = history.history['acc']
    val_acc = history.history['val_acc']
    loss = history.history['loss']
    val_loss = history.history['val_loss']
    x = range(1, len(acc) + 1)

    plt.figure(figsize=(12, 5))
    plt.subplot(1, 2, 1)
    plt.plot(x, acc, 'b', label='Training acc')
    plt.plot(x, val_acc, 'r', label='Validation acc')
    plt.title('Training and validation accuracy')
    plt.legend()
    plt.subplot(1, 2, 2)
    plt.plot(x, loss, 'b', label='Training loss')
    plt.plot(x, val_loss, 'r', label='Validation loss')
    plt.title('Training and validation loss')
    plt.legend()

# Herramienta para la gestión de las etiquetas de salida
from tensorflow.keras.utils import to_categorical

Y_train = to_categorical(encoded_y_train, 2)
Y_test = to_categorical(le.transform(y_test.values), 2)

def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    import itertools
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')
    plt.figure()
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.show()

"""#Ajuste modelo pre-entrenado"""

from transformers import BertTokenizer, TFBertForSequenceClassification
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import SparseCategoricalCrossentropy
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf

# Paso 1: Cargar el modelo preentrenado BERT
model_name = "bert-base-uncased"
model_bert = TFBertForSequenceClassification.from_pretrained(model_name, num_labels=2)

# Paso 2: Tokenización de los datos de entrenamiento y prueba
tokenizer = BertTokenizer.from_pretrained(model_name)

# Asegurarse de que los datos están en el formato correcto (listas de cadenas)
max_length = 512
X_train_list = X_train['review'].tolist()
X_test_list = X_test['review'].tolist()

X_train_tokens = tokenizer(X_train_list, padding=True, truncation=True, return_tensors="tf")
X_test_tokens = tokenizer(X_test_list, padding=True, truncation=True, return_tensors="tf")

# Paso 3: Configurar los datos de salida
le = LabelEncoder()
Y_train_labels = le.fit_transform(y_train.values)
Y_test_labels = le.transform(y_test.values)

# Paso 4: Compilar el modelo
loss = SparseCategoricalCrossentropy(from_logits=True)
model_bert.compile(optimizer='adam', loss=loss, metrics=['accuracy'])

# Paso 5: Entrenar el modelo
history = model_bert.fit(
    X_train_tokens['input_ids'],
    Y_train_labels,
    validation_split=0.2,
    epochs=3,
    batch_size=8
)

# Paso 6: Evaluar el modelo
loss, accuracy = model_bert.evaluate(X_test_tokens['input_ids'], Y_test_labels)
print(f'Loss on test set: {loss}')
print(f'Accuracy on test set: {accuracy}')

# Paso 7: Generar predicciones y métricas de evaluación
predictions = model_bert.predict(X_test_tokens['input_ids']).logits.argmax(axis=1)
print(classification_report(Y_test_labels, predictions, target_names=le.classes_))
conf_matrix = confusion_matrix(Y_test_labels, predictions)

# Paso 8: Visualización de la matriz de confusión
def plot_confusion_matrix(cm, labels):
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.show()

plot_confusion_matrix(conf_matrix, le.classes_)

# Paso 6: Evaluar el modelo
loss, accuracy = model_bert.evaluate(X_test_tokens['input_ids'], Y_test_labels)
print(f'Loss on test set: {loss}')
print(f'Accuracy on test set: {accuracy}')

# Paso 7: Generar predicciones y métricas de evaluación
predictions = model_bert.predict(X_test_tokens['input_ids']).logits.argmax(axis=1)
print(classification_report(Y_test_labels, predictions, target_names=le.classes_))
conf_matrix = confusion_matrix(Y_test_labels, predictions)

# Paso 8: Visualización de la matriz de confusión
def plot_confusion_matrix(cm, labels):
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.show()

plot_confusion_matrix(conf_matrix, le.classes_)

"""## DistilBERT"""

from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import SparseCategoricalCrossentropy
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf

# Paso 1: Cargar el modelo preentrenado DistilBERT
model_name = "distilbert-base-uncased"
model_distilbert = TFDistilBertForSequenceClassification.from_pretrained(model_name, num_labels=2)

# Paso 2: Tokenización de los datos de entrenamiento y prueba
tokenizer = DistilBertTokenizer.from_pretrained(model_name)

# Asegurarse de que los datos están en el formato correcto (listas de cadenas)
max_length = 512
X_train_list = X_train['review'].tolist()
X_test_list = X_test['review'].tolist()

X_train_tokens = tokenizer(X_train_list, padding=True, truncation=True, return_tensors="tf")
X_test_tokens = tokenizer(X_test_list, padding=True, truncation=True, return_tensors="tf")

# Paso 3: Configurar los datos de salida
le = LabelEncoder()
Y_train_labels = le.fit_transform(y_train.values)
Y_test_labels = le.transform(y_test.values)

# Paso 4: Compilar el modelo
loss = SparseCategoricalCrossentropy(from_logits=True)
optimizer = Adam(learning_rate=2e-5)
model_distilbert.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])

# Paso 5: Entrenar el modelo
history = model_distilbert.fit(
    {'input_ids': X_train_tokens['input_ids'], 'attention_mask': X_train_tokens['attention_mask']},
    Y_train_labels,
    validation_split=0.2,
    epochs=3,
    batch_size=8
)

"""#Modelo ajustado para análisis de sentimientos"""

from transformers import BertTokenizer, TFBertForSequenceClassification
from sklearn.metrics import accuracy_score, classification_report
import numpy as np

# Cargar el modelo preajustado
model_name = "textattack/bert-base-uncased-imdb"
model = TFBertForSequenceClassification.from_pretrained(model_name, from_pt=True)

# Tokenización de los datos de prueba
tokenizer = BertTokenizer.from_pretrained(model_name)
X_test_tokens = tokenizer(X_test['review'].tolist(), padding=True, truncation=True, return_tensors="tf")

# Realizar predicciones
predictions = model.predict(X_test_tokens['input_ids'])[0]
predicted_labels = [np.argmax(p) for p in predictions]

# Convertir predicted_labels a formato multilabel-indicator
predicted_multilabels = np.eye(len(np.unique(Y_test)), dtype=int)[predicted_labels]

# Evaluar los resultados
accuracy = accuracy_score(Y_test, predicted_multilabels)
print("Accuracy:", accuracy)
print("Classification Report:")
print(classification_report(Y_test, predicted_multilabels))

from transformers import BertTokenizer, TFBertForSequenceClassification
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Cargar el modelo preajustado
model_name = "textattack/bert-base-uncased-imdb"
model = TFBertForSequenceClassification.from_pretrained(model_name, from_pt=True)

# Tokenización de los datos de prueba
tokenizer = BertTokenizer.from_pretrained(model_name)
X_test_tokens = tokenizer(X_test['review'].tolist(), padding=True, truncation=True, return_tensors="tf")

# Realizar predicciones
predictions = model.predict(X_test_tokens['input_ids'])[0]
predicted_labels = [np.argmax(p) for p in predictions]

# Convertir predicted_labels a formato multilabel-indicator
predicted_multilabels = np.eye(len(np.unique(Y_test)), dtype=int)[predicted_labels]

# Evaluar los resultados
accuracy = accuracy_score(Y_test, predicted_multilabels)
print("Accuracy:", accuracy)
print("Classification Report:")
print(classification_report(Y_test, predicted_multilabels))

# Calcular la matriz de confusión
conf_matrix = confusion_matrix(Y_test.argmax(axis=1), predicted_multilabels.argmax(axis=1))

# Visualizar la matriz de confusión
def plot_confusion_matrix(cm, labels):
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.show()

plot_confusion_matrix(conf_matrix, np.unique(Y_test))

from transformers import BertTokenizer, TFBertForSequenceClassification
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Cargar el modelo preajustado
model_name = "textattack/bert-base-uncased-imdb"
model = TFBertForSequenceClassification.from_pretrained(model_name, from_pt=True)

# Tokenización de los datos de prueba
tokenizer = BertTokenizer.from_pretrained(model_name)
X_test_tokens = tokenizer(X_test['review'].tolist(), padding=True, truncation=True, return_tensors="tf")

# Realizar predicciones
predictions = model.predict(X_test_tokens['input_ids'])[0]
predicted_labels = [np.argmax(p) for p in predictions]

# Convertir predicted_labels a formato multilabel-indicator
predicted_multilabels = np.eye(len(np.unique(Y_test)), dtype=int)[predicted_labels]

# Evaluar los resultados
accuracy = accuracy_score(Y_test, predicted_multilabels)
print("Accuracy:", accuracy)
print("Classification Report:")
print(classification_report(Y_test, predicted_multilabels))

# Calcular la matriz de confusión
conf_matrix = confusion_matrix(Y_test.argmax(axis=1), predicted_multilabels.argmax(axis=1))

# Visualizar la matriz de confusión
def plot_confusion_matrix(cm, labels):
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.show()

plot_confusion_matrix(conf_matrix, np.unique(Y_test))

"""#Comparación y Análisis

1.   Comparar los resultados obtenidos de ambos enfoques para ver cuál se desempeña mejor en tu conjunto de datos específico.
2.   Análisis de las métricas como precisión, recall, F1 score, etc., para tener una comprensión detallada del rendimiento de cada modelo.
3. Discusión de los resultados para interpretar qué enfoque es más adecuado para tu tarea específica y por qué.

#1. Evaluación de los Resultados
##Métricas de Evaluación


*   Precisión (Accuracy): Proporción de predicciones correctas.
*   Recall: Proporción de verdaderos positivos sobre el total de positivos reales.
*   F1 Score: Media armónica de precisión y recall, útil para balancear las dos métricas.
Matriz de Confusión: Para analizar la distribución de las predicciones correctas e incorrectas.

##Análisis de los Resultados por Modelo
### Resultados del Enfoque 1 (Ajuste de un Modelo Pre-entrenado):
- Analiza las métricas de evaluación en el conjunto de prueba.
- Compara el rendimiento antes y después del ajuste.

###Resultados del Enfoque 2 (Modelo ya Ajustado):
- Evalúa las métricas de evaluación directamente en el conjunto de prueba.

2. Comparación de Parámetros de los Modelos
Número de Parámetros:
Anota el número de parámetros de cada modelo. Los modelos más grandes, como GPT-3, pueden tener cientos de miles de millones de parámetros, mientras que modelos más pequeños, como DistilBERT, tienen menos.
Colección de Pre-entrenamiento y Ajuste:
Documenta los conjuntos de datos utilizados para el pre-entrenamiento y el ajuste. La calidad y el tamaño del conjunto de datos pueden influir significativamente en el rendimiento.
3. Comparación de Tiempos de Entrenamiento
Tiempo de Entrenamiento:
Mide y compara el tiempo que tomó ajustar cada modelo en el conjunto de datos de entrenamiento.
Documenta los recursos computacionales utilizados (GPU, TPU, CPU, etc.).
4. Análisis Cualitativo
Ventajas e Inconvenientes Observados:
Enfoque 1:
Ventajas: Posibilidad de personalizar el modelo específicamente para tu tarea y datos.
Inconvenientes: Puede requerir mucho tiempo y recursos computacionales para el ajuste.
Enfoque 2:
Ventajas: Menor tiempo de implementación, ya que el modelo ya está ajustado.
Inconvenientes: Puede no estar perfectamente optimizado para tus datos específicos, lo que puede afectar el rendimiento.
5. Presentación de Resultados y Comparaciones
Tabla de Comparación
Crea una tabla que resuma los principales resultados y características de cada modelo y enfoque:

Modelo	Número de Parámetros	Conjunto de Pre-entrenamiento	Conjunto de Ajuste	Precisión	Recall	F1 Score	Tiempo de Entrenamiento	Ventajas	Inconvenientes
BERT Ajustado	110M	Wikipedia + Libros	Datos Específicos	0.90	0.88	0.89	2 horas	Alta precisión y recall después del ajuste	Tiempo de ajuste relativamente largo
Sentiment Model	345M	Datos genéricos de análisis de sentimientos	N/A	0.85	0.84	0.84	N/A	Implementación rápida y fácil	Menor precisión en datos específicos
Análisis Descriptivo
Desempeño de los Modelos:

Describe cómo se desempeñaron los modelos en términos de métricas de evaluación.
Compara los resultados entre los diferentes enfoques y modelos.
Recursos y Tiempos:

Comenta sobre los recursos computacionales utilizados y los tiempos de entrenamiento.
Discute si el tiempo y los recursos invertidos justifican el rendimiento obtenido.
Implicaciones para Futuras Implementaciones:

Basado en los resultados, proporciona recomendaciones sobre cuál enfoque sería más adecuado para tareas similares en el futuro.
Discute posibles mejoras, como la optimización de hiperparámetros o el uso de modelos híbridos.
6. Conclusiones
Resumen de Hallazgos:

Resume los hallazgos clave de tu análisis, destacando qué modelos y enfoques funcionaron mejor y por qué.
Recomendaciones:

Ofrece recomendaciones basadas en el análisis, considerando el equilibrio entre rendimiento, tiempo de procesamiento y recursos computacionales.
"""